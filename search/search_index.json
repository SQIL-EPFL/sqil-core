{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"SQIL core","text":""},{"location":"#installation","title":"Installation","text":"<pre><code>$ pip install sqil_core\n</code></pre>"},{"location":"#usage","title":"Usage","text":"<p>You can find all the functions available and examples in the documentation. <pre><code>import sqil_core as sqil\n\npath = 'path to your data folder'\n\n# Extract data in one line\nmag, phase, freq = sqil.extract_h5_data(path, ['mag_dB', 'phase', 'ro_freq'])\n</code></pre></p>"},{"location":"#for-developers","title":"For developers","text":""},{"location":"#development","title":"Development","text":"<p>Install poetry if you haven't already (<code>pip install poetry</code>) and then run the following <pre><code>$ poetry install\n$ poetry run pre-commit install\n</code></pre></p>"},{"location":"#test-your-changes","title":"Test your changes","text":"<p><pre><code>$ pip install -e . --user\n</code></pre> If you're using a jupyter notebook remember to restart the kernel</p>"},{"location":"#build","title":"Build","text":"<pre><code>$ poetry run build\n</code></pre>"},{"location":"#docs","title":"Docs","text":"<p>Serve docs <pre><code>$ poetry run docs_serve\n</code></pre></p> <p>Build docs <pre><code>$ poetry run docs_build\n</code></pre></p>"},{"location":"API%20reference/experiment/analysis/","title":"Analysis","text":""},{"location":"API%20reference/experiment/analysis/#sqil_core.experiment._analysis.AnalysisResult","title":"<code>AnalysisResult</code>","text":"<p>Container for storing and managing results from a quantum measurement analysis.</p> <p>Attributes:</p> Name Type Description <code>updated_params</code> <code>dict[str, dict]</code> <p>Dictionary containing the updated parameters for each qubit.</p> <code>figures</code> <code>dict[str, Figure]</code> <p>Dictionary of matplotlib figures.</p> <code>fits</code> <code>dict[str, FitResult]</code> <p>Dictionary of fit results.</p> <code>extra_data</code> <code>dict[str, ndarray]</code> <p>Dictionary of auxiliary computed arrays (e.g., processed IQ data, FFT results).</p> <p>Methods:</p> Name Description <code>add_exp_info_to_figures</code> <p>Annotates each figure with experiment ID and cooldown name from directory path.</p> <code>save_figures</code> <p>Saves all figures as PNG and interactive HTML using mpld3.</p> <code>aggregate_fit_summaries</code> <p>Aggregates human-readable summaries from all fit results.</p> <code>save_fits</code> <p>Saves aggregated fit summaries to a markdown file.</p> <code>save_extra_data</code> <p>Saves extra data arrays into an HDF5 file.</p> <code>save_all</code> <p>Runs all save methods and annotates figures with experimental metadata.</p> Source code in <code>sqil_core/experiment/_analysis.py</code> <pre><code>class AnalysisResult:\n    \"\"\"\n    Container for storing and managing results from a quantum measurement analysis.\n\n    Attributes\n    ----------\n    updated_params : dict[str, dict]\n        Dictionary containing the updated parameters for each qubit.\n    figures : dict[str, matplotlib.figure.Figure]\n        Dictionary of matplotlib figures.\n    fits : dict[str, FitResult]\n        Dictionary of fit results.\n    extra_data : dict[str, np.ndarray]\n        Dictionary of auxiliary computed arrays (e.g., processed IQ data, FFT results).\n\n    Methods\n    -------\n    add_exp_info_to_figures(dir_path)\n        Annotates each figure with experiment ID and cooldown name from directory path.\n    save_figures(dir_path)\n        Saves all figures as PNG and interactive HTML using mpld3.\n    aggregate_fit_summaries()\n        Aggregates human-readable summaries from all fit results.\n    save_fits(dir_path)\n        Saves aggregated fit summaries to a markdown file.\n    save_extra_data(dir_path)\n        Saves extra data arrays into an HDF5 file.\n    save_all(dir_path)\n        Runs all save methods and annotates figures with experimental metadata.\n    \"\"\"\n\n    updated_params: dict[str, dict] = {}\n    figures: dict[str, Figure] = {}\n    fits: dict[str, FitResult] = {}\n    extra_data: dict[str, np.ndarray] = {}\n\n    def __init__(\n        self,\n        updated_params: dict[str, dict] = {},\n        figures: dict[str, Figure] = {},\n        fits: dict[str, FitResult] = {},\n        extra_data: dict[str, np.ndarray] = {},\n    ):\n        self.updated_params = updated_params or {}\n        self.figures = figures or {}\n        self.fits = fits or {}\n        self.extra_data = extra_data or {}\n\n    def add_exp_info_to_figures(self, dir_path: str):\n        if not self.figures:\n            return\n        id = get_measurement_id(dir_path)\n        cooldown_name = Path(dir_path).parts[-3]\n        for _, fig in self.figures.items():\n            # Add dummy text to infer font size\n            dummy_text = fig.text(0, 0, \"dummy\", visible=False)\n            font_size = dummy_text.get_fontsize()\n            dummy_text.remove()\n            fig.text(\n                0.98,\n                0.98,\n                f\"{cooldown_name}\\n{id} | {dir_path[-16:]}\",\n                ha=\"right\",\n                va=\"top\",\n                color=\"gray\",\n                fontsize=font_size * 0.8,\n            )\n\n    def save_figures(self, dir_path: str):\n        \"\"\"Saves figures both as png and interactive html.\"\"\"\n        for key, fig in self.figures.items():\n            path = os.path.join(dir_path, key)\n            fig.savefig(os.path.join(f\"{path}.png\"), bbox_inches=\"tight\", dpi=300)\n            html = mpld3.fig_to_html(fig)\n            with open(f\"{path}.html\", \"w\") as f:\n                f.write(html)\n\n    def aggregate_fit_summaries(self):\n        \"\"\"Aggreate all the fit summaries and include model name.\"\"\"\n        result = \"\"\n        for key, fit in self.fits.items():\n            summary = fit.summary(no_print=True)\n            result += f\"{key}\\nModel: {fit.model_name}\\n{summary}\\n\"\n        return result\n\n    def save_fits(self, dir_path: str):\n        if not self.fits:\n            return\n        with open(os.path.join(dir_path, \"fit.mono.md\"), \"w\", encoding=\"utf-8\") as f:\n            f.write(self.aggregate_fit_summaries())\n\n    def save_extra_data(self, dir_path: str):\n        if not self.extra_data:\n            return\n        with h5py.File(os.path.join(dir_path, \"extra.ddh5\"), \"a\") as f:\n            grp = f.require_group(\"data\")\n            for key, array in self.extra_data.items():\n                # Overwrite if already exists\n                if key in grp:\n                    del grp[key]\n            grp.create_dataset(key, data=array)\n\n    def save_all(self, dir_path: str):\n        self.add_exp_info_to_figures(dir_path)\n        self.save_figures(dir_path)\n        self.save_fits(dir_path)\n        self.save_extra_data(dir_path)\n</code></pre>"},{"location":"API%20reference/experiment/analysis/#sqil_core.experiment._analysis.AnalysisResult.aggregate_fit_summaries","title":"<code>aggregate_fit_summaries()</code>","text":"<p>Aggreate all the fit summaries and include model name.</p> Source code in <code>sqil_core/experiment/_analysis.py</code> <pre><code>def aggregate_fit_summaries(self):\n    \"\"\"Aggreate all the fit summaries and include model name.\"\"\"\n    result = \"\"\n    for key, fit in self.fits.items():\n        summary = fit.summary(no_print=True)\n        result += f\"{key}\\nModel: {fit.model_name}\\n{summary}\\n\"\n    return result\n</code></pre>"},{"location":"API%20reference/experiment/analysis/#sqil_core.experiment._analysis.AnalysisResult.save_figures","title":"<code>save_figures(dir_path)</code>","text":"<p>Saves figures both as png and interactive html.</p> Source code in <code>sqil_core/experiment/_analysis.py</code> <pre><code>def save_figures(self, dir_path: str):\n    \"\"\"Saves figures both as png and interactive html.\"\"\"\n    for key, fig in self.figures.items():\n        path = os.path.join(dir_path, key)\n        fig.savefig(os.path.join(f\"{path}.png\"), bbox_inches=\"tight\", dpi=300)\n        html = mpld3.fig_to_html(fig)\n        with open(f\"{path}.html\", \"w\") as f:\n            f.write(html)\n</code></pre>"},{"location":"API%20reference/experiment/handler/","title":"Experiment Handler","text":""},{"location":"API%20reference/experiment/handler/#sqil_core.experiment._experiment.ExperimentHandler","title":"<code>ExperimentHandler</code>","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>sqil_core/experiment/_experiment.py</code> <pre><code>class ExperimentHandler(ABC):\n    setup: dict\n    instruments: Instruments | None = None\n\n    zi_setup: DeviceSetup\n    zi_session: Session\n    qpu: QPU\n\n    db_schema: dict = None\n\n    def __init__(\n        self,\n        params: dict = {},\n        param_dict_path: str = \"\",\n        setup_path: str = \"\",\n        server=False,\n    ):\n        # Read setup file\n        if not setup_path:\n            config = read_yaml(\"config.yaml\")\n            setup_path = config.get(\"setup_path\", \"setup.py\")\n        self.setup = _extract_variables_from_module(\"setup\", setup_path)\n\n        # Get instruments through the server or connect locally\n        if server:\n            server, instrument_instances = link_instrument_server()\n        else:\n            instrument_dict = self.setup.get(\"instruments\", None)\n            if not instrument_dict:\n                logger.warning(\n                    f\"Unable to find any instruments in {setup_path}\"\n                    + \"Do you have an `instruments` entry in your setup file?\"\n                )\n            # Reset event listeners\n            clear_signal(before_experiment)\n            clear_signal(before_sequence)\n            clear_signal(after_sequence)\n            clear_signal(after_experiment)\n        instrument_instances = connect_instruments(instrument_dict)\n\n        # Create Zurich Instruments session\n        zi = cast(ZI_Instrument, instrument_instances.get(\"zi\", None))\n        if zi is not None:\n            self.zi_setup = zi.generate_setup()\n            # self.zi_setup = DeviceSetup.from_descriptor(zi.descriptor, zi.address)\n            self.zi_session = Session(self.zi_setup)\n            self.zi_session.connect()\n            self._load_qpu(zi.generate_qpu)\n\n        self.instruments = Instruments(instrument_instances)\n        self._setup_instruments()\n\n    def _load_qpu(self, generate_qpu: Callable):\n        qpu_filename = self.setup[\"storage\"].get(\"qpu_filename\", \"qpu.json\")\n        db_path_local = self.setup[\"storage\"][\"db_path_local\"]\n        try:\n            self.qpu = serializers.load(os.path.join(db_path_local, qpu_filename))\n        except FileNotFoundError:\n            logger.warning(\n                f\"Cannot find QPU file name {qpu_filename} in {db_path_local}\"\n            )\n            logger.warning(f\" -&gt; Creating a new QPU file\")\n            self.qpu = generate_qpu(self.zi_setup)\n            os.makedirs(db_path_local, exist_ok=True)\n            w_save(\n                self.qpu,\n                os.path.join(db_path_local, qpu_filename),\n            )\n\n    # Move to server\n    def _setup_instruments(self):\n        \"\"\"Default setup for all instruments with support for custom setups\"\"\"\n        logger.info(\"Setting up instruments\")\n        if not hasattr(self, \"instruments\"):\n            logger.warning(\"No instruments to set up\")\n            return\n\n        for instrument in self.instruments:\n            if not hasattr(instrument, \"setup\"):\n                continue\n            instrument.setup()\n\n    @abstractmethod\n    def sequence(self, *args, **kwargs):\n        \"\"\"Experimental sequence defined by the user\"\"\"\n        pass\n\n    @abstractmethod\n    def analyze(self, path, *args, **kwargs):\n        pass\n\n    def run(self, *args, **kwargs):\n        try:\n            db_type = self.setup.get(\"storage\", {}).get(\"db_type\", \"\")\n\n            if db_type == \"plottr\":\n                return self.run_with_plottr(*args, **kwargs)\n            else:\n                return self.run_raw(*args, **kwargs)\n\n        finally:\n            # Close and delete QCodes instances to avoid connection issues in following experiments\n            QCodesInstrument.close_all()\n            for instrument in self.instruments:\n                del instrument\n\n    def run_with_plottr(self, *args, **kwargs):\n        logger.info(\"Before exp\")\n        before_experiment.send(sender=self)\n\n        # Map input parameters index to their name\n        params_map, _ = map_inputs(self.sequence)\n\n        # Get information on sweeps\n        sweeps: dict = kwargs.get(\"sweeps\", None)\n        sweep_keys = []\n        sweep_grid = []\n        sweep_schema = {}\n        if sweeps is not None:\n            # Name of the parameters to sweep\n            sweep_keys = list(sweeps.keys())\n            # Create a mesh grid of all the sweep parameters\n            sweep_grid = list(itertools.product(*sweeps.values()))\n            # Add sweeps to the database schema\n            for i, key in enumerate(sweep_keys):\n                # TODO: dynamically add unit\n                sweep_schema[f\"sweep{i}\"] = {\"role\": \"axis\", \"param_id\": key}\n\n        # Create the plotter datadict (database) using the inferred schema\n        db_schema = {**self.db_schema, **sweep_schema}\n        datadict = build_plottr_dict(db_schema)\n        # Get local and server storage folders\n        db_path = self.setup[\"storage\"][\"db_path\"]\n        db_path_local = self.setup[\"storage\"][\"db_path_local\"]\n\n        # TODO: dynamically assign self.exp_name to class name if not provided\n        with DDH5Writer(datadict, db_path_local, name=self.exp_name) as writer:\n            # Get the path to the folder where the data will be stored\n            storage_path = get_plottr_path(writer, db_path)\n            storage_path_local = get_plottr_path(writer, db_path_local)\n            # Save helper files\n            writer.save_text(\"paths.md\", f\"{storage_path_local}\\n{storage_path}\")\n            # Save backup qpu\n            old_qubits = self.qpu.copy_quantum_elements()\n            serializers.save(self.qpu, os.path.join(storage_path_local, \"qpu_old.json\"))\n\n            # TODO: for index sweep don't recompile laboneq\n            for sweep_values in sweep_grid or [None]:\n                data_to_save = {}\n\n                # Run/create the experiment. Creates it for laboneq, runs it otherwise\n                seq = self.sequence(*args, **kwargs)\n                # Detect if the sequence created a laboneq experiment\n                is_laboneq_exp = type(seq) == LaboneQExperiment\n\n                if is_laboneq_exp:\n                    qu_indices = kwargs.get(\"qu_idx\", [0])\n                    if type(qu_indices) == int:\n                        qu_indices = [qu_indices]\n                    used_qubits = [self.qpu.quantum_elements[i] for i in qu_indices]\n                    qu_idx_by_uid = [qubit.uid for qubit in self.qpu.quantum_elements]\n                    # TODO: save and re-apply old qubit params\n                    # Reset to the first value of every sweep,\n                    # then override current sweep value for all qubits\n                    for qubit in used_qubits:\n                        tmp = dict(zip(sweep_keys, sweep_values or []))\n                        qubit.update(**tmp)\n                    # Create the experiment (required to update params)\n                    seq = self.sequence(*args, **kwargs)\n                    compiled_exp = compile_experiment(self.zi_session, seq)\n                    # pulse_sheet(self.zi_setup, compiled_exp, self.exp_name)\n                    before_sequence.send(sender=self)\n                    result = run_experiment(self.zi_session, compiled_exp)\n                    after_sequence.send(sender=self)\n                    # TODO: handle multiple qubits. Maybe different datadicts?\n                    raw_data = result[qu_idx_by_uid[qu_indices[0]]].result.data\n                    data_to_save[\"data\"] = raw_data\n                    result = raw_data\n                else:\n                    # TODO: handle results for different instrumets\n                    data_to_save[\"data\"] = seq\n\n                # Add parameters to the data to save\n                datadict_keys = datadict.keys()\n                for key, value in params_map.items():\n                    if key in datadict_keys:\n                        data_to_save[key] = args[value]\n                # Add sweeps to the data to save\n                if sweeps is not None:\n                    for i, key in enumerate(sweep_keys):\n                        data_to_save[f\"sweep{i}\"] = sweep_values[i]\n\n                # Save data using plottr\n                writer.add_data(**data_to_save)\n\n            after_experiment.send()\n\n        # Reset the qpu to its previous state\n        self.qpu.quantum_operations.detach_qpu()\n        self.qpu = QPU(old_qubits, self.qpu.quantum_operations)\n\n        # Run analysis script\n        try:\n            anal_res = self.analyze(storage_path_local, *args, **kwargs)\n            if type(anal_res) == AnalysisResult:\n                anal_res = cast(AnalysisResult, anal_res)\n                anal_res.save_all(storage_path_local)\n                # Update QPU\n                if is_laboneq_exp and not kwargs.get(\"no_update\", False):\n                    for qu_id in anal_res.updated_params.keys():\n                        qubit = self.qpu.quantum_element_by_uid(qu_id)\n                        qubit.update(**anal_res.updated_params[qu_id])\n                # writer.save_text(\"analysis.md\", anal_res)\n                plt.show()\n        except Exception as e:\n            logger.error(f\"Error while analyzing the data {e}\")\n\n        w_save(self.qpu, os.path.join(storage_path_local, \"qpu_new.json\"))\n        qpu_filename = self.setup[\"storage\"].get(\"qpu_filename\", \"qpu.json\")\n        w_save(\n            self.qpu,\n            os.path.join(db_path_local, qpu_filename),\n        )\n\n        # Copy the local folder to the server\n        copy_folder(storage_path_local, storage_path)\n\n    def run_raw(self, *args, **kwargs):\n        before_experiment.send(sender=self)\n\n        seq = self.sequence(*args, **kwargs)\n        is_laboneq_exp = type(seq) == LaboneQExperiment\n        result = None\n\n        if is_laboneq_exp:\n            compiled_exp = compile_experiment(self.zi_session, seq)\n            result = run_experiment(self.zi_session, compiled_exp)\n        else:\n            result = seq\n\n        after_experiment.send(sender=self)\n\n        return result\n\n    def sweep_around(\n        self,\n        center: str | float,\n        span: float | tuple[float, float],\n        n_points: int = None,\n        step: float = None,\n        scale: str = \"linear\",\n        qu_uid=\"q0\",\n    ):\n        \"\"\"\n        Generates a sweep of values around a specified center, either numerically or by referencing\n        a qubit parameter.\n\n        Parameters\n        ----------\n        center : str or float\n            Center of the sweep. If a string, it's interpreted as the name of a qubit parameter\n            and resolved via `qubit_value`. If a float, used directly.\n        span : float or tuple of float\n            If a float, sweep will extend symmetrically by `span` on both sides of `center`.\n            If a tuple `(left, right)`, creates an asymmetric sweep: `center - left` to `center + right`.\n        n_points : int, optional\n            Number of points in the sweep. Specify exactly one of `n_points` or `step`.\n        step : float, optional\n            Step size in the sweep. Specify exactly one of `n_points` or `step`.\n        scale : {'linear', 'log'}, default 'linear'\n            Whether to generate the sweep on a linear or logarithmic scale.\n            For logarithmic sweeps, all generated values must be &gt; 0.\n        qu_uid : str, default \"q0\"\n            Qubit identifier used to resolve `center` if it is a parameter name.\n\n        Returns\n        -------\n        np.ndarray\n            Array of sweep values.\n\n        Raises\n        ------\n        AttributeError\n            If `center` is a string and not found in the qubit's parameter set.\n        ValueError\n            If scale is not one of 'linear' or 'log'.\n            If a log-scale sweep is requested with non-positive start/stop values.\n            If both or neither of `n_points` and `step` are provided.\n\n        Notes\n        -----\n        - For log scale and `step`-based sweeps, the step is interpreted in multiplicative terms,\n          and an approximate number of points is derived.\n        - Sweep boundaries are inclusive when using `step`, thanks to the `+ step / 2` adjustment.\n        \"\"\"\n\n        if isinstance(center, str):\n            value = self.qubit_value(param_id=center, qu_uid=qu_uid)\n            if value is None:\n                raise AttributeError(\n                    f\"No attribute {center} in qubit {qu_uid} parameters.\"\n                )\n            center = value\n\n        # Handle symmetric or asymmetric span\n        if isinstance(span, tuple):\n            left, right = span\n        else:\n            left = right = span\n\n        start = center - left\n        stop = center + right\n\n        if scale not in (\"linear\", \"log\"):\n            raise ValueError(\"scale must be 'linear' or 'log'\")\n\n        if start &lt;= 0 or stop &lt;= 0:\n            if scale == \"log\":\n                raise ValueError(\"Logarithmic sweep requires all values &gt; 0\")\n\n        if (n_points is None) == (step is None):\n            raise ValueError(\"Specify exactly one of 'n_points' or 'step'\")\n\n        if scale == \"linear\":\n            if step is not None:\n                return np.arange(start, stop + step / 2, step)\n            else:\n                return np.linspace(start, stop, n_points)\n\n        else:  # scale == \"log\"\n            if step is not None:\n                # Compute approximate number of points from step in log space\n                log_start = np.log10(start)\n                log_stop = np.log10(stop)\n                num_steps = (\n                    int(np.floor((log_stop - log_start) / np.log10(1 + step / start)))\n                    + 1\n                )\n                return np.logspace(log_start, log_stop, num=num_steps)\n            else:\n                return np.logspace(np.log10(start), np.log10(stop), n_points)\n\n    def qubit_value(self, param_id, qu_uid=\"q0\"):\n        \"\"\"Get a qubit parameter value from the QPU.\"\"\"\n        params = self.qpu.quantum_element_by_uid(qu_uid).parameters\n        return attrs.asdict(params).get(param_id)\n</code></pre>"},{"location":"API%20reference/experiment/handler/#sqil_core.experiment._experiment.ExperimentHandler.qubit_value","title":"<code>qubit_value(param_id, qu_uid='q0')</code>","text":"<p>Get a qubit parameter value from the QPU.</p> Source code in <code>sqil_core/experiment/_experiment.py</code> <pre><code>def qubit_value(self, param_id, qu_uid=\"q0\"):\n    \"\"\"Get a qubit parameter value from the QPU.\"\"\"\n    params = self.qpu.quantum_element_by_uid(qu_uid).parameters\n    return attrs.asdict(params).get(param_id)\n</code></pre>"},{"location":"API%20reference/experiment/handler/#sqil_core.experiment._experiment.ExperimentHandler.sequence","title":"<code>sequence(*args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Experimental sequence defined by the user</p> Source code in <code>sqil_core/experiment/_experiment.py</code> <pre><code>@abstractmethod\ndef sequence(self, *args, **kwargs):\n    \"\"\"Experimental sequence defined by the user\"\"\"\n    pass\n</code></pre>"},{"location":"API%20reference/experiment/handler/#sqil_core.experiment._experiment.ExperimentHandler.sweep_around","title":"<code>sweep_around(center, span, n_points=None, step=None, scale='linear', qu_uid='q0')</code>","text":"<p>Generates a sweep of values around a specified center, either numerically or by referencing a qubit parameter.</p> <p>Parameters:</p> Name Type Description Default <code>center</code> <code>str or float</code> <p>Center of the sweep. If a string, it's interpreted as the name of a qubit parameter and resolved via <code>qubit_value</code>. If a float, used directly.</p> required <code>span</code> <code>float or tuple of float</code> <p>If a float, sweep will extend symmetrically by <code>span</code> on both sides of <code>center</code>. If a tuple <code>(left, right)</code>, creates an asymmetric sweep: <code>center - left</code> to <code>center + right</code>.</p> required <code>n_points</code> <code>int</code> <p>Number of points in the sweep. Specify exactly one of <code>n_points</code> or <code>step</code>.</p> <code>None</code> <code>step</code> <code>float</code> <p>Step size in the sweep. Specify exactly one of <code>n_points</code> or <code>step</code>.</p> <code>None</code> <code>scale</code> <code>('linear', 'log')</code> <p>Whether to generate the sweep on a linear or logarithmic scale. For logarithmic sweeps, all generated values must be &gt; 0.</p> <code>'linear'</code> <code>qu_uid</code> <code>str</code> <p>Qubit identifier used to resolve <code>center</code> if it is a parameter name.</p> <code>\"q0\"</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Array of sweep values.</p> <p>Raises:</p> Type Description <code>AttributeError</code> <p>If <code>center</code> is a string and not found in the qubit's parameter set.</p> <code>ValueError</code> <p>If scale is not one of 'linear' or 'log'. If a log-scale sweep is requested with non-positive start/stop values. If both or neither of <code>n_points</code> and <code>step</code> are provided.</p> Notes <ul> <li>For log scale and <code>step</code>-based sweeps, the step is interpreted in multiplicative terms,   and an approximate number of points is derived.</li> <li>Sweep boundaries are inclusive when using <code>step</code>, thanks to the <code>+ step / 2</code> adjustment.</li> </ul> Source code in <code>sqil_core/experiment/_experiment.py</code> <pre><code>def sweep_around(\n    self,\n    center: str | float,\n    span: float | tuple[float, float],\n    n_points: int = None,\n    step: float = None,\n    scale: str = \"linear\",\n    qu_uid=\"q0\",\n):\n    \"\"\"\n    Generates a sweep of values around a specified center, either numerically or by referencing\n    a qubit parameter.\n\n    Parameters\n    ----------\n    center : str or float\n        Center of the sweep. If a string, it's interpreted as the name of a qubit parameter\n        and resolved via `qubit_value`. If a float, used directly.\n    span : float or tuple of float\n        If a float, sweep will extend symmetrically by `span` on both sides of `center`.\n        If a tuple `(left, right)`, creates an asymmetric sweep: `center - left` to `center + right`.\n    n_points : int, optional\n        Number of points in the sweep. Specify exactly one of `n_points` or `step`.\n    step : float, optional\n        Step size in the sweep. Specify exactly one of `n_points` or `step`.\n    scale : {'linear', 'log'}, default 'linear'\n        Whether to generate the sweep on a linear or logarithmic scale.\n        For logarithmic sweeps, all generated values must be &gt; 0.\n    qu_uid : str, default \"q0\"\n        Qubit identifier used to resolve `center` if it is a parameter name.\n\n    Returns\n    -------\n    np.ndarray\n        Array of sweep values.\n\n    Raises\n    ------\n    AttributeError\n        If `center` is a string and not found in the qubit's parameter set.\n    ValueError\n        If scale is not one of 'linear' or 'log'.\n        If a log-scale sweep is requested with non-positive start/stop values.\n        If both or neither of `n_points` and `step` are provided.\n\n    Notes\n    -----\n    - For log scale and `step`-based sweeps, the step is interpreted in multiplicative terms,\n      and an approximate number of points is derived.\n    - Sweep boundaries are inclusive when using `step`, thanks to the `+ step / 2` adjustment.\n    \"\"\"\n\n    if isinstance(center, str):\n        value = self.qubit_value(param_id=center, qu_uid=qu_uid)\n        if value is None:\n            raise AttributeError(\n                f\"No attribute {center} in qubit {qu_uid} parameters.\"\n            )\n        center = value\n\n    # Handle symmetric or asymmetric span\n    if isinstance(span, tuple):\n        left, right = span\n    else:\n        left = right = span\n\n    start = center - left\n    stop = center + right\n\n    if scale not in (\"linear\", \"log\"):\n        raise ValueError(\"scale must be 'linear' or 'log'\")\n\n    if start &lt;= 0 or stop &lt;= 0:\n        if scale == \"log\":\n            raise ValueError(\"Logarithmic sweep requires all values &gt; 0\")\n\n    if (n_points is None) == (step is None):\n        raise ValueError(\"Specify exactly one of 'n_points' or 'step'\")\n\n    if scale == \"linear\":\n        if step is not None:\n            return np.arange(start, stop + step / 2, step)\n        else:\n            return np.linspace(start, stop, n_points)\n\n    else:  # scale == \"log\"\n        if step is not None:\n            # Compute approximate number of points from step in log space\n            log_start = np.log10(start)\n            log_stop = np.log10(stop)\n            num_steps = (\n                int(np.floor((log_stop - log_start) / np.log10(1 + step / start)))\n                + 1\n            )\n            return np.logspace(log_start, log_stop, num=num_steps)\n        else:\n            return np.logspace(np.log10(start), np.log10(stop), n_points)\n</code></pre>"},{"location":"API%20reference/experiment/handler/#sqil_core.experiment._experiment.Instruments","title":"<code>Instruments</code>","text":"Source code in <code>sqil_core/experiment/_experiment.py</code> <pre><code>class Instruments:\n    def __init__(self, data):\n        self._instruments = data\n        for key, value in data.items():\n            setattr(self, key, value)\n\n    def __iter__(self):\n        \"\"\"Allow iteration directly over instrument instances.\"\"\"\n        return iter(self._instruments.values())\n</code></pre>"},{"location":"API%20reference/experiment/handler/#sqil_core.experiment._experiment.Instruments.__iter__","title":"<code>__iter__()</code>","text":"<p>Allow iteration directly over instrument instances.</p> Source code in <code>sqil_core/experiment/_experiment.py</code> <pre><code>def __iter__(self):\n    \"\"\"Allow iteration directly over instrument instances.\"\"\"\n    return iter(self._instruments.values())\n</code></pre>"},{"location":"API%20reference/experiment/handler/#sqil_core.experiment._experiment.build_plottr_dict","title":"<code>build_plottr_dict(db_schema)</code>","text":"<p>Create a DataDict object from the given schema.</p> Source code in <code>sqil_core/experiment/_experiment.py</code> <pre><code>def build_plottr_dict(db_schema):\n    \"\"\"Create a DataDict object from the given schema.\"\"\"\n    axes = []\n    db = {}\n\n    data_key = \"data\"\n    data_unit = \"\"\n\n    for key, value in db_schema.items():\n        if value.get(\"role\") in (\"axis\", \"x-axis\"):\n            unit = value.get(\"unit\", \"\")\n            db[key] = dict(unit=unit)\n            axes.append(key)\n        elif value.get(\"role\") == \"data\":\n            data_key = key\n            data_unit = value.get(\"unit\", \"\")\n    db[data_key] = dict(axes=axes, unit=data_unit)\n    datadict = DataDict(**db)\n\n    datadict.add_meta(\"schema\", json.dumps(db_schema))\n\n    return datadict\n</code></pre>"},{"location":"API%20reference/experiment/handler/#sqil_core.experiment._experiment.map_inputs","title":"<code>map_inputs(func)</code>","text":"<p>Extracts parameter names and keyword arguments from a function signature.</p> Source code in <code>sqil_core/experiment/_experiment.py</code> <pre><code>def map_inputs(func):\n    \"\"\"Extracts parameter names and keyword arguments from a function signature.\"\"\"\n    sig = inspect.signature(func)\n    params = {}\n    kwargs = []\n\n    for index, (name, param) in enumerate(sig.parameters.items()):\n        if param.default == inspect.Parameter.empty:\n            # Positional or required argument\n            params[name] = index\n        else:\n            # Keyword argument\n            kwargs.append(name)\n\n    return params, kwargs\n</code></pre>"},{"location":"API%20reference/experiment/instruments/","title":"Instruments","text":""},{"location":"API%20reference/experiment/instruments/#sqil_core.experiment.instruments._instrument.Instrument","title":"<code>Instrument</code>","text":"<p>               Bases: <code>FunctionOverrideHandler</code>, <code>ABC</code></p> <p>Base class for instruments with configurable behavior.</p> <p>Supports overriding <code>connect</code>, <code>setup</code>, and <code>disconnect</code> methods via a configuration dictionary.</p> Source code in <code>sqil_core/experiment/instruments/_instrument.py</code> <pre><code>@Pyro5.server.expose\nclass Instrument(FunctionOverrideHandler, ABC):\n    \"\"\"\n    Base class for instruments with configurable behavior.\n\n    Supports overriding `connect`, `setup`, and `disconnect` methods\n    via a configuration dictionary.\n    \"\"\"\n\n    def __init__(self, id: str, config: dict):\n        \"\"\"\n        Initializes the instrument with an ID and configuration.\n\n        If `connect`, `setup`, or `disconnect` are provided in `config`,\n        they override the default implementations.\n        \"\"\"\n        super().__init__()\n\n        self._id = id\n        self._type = config.get(\"type\", \"\")\n        self._model = config.get(\"model\", \"\")\n        self._name = config.get(\"name\", \"\")\n        self._address = config.get(\"address\", \"\")\n        self._variables = config.get(\"variables\", {})\n        self._config = config\n        self._device = None\n\n        self._default_functions = {\n            \"connect\": self._default_connect,\n            \"setup\": self._default_setup,\n            \"disconnect\": self._default_disconnect,\n            \"on_before_experiment\": self._default_on_before_experiment,\n            \"on_after_experiment\": self._default_on_after_experiment,\n            \"on_before_sequence\": self._default_on_before_sequence,\n            \"on_after_sequence\": self._default_on_after_sequence,\n        }\n        self._functions = self._default_functions.copy()\n\n        # Override functions if provided in config\n        for method_name in self._default_functions:\n            if method := config.get(method_name):\n                self.override_function(method_name, method)\n\n        self._default_functions = self._functions.copy()\n        self._device = self.connect()  # Auto-connect on instantiation\n\n        # Subscribe to events\n        self._subscribe_to_events()\n\n    def _subscribe_to_events(self):\n        before_experiment.connect(\n            lambda *a, **kw: self.call(\"on_before_experiment\", *a, **kw), weak=False\n        )\n        before_sequence.connect(\n            lambda *a, **kw: self.call(\"on_before_sequence\", *a, **kw), weak=False\n        )\n        after_sequence.connect(\n            lambda *a, **kw: self.call(\"on_after_sequence\", *a, **kw), weak=False\n        )\n        after_experiment.connect(\n            lambda *a, **kw: self.call(\"on_after_experiment\", *a, **kw), weak=False\n        )\n\n    def connect(self, *args, **kwargs):\n        \"\"\"Calls the overridden or default `connect` method.\"\"\"\n        return self.call(\"connect\", *args, **kwargs)\n\n    @abstractmethod\n    def _default_connect(self, *args, **kwargs):\n        \"\"\"Default `connect` implementation (must be overridden).\"\"\"\n        pass\n\n    def setup(self, *args, **kwargs):\n        \"\"\"Calls the overridden or default `setup` method.\"\"\"\n        return self.call(\"setup\", *args, **kwargs)\n\n    @abstractmethod\n    def _default_setup(self, *args, **kwargs):\n        \"\"\"Default `setup` implementation (must be overridden).\"\"\"\n        pass\n\n    def disconnect(self, *args, **kwargs):\n        \"\"\"Calls the overridden or default `disconnect` method.\"\"\"\n        return self.call(\"disconnect\", *args, **kwargs)\n\n    @abstractmethod\n    def _default_disconnect(self, *args, **kwargs):\n        pass\n\n    def on_before_experiment(self, *args, **kwargs):\n        \"\"\"Calls the overridden or default `on_before_experiment` method.\"\"\"\n        return self.call(\"on_before_experiment\", *args, **kwargs)\n\n    def _default_on_before_experiment(self, *args, **kwargs):\n        pass\n\n    def on_before_sequence(self, *args, **kwargs):\n        \"\"\"Calls the overridden or default `on_before_sequence` method.\"\"\"\n        return self.call(\"on_before_sequence\", *args, **kwargs)\n\n    def _default_on_before_sequence(self, *args, **kwargs):\n        pass\n\n    def on_after_experiment(self, *args, **kwargs):\n        \"\"\"Calls the overridden or default `on_after_experiment` method.\"\"\"\n        return self.call(\"on_after_experiment\", *args, **kwargs)\n\n    def _default_on_after_experiment(self, *args, **kwargs):\n        pass\n\n    def on_after_sequence(self, *args, **kwargs):\n        \"\"\"Calls the overridden or default `on_after_sequence` method.\"\"\"\n        return self.call(\"on_after_sequence\", *args, **kwargs)\n\n    def _default_on_after_sequence(self, *args, **kwargs):\n        pass\n\n    def __getattr__(self, name):\n        \"\"\"\n        Dynamically expose all attributes to Pyro server.\n        \"\"\"\n        if name in self.__dict__:\n            return self.__dict__[name]\n        raise AttributeError(\n            f\"'{self.__class__.__name__}' object has no attribute '{name}'\"\n        )\n\n    def get_variable(self, key, *args, **kwargs):\n        var = self._variables.get(key, None)\n        if callable(var):\n            var = var(*args, **kwargs)\n        return var\n\n    @property\n    def id(self):\n        \"\"\"Instrument ID (read-only).\"\"\"\n        return self._id\n\n    @property\n    def type(self):\n        \"\"\"Instrument type (read-only).\"\"\"\n        return self._type\n\n    @property\n    def model(self):\n        \"\"\"Instrument model (read-only).\"\"\"\n        return self._model\n\n    @property\n    def name(self):\n        \"\"\"Instrument name (read-only).\"\"\"\n        return self._name\n\n    @property\n    def address(self):\n        \"\"\"Instrument address (read-only).\"\"\"\n        return self._address\n\n    @property\n    def variables(self):\n        \"\"\"Instrument variables (read-only).\"\"\"\n        return self._variables\n\n    @property\n    def config(self):\n        \"\"\"Instrument configuration dictionary (read-only).\"\"\"\n        return self._config\n\n    @property\n    def device(self):\n        \"\"\"Raw instrument instance (read-only).\"\"\"\n        return self._device\n</code></pre>"},{"location":"API%20reference/experiment/instruments/#sqil_core.experiment.instruments._instrument.Instrument.address","title":"<code>address</code>  <code>property</code>","text":"<p>Instrument address (read-only).</p>"},{"location":"API%20reference/experiment/instruments/#sqil_core.experiment.instruments._instrument.Instrument.config","title":"<code>config</code>  <code>property</code>","text":"<p>Instrument configuration dictionary (read-only).</p>"},{"location":"API%20reference/experiment/instruments/#sqil_core.experiment.instruments._instrument.Instrument.device","title":"<code>device</code>  <code>property</code>","text":"<p>Raw instrument instance (read-only).</p>"},{"location":"API%20reference/experiment/instruments/#sqil_core.experiment.instruments._instrument.Instrument.id","title":"<code>id</code>  <code>property</code>","text":"<p>Instrument ID (read-only).</p>"},{"location":"API%20reference/experiment/instruments/#sqil_core.experiment.instruments._instrument.Instrument.model","title":"<code>model</code>  <code>property</code>","text":"<p>Instrument model (read-only).</p>"},{"location":"API%20reference/experiment/instruments/#sqil_core.experiment.instruments._instrument.Instrument.name","title":"<code>name</code>  <code>property</code>","text":"<p>Instrument name (read-only).</p>"},{"location":"API%20reference/experiment/instruments/#sqil_core.experiment.instruments._instrument.Instrument.type","title":"<code>type</code>  <code>property</code>","text":"<p>Instrument type (read-only).</p>"},{"location":"API%20reference/experiment/instruments/#sqil_core.experiment.instruments._instrument.Instrument.variables","title":"<code>variables</code>  <code>property</code>","text":"<p>Instrument variables (read-only).</p>"},{"location":"API%20reference/experiment/instruments/#sqil_core.experiment.instruments._instrument.Instrument.__getattr__","title":"<code>__getattr__(name)</code>","text":"<p>Dynamically expose all attributes to Pyro server.</p> Source code in <code>sqil_core/experiment/instruments/_instrument.py</code> <pre><code>def __getattr__(self, name):\n    \"\"\"\n    Dynamically expose all attributes to Pyro server.\n    \"\"\"\n    if name in self.__dict__:\n        return self.__dict__[name]\n    raise AttributeError(\n        f\"'{self.__class__.__name__}' object has no attribute '{name}'\"\n    )\n</code></pre>"},{"location":"API%20reference/experiment/instruments/#sqil_core.experiment.instruments._instrument.Instrument.__init__","title":"<code>__init__(id, config)</code>","text":"<p>Initializes the instrument with an ID and configuration.</p> <p>If <code>connect</code>, <code>setup</code>, or <code>disconnect</code> are provided in <code>config</code>, they override the default implementations.</p> Source code in <code>sqil_core/experiment/instruments/_instrument.py</code> <pre><code>def __init__(self, id: str, config: dict):\n    \"\"\"\n    Initializes the instrument with an ID and configuration.\n\n    If `connect`, `setup`, or `disconnect` are provided in `config`,\n    they override the default implementations.\n    \"\"\"\n    super().__init__()\n\n    self._id = id\n    self._type = config.get(\"type\", \"\")\n    self._model = config.get(\"model\", \"\")\n    self._name = config.get(\"name\", \"\")\n    self._address = config.get(\"address\", \"\")\n    self._variables = config.get(\"variables\", {})\n    self._config = config\n    self._device = None\n\n    self._default_functions = {\n        \"connect\": self._default_connect,\n        \"setup\": self._default_setup,\n        \"disconnect\": self._default_disconnect,\n        \"on_before_experiment\": self._default_on_before_experiment,\n        \"on_after_experiment\": self._default_on_after_experiment,\n        \"on_before_sequence\": self._default_on_before_sequence,\n        \"on_after_sequence\": self._default_on_after_sequence,\n    }\n    self._functions = self._default_functions.copy()\n\n    # Override functions if provided in config\n    for method_name in self._default_functions:\n        if method := config.get(method_name):\n            self.override_function(method_name, method)\n\n    self._default_functions = self._functions.copy()\n    self._device = self.connect()  # Auto-connect on instantiation\n\n    # Subscribe to events\n    self._subscribe_to_events()\n</code></pre>"},{"location":"API%20reference/experiment/instruments/#sqil_core.experiment.instruments._instrument.Instrument.connect","title":"<code>connect(*args, **kwargs)</code>","text":"<p>Calls the overridden or default <code>connect</code> method.</p> Source code in <code>sqil_core/experiment/instruments/_instrument.py</code> <pre><code>def connect(self, *args, **kwargs):\n    \"\"\"Calls the overridden or default `connect` method.\"\"\"\n    return self.call(\"connect\", *args, **kwargs)\n</code></pre>"},{"location":"API%20reference/experiment/instruments/#sqil_core.experiment.instruments._instrument.Instrument.disconnect","title":"<code>disconnect(*args, **kwargs)</code>","text":"<p>Calls the overridden or default <code>disconnect</code> method.</p> Source code in <code>sqil_core/experiment/instruments/_instrument.py</code> <pre><code>def disconnect(self, *args, **kwargs):\n    \"\"\"Calls the overridden or default `disconnect` method.\"\"\"\n    return self.call(\"disconnect\", *args, **kwargs)\n</code></pre>"},{"location":"API%20reference/experiment/instruments/#sqil_core.experiment.instruments._instrument.Instrument.on_after_experiment","title":"<code>on_after_experiment(*args, **kwargs)</code>","text":"<p>Calls the overridden or default <code>on_after_experiment</code> method.</p> Source code in <code>sqil_core/experiment/instruments/_instrument.py</code> <pre><code>def on_after_experiment(self, *args, **kwargs):\n    \"\"\"Calls the overridden or default `on_after_experiment` method.\"\"\"\n    return self.call(\"on_after_experiment\", *args, **kwargs)\n</code></pre>"},{"location":"API%20reference/experiment/instruments/#sqil_core.experiment.instruments._instrument.Instrument.on_after_sequence","title":"<code>on_after_sequence(*args, **kwargs)</code>","text":"<p>Calls the overridden or default <code>on_after_sequence</code> method.</p> Source code in <code>sqil_core/experiment/instruments/_instrument.py</code> <pre><code>def on_after_sequence(self, *args, **kwargs):\n    \"\"\"Calls the overridden or default `on_after_sequence` method.\"\"\"\n    return self.call(\"on_after_sequence\", *args, **kwargs)\n</code></pre>"},{"location":"API%20reference/experiment/instruments/#sqil_core.experiment.instruments._instrument.Instrument.on_before_experiment","title":"<code>on_before_experiment(*args, **kwargs)</code>","text":"<p>Calls the overridden or default <code>on_before_experiment</code> method.</p> Source code in <code>sqil_core/experiment/instruments/_instrument.py</code> <pre><code>def on_before_experiment(self, *args, **kwargs):\n    \"\"\"Calls the overridden or default `on_before_experiment` method.\"\"\"\n    return self.call(\"on_before_experiment\", *args, **kwargs)\n</code></pre>"},{"location":"API%20reference/experiment/instruments/#sqil_core.experiment.instruments._instrument.Instrument.on_before_sequence","title":"<code>on_before_sequence(*args, **kwargs)</code>","text":"<p>Calls the overridden or default <code>on_before_sequence</code> method.</p> Source code in <code>sqil_core/experiment/instruments/_instrument.py</code> <pre><code>def on_before_sequence(self, *args, **kwargs):\n    \"\"\"Calls the overridden or default `on_before_sequence` method.\"\"\"\n    return self.call(\"on_before_sequence\", *args, **kwargs)\n</code></pre>"},{"location":"API%20reference/experiment/instruments/#sqil_core.experiment.instruments._instrument.Instrument.setup","title":"<code>setup(*args, **kwargs)</code>","text":"<p>Calls the overridden or default <code>setup</code> method.</p> Source code in <code>sqil_core/experiment/instruments/_instrument.py</code> <pre><code>def setup(self, *args, **kwargs):\n    \"\"\"Calls the overridden or default `setup` method.\"\"\"\n    return self.call(\"setup\", *args, **kwargs)\n</code></pre>"},{"location":"API%20reference/experiment/instruments/#sqil_core.experiment.instruments.zurich_instruments.ZI_Instrument","title":"<code>ZI_Instrument</code>","text":"<p>               Bases: <code>Instrument</code></p> Source code in <code>sqil_core/experiment/instruments/zurich_instruments.py</code> <pre><code>class ZI_Instrument(Instrument):\n    _descriptor = \"\"\n    _generate_setup = None\n    _generate_qpu = None\n\n    def __init__(self, id, config):\n        super().__init__(id, config)\n        self._descriptor = config.get(\"descriptor\", \"\")\n\n        self._generate_setup = config.get(\"generate_setup\", None)\n        if not self._generate_setup:\n            raise NotImplementedError(\n                \"get_setup is not implemented in your setup file.\\n\"\n                + \"You should define it as part of the zi section of your instruments dictionary.\\n\"\n                + \"instruments['zi']['generate_setup']\"\n            )\n\n        self._generate_qpu = config.get(\"generate_qpu\", None)\n        if not self._generate_qpu:\n            raise NotImplementedError(\n                \"get_qpu is not implemented in your setup file.\\n\"\n                + \"You should define it as part of the zi section of your instruments dictionary.\\n\"\n                + \"instruments['zi']['generate_qpu']\"\n            )\n\n    def generate_setup(self, *params, **kwargs) -&gt; DeviceSetup:\n        return self._generate_setup(*params, **kwargs)\n\n    def generate_qpu(self, *params, **kwargs) -&gt; QPU:\n        return self._generate_qpu(*params, **kwargs)\n\n    def _default_connect(self):\n        pass\n        # setup = self.config.get(\"setup_obj\", None)\n        # if setup is not None:\n        #     self._session = Session(setup)\n        #     return self.session\n        # raise \"Zuirch instruments needs a 'setup_obj' field in your setup file\"\n\n    def _default_setup(self):\n        pass\n\n    def _default_disconnect(self):\n        pass\n\n    @property\n    def descriptor(self):\n        \"\"\"LaboneQ descriptor (read-only) - deprecated.\"\"\"\n        return self._descriptor\n</code></pre>"},{"location":"API%20reference/experiment/instruments/#sqil_core.experiment.instruments.zurich_instruments.ZI_Instrument.descriptor","title":"<code>descriptor</code>  <code>property</code>","text":"<p>LaboneQ descriptor (read-only) - deprecated.</p>"},{"location":"API%20reference/experiment/instruments/#sqil_core.experiment.instruments.local_oscillator.LocalOscillatorBase","title":"<code>LocalOscillatorBase</code>","text":"<p>               Bases: <code>Instrument</code>, <code>ABC</code></p> Source code in <code>sqil_core/experiment/instruments/local_oscillator.py</code> <pre><code>class LocalOscillatorBase(Instrument, ABC):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n    @abstractmethod\n    def set_frequency(self, value) -&gt; None:\n        \"\"\"Set the frequency of the local oscillator.\"\"\"\n        pass\n\n    @abstractmethod\n    def set_power(self, value) -&gt; None:\n        \"\"\"Set the power of the local oscillator.\"\"\"\n        pass\n\n    @abstractmethod\n    def turn_on(self) -&gt; None:\n        \"\"\"Turn the local oscillator on.\"\"\"\n        pass\n\n    @abstractmethod\n    def turn_off(self) -&gt; None:\n        \"\"\"Turn the local oscillator off.\"\"\"\n        pass\n</code></pre>"},{"location":"API%20reference/experiment/instruments/#sqil_core.experiment.instruments.local_oscillator.LocalOscillatorBase.set_frequency","title":"<code>set_frequency(value)</code>  <code>abstractmethod</code>","text":"<p>Set the frequency of the local oscillator.</p> Source code in <code>sqil_core/experiment/instruments/local_oscillator.py</code> <pre><code>@abstractmethod\ndef set_frequency(self, value) -&gt; None:\n    \"\"\"Set the frequency of the local oscillator.\"\"\"\n    pass\n</code></pre>"},{"location":"API%20reference/experiment/instruments/#sqil_core.experiment.instruments.local_oscillator.LocalOscillatorBase.set_power","title":"<code>set_power(value)</code>  <code>abstractmethod</code>","text":"<p>Set the power of the local oscillator.</p> Source code in <code>sqil_core/experiment/instruments/local_oscillator.py</code> <pre><code>@abstractmethod\ndef set_power(self, value) -&gt; None:\n    \"\"\"Set the power of the local oscillator.\"\"\"\n    pass\n</code></pre>"},{"location":"API%20reference/experiment/instruments/#sqil_core.experiment.instruments.local_oscillator.LocalOscillatorBase.turn_off","title":"<code>turn_off()</code>  <code>abstractmethod</code>","text":"<p>Turn the local oscillator off.</p> Source code in <code>sqil_core/experiment/instruments/local_oscillator.py</code> <pre><code>@abstractmethod\ndef turn_off(self) -&gt; None:\n    \"\"\"Turn the local oscillator off.\"\"\"\n    pass\n</code></pre>"},{"location":"API%20reference/experiment/instruments/#sqil_core.experiment.instruments.local_oscillator.LocalOscillatorBase.turn_on","title":"<code>turn_on()</code>  <code>abstractmethod</code>","text":"<p>Turn the local oscillator on.</p> Source code in <code>sqil_core/experiment/instruments/local_oscillator.py</code> <pre><code>@abstractmethod\ndef turn_on(self) -&gt; None:\n    \"\"\"Turn the local oscillator on.\"\"\"\n    pass\n</code></pre>"},{"location":"API%20reference/experiment/instruments/#sqil_core.experiment.instruments.local_oscillator.SqilRohdeSchwarzSGS100A","title":"<code>SqilRohdeSchwarzSGS100A</code>","text":"<p>               Bases: <code>LocalOscillatorBase</code></p> <p>Frequency:     [1 MHz, 20 GHz], resolution 0.001 Hz Power:     [-120 dB, 25 dB], resolution 0.01 dB</p> Source code in <code>sqil_core/experiment/instruments/local_oscillator.py</code> <pre><code>class SqilRohdeSchwarzSGS100A(LocalOscillatorBase):\n    \"\"\"\n    Frequency:\n        [1 MHz, 20 GHz], resolution 0.001 Hz\n    Power:\n        [-120 dB, 25 dB], resolution 0.01 dB\n    \"\"\"\n\n    def _default_connect(self, *args, **kwargs):\n        logger.info(f\"Connecting to {self.name} ({self.model})\")\n        return RohdeSchwarzSGS100A(self.name, self.address)\n\n    def _default_disconnect(self, *args, **kwargs):\n        logger.info(f\"Disconnecting from {self.name} ({self.model})\")\n        self.turn_off()\n\n    def _default_setup(self, *args, **kwargs):\n        logger.info(f\"Setting up {self.name}\")\n        self.turn_off()\n        self.set_power(-60)\n\n    def set_frequency(self, value) -&gt; None:\n        self.device.frequency(value)\n\n    def set_power(self, value) -&gt; None:\n        self.device.power(value)\n\n    def turn_on(self) -&gt; None:\n        self.device.on()\n\n    def turn_off(self) -&gt; None:\n        self.device.off()\n</code></pre>"},{"location":"API%20reference/experiment/instruments/#sqil_core.experiment.instruments.local_oscillator.SqilSignalCoreSC5511A","title":"<code>SqilSignalCoreSC5511A</code>","text":"<p>               Bases: <code>LocalOscillatorBase</code></p> <p>PORT 1 specifications Frequency:     [100 MHz, 20 GHz], resolution 1 Hz Power:     @ freq &lt; 18 GHz: [-20 dBm, 15 dBm], resolution 0.01 dBm     @ freq &gt; 18 GHz: [-20 dBm, 10 dBm], resolution 0.01 dBm</p> Source code in <code>sqil_core/experiment/instruments/local_oscillator.py</code> <pre><code>class SqilSignalCoreSC5511A(LocalOscillatorBase):\n    \"\"\"\n    PORT 1 specifications\n    Frequency:\n        [100 MHz, 20 GHz], resolution 1 Hz\n    Power:\n        @ freq &lt; 18 GHz: [-20 dBm, 15 dBm], resolution 0.01 dBm\n        @ freq &gt; 18 GHz: [-20 dBm, 10 dBm], resolution 0.01 dBm\n    \"\"\"\n\n    def _default_connect(self, *args, **kwargs):\n        logger.info(f\"Connecting to {self.name} ({self.model})\")\n        return SignalCore_SC5511A(self.name, self.address)\n\n    def _default_disconnect(self, *args, **kwargs):\n        logger.info(f\"Disconnecting from {self.name} ({self.model})\")\n        self.turn_off()\n\n    def _default_setup(self, *args, **kwargs):\n        logger.info(f\"Setting up {self.name}\")\n        self.turn_off()\n        self.set_power(-40)\n        self.device.do_set_reference_source(1)  # to enable phase locking\n        self.device.do_set_standby(True)  # update PLL locking\n        self.device.do_set_standby(False)\n\n    def set_frequency(self, value) -&gt; None:\n        self.device.do_set_ref_out_freq(value)\n\n    def set_power(self, value) -&gt; None:\n        self.device.power(value)\n\n    def turn_on(self) -&gt; None:\n        self.device.do_set_output_status(1)\n\n    def turn_off(self) -&gt; None:\n        self.device.do_set_output_status(0)\n</code></pre>"},{"location":"API%20reference/experiment/instruments/#sqil_core.experiment.instruments.local_oscillator.SqilSignalCoreSC5521A","title":"<code>SqilSignalCoreSC5521A</code>","text":"<p>               Bases: <code>LocalOscillatorBase</code></p> <p>Frequency:     [160 MHz, 40 GHz], resolution 1 Hz Power:     @ freq &lt; 30 GHz: [-10 dBm, 15 dBm], resolution 0.1 dBm     @ freq &lt; 35 GHz: [-10 dBm, 10 dBm], resolution 0.1 dBm     @ freq &gt; 35 GHz: [-10 dBm, 3 dBm], resolution 0.1 dBm</p> Source code in <code>sqil_core/experiment/instruments/local_oscillator.py</code> <pre><code>class SqilSignalCoreSC5521A(LocalOscillatorBase):\n    \"\"\"\n    Frequency:\n        [160 MHz, 40 GHz], resolution 1 Hz\n    Power:\n        @ freq &lt; 30 GHz: [-10 dBm, 15 dBm], resolution 0.1 dBm\n        @ freq &lt; 35 GHz: [-10 dBm, 10 dBm], resolution 0.1 dBm\n        @ freq &gt; 35 GHz: [-10 dBm, 3 dBm], resolution 0.1 dBm\n    \"\"\"\n\n    def _default_connect(self, *args, **kwargs):\n        logger.info(f\"Connecting to {self.name} ({self.model})\")\n        return SC5521A(self.name)\n\n    def _default_disconnect(self, *args, **kwargs):\n        logger.info(f\"Disconnecting from {self.name} ({self.model})\")\n        self.turn_off()\n\n    def _default_setup(self, *args, **kwargs):\n        logger.info(f\"Setting up {self.name}\")\n        self.turn_off()\n        self.set_power(-40)\n\n    def set_frequency(self, value) -&gt; None:\n        self.device.clock_frequency(value)\n\n    def set_power(self, value) -&gt; None:\n        self.device.power(value)\n\n    def turn_on(self) -&gt; None:\n        self.device.status(\"on\")\n\n    def turn_off(self) -&gt; None:\n        self.device.status(\"off\")\n</code></pre>"},{"location":"API%20reference/fit/core/","title":"Core","text":""},{"location":"API%20reference/fit/core/#sqil_core.fit._core.FitResult","title":"<code>FitResult</code>","text":"<p>Stores the result of a fitting procedure.</p> <p>This class encapsulates the fitted parameters, their standard errors, optimizer output, and fit quality metrics. It also provides functionality for summarizing the results and making predictions using the fitted model.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>Array of fitted parameters.</p> required <code>std_err</code> <code>dict</code> <p>Array of standard errors of the fitted parameters.</p> required <code>fit_output</code> <code>any</code> <p>Raw output from the optimization routine.</p> required <code>metrics</code> <code>dict</code> <p>Dictionary of fit quality metrics (e.g., R-squared, reduced chi-squared).</p> <code>{}</code> <code>predict</code> <code>callable</code> <p>Function of x that returns predictions based on the fitted parameters. If not provided, an exception will be raised when calling it.</p> <code>None</code> <code>param_names</code> <code>list</code> <p>List of parameter names, defaulting to a range based on the number of parameters.</p> <code>None</code> <code>model_name</code> <code>str</code> <p>Name of the model used to fit the data.</p> <code>None</code> <code>metadata</code> <code>dict</code> <p>Additional information that can be passed in the fit result.</p> <code>{}</code> <p>Methods:</p> Name Description <code>summary</code> <p>Prints a detailed summary of the fit results, including parameter values, standard errors, and fit quality metrics.</p> <code>_no_prediction</code> <p>Raises an exception when no prediction function is available.</p> Source code in <code>sqil_core/fit/_core.py</code> <pre><code>class FitResult:\n    \"\"\"\n    Stores the result of a fitting procedure.\n\n    This class encapsulates the fitted parameters, their standard errors, optimizer output,\n    and fit quality metrics. It also provides functionality for summarizing the results and\n    making predictions using the fitted model.\n\n    Parameters\n    ----------\n    params : dict\n        Array of fitted parameters.\n    std_err : dict\n        Array of standard errors of the fitted parameters.\n    fit_output : any\n        Raw output from the optimization routine.\n    metrics : dict, optional\n        Dictionary of fit quality metrics (e.g., R-squared, reduced chi-squared).\n    predict : callable, optional\n        Function of x that returns predictions based on the fitted parameters.\n        If not provided, an exception will be raised when calling it.\n    param_names : list, optional\n        List of parameter names, defaulting to a range based on the number of parameters.\n    model_name : str, optional\n        Name of the model used to fit the data.\n    metadata : dict, optional\n        Additional information that can be passed in the fit result.\n\n    Methods\n    -------\n    summary()\n        Prints a detailed summary of the fit results, including parameter values,\n        standard errors, and fit quality metrics.\n    _no_prediction()\n        Raises an exception when no prediction function is available.\n    \"\"\"\n\n    def __init__(\n        self,\n        params,\n        std_err,\n        fit_output,\n        metrics={},\n        predict=None,\n        param_names=None,\n        model_name=None,\n        metadata={},\n    ):\n        self.params = params\n        self.std_err = std_err\n        self.output = fit_output\n        self.metrics = metrics\n        self.predict = predict or self._no_prediction\n        self.param_names = param_names or list(range(len(params)))\n        self.model_name = model_name\n        self.metadata = metadata\n\n        self.params_by_name = dict(zip(self.param_names, self.params))\n\n    def __repr__(self):\n        return (\n            f\"FitResult(\\n\"\n            f\"  params={self.params},\\n\"\n            f\"  std_err={self.std_err},\\n\"\n            f\"  metrics={self.metrics}\\n)\"\n        )\n\n    def summary(self, no_print=False):\n        \"\"\"Prints a detailed summary of the fit results.\"\"\"\n        s = format_fit_metrics(self.metrics) + \"\\n\"\n        s += format_fit_params(\n            self.param_names,\n            self.params,\n            self.std_err,\n            np.array(self.std_err) / self.params * 100,\n        )\n        if not no_print:\n            print(s)\n        return s\n\n    def quality(self, recipe=\"nrmse\"):\n        return evaluate_fit_quality(self.metrics, recipe)\n\n    def is_acceptable(self, recipe=\"nrmse\", threshold=FitQuality.ACCEPTABLE):\n        return self.quality(recipe) &gt;= threshold\n\n    def _no_prediction(self):\n        raise Exception(\"No predition function available\")\n</code></pre>"},{"location":"API%20reference/fit/core/#sqil_core.fit._core.FitResult.summary","title":"<code>summary(no_print=False)</code>","text":"<p>Prints a detailed summary of the fit results.</p> Source code in <code>sqil_core/fit/_core.py</code> <pre><code>def summary(self, no_print=False):\n    \"\"\"Prints a detailed summary of the fit results.\"\"\"\n    s = format_fit_metrics(self.metrics) + \"\\n\"\n    s += format_fit_params(\n        self.param_names,\n        self.params,\n        self.std_err,\n        np.array(self.std_err) / self.params * 100,\n    )\n    if not no_print:\n        print(s)\n    return s\n</code></pre>"},{"location":"API%20reference/fit/core/#sqil_core.fit._core.compute_adjusted_standard_errors","title":"<code>compute_adjusted_standard_errors(pcov, residuals, red_chi2=None, cov_rescaled=True, sigma=None)</code>","text":"<p>Compute adjusted standard errors for fitted parameters.</p> <p>This function adjusts the covariance matrix based on the reduced chi-squared value and calculates the standard errors for each parameter. It accounts for cases where the covariance matrix is not available or the fit is nearly perfect.</p> <p>Parameters:</p> Name Type Description Default <code>pcov</code> <code>ndarray</code> <p>Covariance matrix of the fitted parameters, typically obtained from an optimization routine.</p> required <code>residuals</code> <code>ndarray</code> <p>Residuals of the fit, defined as the difference between observed and model-predicted values.</p> required <code>red_chi2</code> <code>float</code> <p>Precomputed reduced chi-squared value. If <code>None</code>, it is computed from <code>residuals</code> and <code>sigma</code>.</p> <code>None</code> <code>cov_rescaled</code> <code>bool</code> <p>Whether the fitting process already rescales the covariance matrix with the reduced chi-squared.</p> <code>True</code> <code>sigma</code> <code>ndarray</code> <p>Experimental uncertainties. Only used if <code>cov_rescaled=False</code> AND known experimental errors are available.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Standard errors for each fitted parameter. If the covariance matrix is undefined, returns <code>None</code>.</p> Warnings <ul> <li>If the covariance matrix is not available (<code>pcov is None</code>), the function   issues a warning about possible numerical instability or a near-perfect fit.</li> <li>If the reduced chi-squared value is <code>NaN</code>, the function returns <code>NaN</code> for   all standard errors.</li> </ul> Notes <ul> <li>The covariance matrix is scaled by the reduced chi-squared value to adjust   for under- or overestimation of uncertainties.</li> <li>If <code>red_chi2</code> is not provided, it is computed internally using the residuals.</li> <li>If a near-perfect fit is detected (all residuals close to zero), the function   warns that standard errors may not be necessary.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; pcov = np.array([[0.04, 0.01], [0.01, 0.09]])\n&gt;&gt;&gt; residuals = np.array([0.1, -0.2, 0.15])\n&gt;&gt;&gt; compute_adjusted_standard_errors(pcov, residuals)\narray([0.2, 0.3])\n</code></pre> Source code in <code>sqil_core/fit/_core.py</code> <pre><code>def compute_adjusted_standard_errors(\n    pcov: np.ndarray,\n    residuals: np.ndarray,\n    red_chi2=None,\n    cov_rescaled=True,\n    sigma=None,\n) -&gt; np.ndarray:\n    \"\"\"\n    Compute adjusted standard errors for fitted parameters.\n\n    This function adjusts the covariance matrix based on the reduced chi-squared\n    value and calculates the standard errors for each parameter. It accounts for\n    cases where the covariance matrix is not available or the fit is nearly perfect.\n\n    Parameters\n    ----------\n    pcov : np.ndarray\n        Covariance matrix of the fitted parameters, typically obtained from an\n        optimization routine.\n    residuals : np.ndarray\n        Residuals of the fit, defined as the difference between observed and\n        model-predicted values.\n    red_chi2 : float, optional\n        Precomputed reduced chi-squared value. If `None`, it is computed from\n        `residuals` and `sigma`.\n    cov_rescaled : bool, default=True\n        Whether the fitting process already rescales the covariance matrix with\n        the reduced chi-squared.\n    sigma : np.ndarray, optional\n        Experimental uncertainties. Only used if `cov_rescaled=False` AND\n        known experimental errors are available.\n\n    Returns\n    -------\n    np.ndarray\n        Standard errors for each fitted parameter. If the covariance matrix is\n        undefined, returns `None`.\n\n    Warnings\n    --------\n    - If the covariance matrix is not available (`pcov is None`), the function\n      issues a warning about possible numerical instability or a near-perfect fit.\n    - If the reduced chi-squared value is `NaN`, the function returns `NaN` for\n      all standard errors.\n\n    Notes\n    -----\n    - The covariance matrix is scaled by the reduced chi-squared value to adjust\n      for under- or overestimation of uncertainties.\n    - If `red_chi2` is not provided, it is computed internally using the residuals.\n    - If a near-perfect fit is detected (all residuals close to zero), the function\n      warns that standard errors may not be necessary.\n\n    Examples\n    --------\n    &gt;&gt;&gt; pcov = np.array([[0.04, 0.01], [0.01, 0.09]])\n    &gt;&gt;&gt; residuals = np.array([0.1, -0.2, 0.15])\n    &gt;&gt;&gt; compute_adjusted_standard_errors(pcov, residuals)\n    array([0.2, 0.3])\n    \"\"\"\n    # Check for invalid covariance\n    if pcov is None:\n        if np.allclose(residuals, 0, atol=1e-10):\n            warnings.warn(\n                \"Covariance matrix could not be estimated due to an almost perfect fit. \"\n                \"Standard errors are undefined but may not be necessary in this case.\"\n            )\n        else:\n            warnings.warn(\n                \"Covariance matrix could not be estimated. This could be due to poor model fit \"\n                \"or numerical instability. Review the data or model configuration.\"\n            )\n        return None\n\n    # Calculate reduced chi-squared\n    n_params = len(np.diag(pcov))\n    if red_chi2 is None:\n        _, red_chi2 = compute_chi2(\n            residuals, n_params, cov_rescaled=cov_rescaled, sigma=sigma\n        )\n\n    # Rescale the covariance matrix\n    if np.isnan(red_chi2):\n        pcov_rescaled = np.nan\n    else:\n        pcov_rescaled = pcov * red_chi2\n\n    # Calculate standard errors for each parameter\n    if np.any(np.isnan(pcov_rescaled)):\n        standard_errors = np.full(n_params, np.nan, dtype=float)\n    else:\n        standard_errors = np.sqrt(np.diag(pcov_rescaled))\n\n    return standard_errors\n</code></pre>"},{"location":"API%20reference/fit/core/#sqil_core.fit._core.compute_aic","title":"<code>compute_aic(residuals, n_params)</code>","text":"<p>Computes the Akaike Information Criterion (AIC) for a given model fit.</p> <p>The AIC is a metric used to compare the relative quality of statistical models for a given dataset. It balances model fit with complexity, penalizing models with more parameters to prevent overfitting.</p> <p>Interpretation: The AIC has no maeaning on its own, only the difference between the AIC of model1 and the one of model2. \u0394AIC = AIC_1 - AIC_2 If \u0394AIC &gt; 10 -&gt; model 2 fits much better.</p> <p>Parameters:</p> Name Type Description Default <code>residuals</code> <code>ndarray</code> <p>Array of residuals between the observed data and model predictions.</p> required <code>n_params</code> <code>int</code> <p>Number of free parameters in the fitted model.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The Akaike Information Criterion value.</p> Source code in <code>sqil_core/fit/_core.py</code> <pre><code>def compute_aic(residuals: np.ndarray, n_params: int) -&gt; float:\n    \"\"\"\n    Computes the Akaike Information Criterion (AIC) for a given model fit.\n\n    The AIC is a metric used to compare the relative quality of statistical models\n    for a given dataset. It balances model fit with complexity, penalizing models\n    with more parameters to prevent overfitting.\n\n    Interpretation: The AIC has no maeaning on its own, only the difference between\n    the AIC of model1 and the one of model2.\n    \u0394AIC = AIC_1 - AIC_2\n    If \u0394AIC &gt; 10 -&gt; model 2 fits much better.\n\n    Parameters\n    ----------\n    residuals : np.ndarray\n        Array of residuals between the observed data and model predictions.\n    n_params : int\n        Number of free parameters in the fitted model.\n\n    Returns\n    -------\n    float\n        The Akaike Information Criterion value.\n    \"\"\"\n\n    n = len(residuals)\n    rss = np.sum(residuals**2)\n    return 2 * n_params + n * np.log(rss / n)\n</code></pre>"},{"location":"API%20reference/fit/core/#sqil_core.fit._core.compute_chi2","title":"<code>compute_chi2(residuals, n_params=None, cov_rescaled=True, sigma=None)</code>","text":"<p>Compute the chi-squared (\u03c7\u00b2) and reduced chi-squared (\u03c7\u00b2_red) statistics.</p> <p>This function calculates the chi-squared value based on residuals and an estimated or provided uncertainty (<code>sigma</code>). If the number of model parameters (<code>n_params</code>) is specified, it also computes the reduced chi-squared.</p> <p>Parameters:</p> Name Type Description Default <code>residuals</code> <code>ndarray</code> <p>The difference between observed and model-predicted values.</p> required <code>n_params</code> <code>int</code> <p>Number of fitted parameters. If provided, the function also computes the reduced chi-squared (\u03c7\u00b2_red).</p> <code>None</code> <code>cov_rescaled</code> <code>bool</code> <p>Whether the covariance matrix has been already rescaled by the fit method. If <code>True</code>, the function assumes proper uncertainty scaling. Otherwise, it estimates uncertainty from the standard deviation of the residuals.</p> <code>True</code> <code>sigma</code> <code>ndarray</code> <p>Experimental uncertainties. Should only be used when the fitting process does not account for experimental errors AND known uncertainties are available.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>chi2</code> <code>float</code> <p>The chi-squared statistic (\u03c7\u00b2), which measures the goodness of fit.</p> <code>red_chi2</code> <code>float (if `n_params` is provided)</code> <p>The reduced chi-squared statistic (\u03c7\u00b2_red), computed as \u03c7\u00b2 divided by the degrees of freedom (N - p). If <code>n_params</code> is <code>None</code>, only \u03c7\u00b2 is returned.</p> Warnings <ul> <li>If the degrees of freedom (N - p) is non-positive, a warning is issued,   and \u03c7\u00b2_red is set to NaN. This may indicate overfitting or an insufficient   number of data points.</li> <li>If any uncertainty value in <code>sigma</code> is zero, it is replaced with machine epsilon   to prevent division by zero.</li> </ul> Notes <ul> <li>If <code>sigma</code> is not provided and <code>cov_rescaled=False</code>, the function estimates   the uncertainty using the standard deviation of residuals.</li> <li>The reduced chi-squared value (\u03c7\u00b2_red) should ideally be close to 1 for a good fit.   Values significantly greater than 1 indicate underfitting, while values much less   than 1 suggest overfitting.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; residuals = np.array([0.1, -0.2, 0.15, -0.05])\n&gt;&gt;&gt; compute_chi2(residuals, n_params=2)\n(0.085, 0.0425)  # Example output\n</code></pre> Source code in <code>sqil_core/fit/_core.py</code> <pre><code>def compute_chi2(residuals, n_params=None, cov_rescaled=True, sigma: np.ndarray = None):\n    \"\"\"\n    Compute the chi-squared (\u03c7\u00b2) and reduced chi-squared (\u03c7\u00b2_red) statistics.\n\n    This function calculates the chi-squared value based on residuals and an\n    estimated or provided uncertainty (`sigma`). If the number of model parameters\n    (`n_params`) is specified, it also computes the reduced chi-squared.\n\n    Parameters\n    ----------\n    residuals : np.ndarray\n        The difference between observed and model-predicted values.\n    n_params : int, optional\n        Number of fitted parameters. If provided, the function also computes\n        the reduced chi-squared (\u03c7\u00b2_red).\n    cov_rescaled : bool, default=True\n        Whether the covariance matrix has been already rescaled by the fit method.\n        If `True`, the function assumes proper uncertainty scaling. Otherwise,\n        it estimates uncertainty from the standard deviation of the residuals.\n    sigma : np.ndarray, optional\n        Experimental uncertainties. Should only be used when the fitting process\n        does not account for experimental errors AND known uncertainties are available.\n\n    Returns\n    -------\n    chi2 : float\n        The chi-squared statistic (\u03c7\u00b2), which measures the goodness of fit.\n    red_chi2 : float (if `n_params` is provided)\n        The reduced chi-squared statistic (\u03c7\u00b2_red), computed as \u03c7\u00b2 divided by\n        the degrees of freedom (N - p). If `n_params` is `None`, only \u03c7\u00b2 is returned.\n\n    Warnings\n    --------\n    - If the degrees of freedom (N - p) is non-positive, a warning is issued,\n      and \u03c7\u00b2_red is set to NaN. This may indicate overfitting or an insufficient\n      number of data points.\n    - If any uncertainty value in `sigma` is zero, it is replaced with machine epsilon\n      to prevent division by zero.\n\n    Notes\n    -----\n    - If `sigma` is not provided and `cov_rescaled=False`, the function estimates\n      the uncertainty using the standard deviation of residuals.\n    - The reduced chi-squared value (\u03c7\u00b2_red) should ideally be close to 1 for a good fit.\n      Values significantly greater than 1 indicate underfitting, while values much less\n      than 1 suggest overfitting.\n\n    Examples\n    --------\n    &gt;&gt;&gt; residuals = np.array([0.1, -0.2, 0.15, -0.05])\n    &gt;&gt;&gt; compute_chi2(residuals, n_params=2)\n    (0.085, 0.0425)  # Example output\n    \"\"\"\n    # If the optimization does not account for th experimental sigma,\n    # approximate it with the std of the residuals\n    S = 1 if cov_rescaled else np.std(residuals)\n    # If the experimental error is provided, use that instead\n    if sigma is not None:\n        S = sigma\n\n    # Replace 0 elements of S with the machine epsilon to avoid divisions by 0\n    if not np.isscalar(S):\n        S_safe = np.where(S == 0, np.finfo(float).eps, S)\n    else:\n        S_safe = np.finfo(float).eps if S == 0 else S\n\n    # Compute chi squared\n    chi2 = np.sum((residuals / S_safe) ** 2)\n    # If number of parameters is not provided return just chi2\n    if n_params is None:\n        return chi2\n\n    # Reduced chi squared\n    dof = len(residuals) - n_params  # degrees of freedom (N - p)\n    if dof &lt;= 0:\n        warnings.warn(\n            \"Degrees of freedom (dof) is non-positive. This may indicate overfitting or insufficient data.\"\n        )\n        red_chi2 = np.nan\n    else:\n        red_chi2 = chi2 / dof\n\n    return chi2, red_chi2\n</code></pre>"},{"location":"API%20reference/fit/core/#sqil_core.fit._core.compute_nrmse","title":"<code>compute_nrmse(residuals, y_data)</code>","text":"<p>Computes the Normalized Root Mean Squared Error (NRMSE) of a model fit.</p> <p>Lower is better.</p> <p>The NRMSE is a scale-independent metric that quantifies the average magnitude of residual errors normalized by the range of the observed data. It is useful for comparing the fit quality across different datasets or models.</p> <p>For complex data it's computed using the L2 norm and the span of the magnitude.</p> <p>Parameters:</p> Name Type Description Default <code>residuals</code> <code>ndarray</code> <p>Array of residuals between the observed data and model predictions.</p> required <code>y_data</code> <code>ndarray</code> <p>The original observed data used in the model fitting.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The normalized root mean squared error (NRMSE).</p> Source code in <code>sqil_core/fit/_core.py</code> <pre><code>def compute_nrmse(residuals: np.ndarray, y_data: np.ndarray) -&gt; float:\n    \"\"\"\n    Computes the Normalized Root Mean Squared Error (NRMSE) of a model fit.\n\n    Lower is better.\n\n    The NRMSE is a scale-independent metric that quantifies the average magnitude\n    of residual errors normalized by the range of the observed data. It is useful\n    for comparing the fit quality across different datasets or models.\n\n    For complex data it's computed using the L2 norm and the span of the magnitude.\n\n    Parameters\n    ----------\n    residuals : np.ndarray\n        Array of residuals between the observed data and model predictions.\n    y_data : np.ndarray\n        The original observed data used in the model fitting.\n\n    Returns\n    -------\n    float\n        The normalized root mean squared error (NRMSE).\n    \"\"\"\n    n = len(residuals)\n    if np.iscomplexobj(y_data):\n        y_abs_span = np.max(np.abs(y_data)) - np.min(np.abs(y_data))\n        if y_abs_span == 0:\n            warnings.warn(\n                \"y_data has zero span in magnitude. NRMSE is undefined.\", RuntimeWarning\n            )\n            return np.nan\n        rmse = np.linalg.norm(residuals) / np.sqrt(n)\n        nrmse = rmse / y_abs_span\n    else:\n        y_span = np.max(y_data) - np.min(y_data)\n        if y_span == 0:\n            warnings.warn(\"y_data has zero span. NRMSE is undefined.\", RuntimeWarning)\n            return np.nan\n        rss = np.sum(residuals**2)\n        nrmse = np.sqrt(rss / n) / y_span\n\n    return nrmse\n</code></pre>"},{"location":"API%20reference/fit/core/#sqil_core.fit._core.fit_input","title":"<code>fit_input(fit_func)</code>","text":"<p>Decorator to handle optional fitting inputs like initial guesses, bounds, and fixed parameters for a fitting function.</p> <ul> <li><code>guess</code> : list or np.ndarray, optional, default=None     The initial guess for the fit. If None it's not passed to the fit function.</li> <li><code>bounds</code> : list or np.ndarray, optional, default=(-np.inf, np.inf)     The bounds on the fit parameters in the form [(min, max), (min, max), ...].</li> <li><code>fixed_params</code> : list or np.ndarray, optional, default=None     Indices of the parameters that must remain fixed during the optimization.     For example fitting <code>f(x, a, b)</code>, if we want to fix the value of <code>a</code> we would pass     <code>fit_f(guess=[a_guess, b_guess], fixed_params=[0])</code></li> <li><code>fixed_bound_factor</code> : float, optional, default=1e-6     The relative tolerance allowed for parameters that must remain fixed (<code>fixed_params</code>).</li> </ul> <p>IMPORTANT: This decorator requires the x and y input vectors to be named <code>x_data</code> and <code>y_data</code>.     The initial guess must be called <code>guess</code> and the bounds <code>bounds</code>.</p> <p>Parameters:</p> Name Type Description Default <code>fit_func</code> <code>callable</code> <p>The fitting function to be decorated. This function should accept <code>x_data</code> and <code>y_data</code> as mandatory parameters and may optionally accept <code>guess</code> and <code>bounds</code> (plus any other additional parameter).</p> required <p>Returns:</p> Type Description <code>callable</code> <p>A wrapper function that processes the input arguments and then calls the original fitting function with the preprocessed inputs. This function also handles warnings if unsupported parameters are passed to the fit function.</p> Notes <ul> <li>The parameters in <code>guess</code>, <code>bounds</code> and <code>fixed_params</code> must be in the same order as in the   modeled function definition.</li> <li>The decorator can fix certain parameters by narrowing their bounds based on an initial guess   and a specified <code>fixed_bound_factor</code>.</li> <li>The decorator processes bounds by setting them as <code>(-np.inf, np.inf)</code> if they are not specified (<code>None</code>).</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; @fit_input\n... def my_fit_func(x_data, y_data, guess=None, bounds=None, fixed_params=None):\n...     # Perform fitting...\n...     return fit_result\n&gt;&gt;&gt; x_data = np.linspace(0, 10, 100)\n&gt;&gt;&gt; y_data = np.sin(x_data) + np.random.normal(0, 0.1, 100)\n&gt;&gt;&gt; result = my_fit_func(x_data, y_data, guess=[1, 1], bounds=[(0, 5), (-np.inf, np.inf)])\n</code></pre> Source code in <code>sqil_core/fit/_core.py</code> <pre><code>def fit_input(fit_func):\n    \"\"\"\n    Decorator to handle optional fitting inputs like initial guesses, bounds, and fixed parameters\n    for a fitting function.\n\n    - `guess` : list or np.ndarray, optional, default=None\n        The initial guess for the fit. If None it's not passed to the fit function.\n    - `bounds` : list or np.ndarray, optional, default=(-np.inf, np.inf)\n        The bounds on the fit parameters in the form [(min, max), (min, max), ...].\n    - `fixed_params` : list or np.ndarray, optional, default=None\n        Indices of the parameters that must remain fixed during the optimization.\n        For example fitting `f(x, a, b)`, if we want to fix the value of `a` we would pass\n        `fit_f(guess=[a_guess, b_guess], fixed_params=[0])`\n    - `fixed_bound_factor` : float, optional, default=1e-6\n        The relative tolerance allowed for parameters that must remain fixed (`fixed_params`).\n\n    IMPORTANT: This decorator requires the x and y input vectors to be named `x_data` and `y_data`.\n        The initial guess must be called `guess` and the bounds `bounds`.\n\n    Parameters\n    ----------\n    fit_func : callable\n        The fitting function to be decorated. This function should accept `x_data` and `y_data` as\n        mandatory parameters and may optionally accept `guess` and `bounds` (plus any other additional\n        parameter).\n\n    Returns\n    -------\n    callable\n        A wrapper function that processes the input arguments and then calls the original fitting\n        function with the preprocessed inputs. This function also handles warnings if unsupported\n        parameters are passed to the fit function.\n\n    Notes\n    -----\n    - The parameters in `guess`, `bounds` and `fixed_params` must be in the same order as in the\n      modeled function definition.\n    - The decorator can fix certain parameters by narrowing their bounds based on an initial guess\n      and a specified `fixed_bound_factor`.\n    - The decorator processes bounds by setting them as `(-np.inf, np.inf)` if they are not specified (`None`).\n\n    Examples\n    -------\n    &gt;&gt;&gt; @fit_input\n    ... def my_fit_func(x_data, y_data, guess=None, bounds=None, fixed_params=None):\n    ...     # Perform fitting...\n    ...     return fit_result\n    &gt;&gt;&gt; x_data = np.linspace(0, 10, 100)\n    &gt;&gt;&gt; y_data = np.sin(x_data) + np.random.normal(0, 0.1, 100)\n    &gt;&gt;&gt; result = my_fit_func(x_data, y_data, guess=[1, 1], bounds=[(0, 5), (-np.inf, np.inf)])\n    \"\"\"\n\n    @wraps(fit_func)\n    def wrapper(\n        *params,\n        guess=None,\n        bounds=None,\n        fixed_params=None,\n        fixed_bound_factor=1e-6,\n        sigma=None,\n        **kwargs,\n    ):\n        # Inspect function to check if it requires guess and bounds\n        func_params = inspect.signature(fit_func).parameters\n\n        # Check if the user passed parameters that are not supported by the fit fun\n        if (guess is not None) and (\"guess\" not in func_params):\n            warnings.warn(\"The fit function doesn't allow any initial guess.\")\n        if (bounds is not None) and (\"bounds\" not in func_params):\n            warnings.warn(\"The fit function doesn't allow any bounds.\")\n        if (fixed_params is not None) and (guess is None):\n            raise ValueError(\"Using fixed_params requires an initial guess.\")\n\n        # Process bounds if the function accepts it\n        if (bounds is not None) and (\"bounds\" in func_params):\n            processed_bounds = np.array(\n                [(-np.inf, np.inf) if b is None else b for b in bounds],\n                dtype=np.float64,\n            )\n            lower_bounds, upper_bounds = (\n                processed_bounds[:, 0],\n                processed_bounds[:, 1],\n            )\n        else:\n            lower_bounds, upper_bounds = None, None\n\n        # Fix parameters by setting a very tight bound\n        if (fixed_params is not None) and (guess is not None):\n            if bounds is None:\n                lower_bounds = -np.inf * np.ones(len(guess))\n                upper_bounds = np.inf * np.ones(len(guess))\n            for idx in fixed_params:\n                tolerance = (\n                    abs(guess[idx]) * fixed_bound_factor\n                    if guess[idx] != 0\n                    else fixed_bound_factor\n                )\n                lower_bounds[idx] = guess[idx] - tolerance\n                upper_bounds[idx] = guess[idx] + tolerance\n\n        # Prepare arguments dynamically\n        fit_args = {**kwargs}\n\n        if guess is not None and \"guess\" in func_params:\n            fit_args[\"guess\"] = guess\n        if (\n            (bounds is not None) or (fixed_params is not None)\n        ) and \"bounds\" in func_params:\n            fit_args[\"bounds\"] = (lower_bounds, upper_bounds)\n\n        # Call the wrapped function with preprocessed inputs\n        fit_args = {**kwargs, **fit_args}\n        return fit_func(*params, **fit_args)\n\n    return wrapper\n</code></pre>"},{"location":"API%20reference/fit/core/#sqil_core.fit._core.fit_output","title":"<code>fit_output(fit_func)</code>","text":"<p>Decorator to standardize the output of fitting functions.</p> <p>This decorator processes the raw output of various fitting libraries (such as SciPy's curve_fit, least_squares leastsq, and minimize, as well as lmfit) and converts it into a unified <code>FitResult</code> object. It extracts optimized parameters, their standard errors, fit quality metrics, and a prediction function.</p> <p>Parameters:</p> Name Type Description Default <code>fit_func</code> <code>Callable</code> <p>A function that performs fitting and returns raw fit output, possibly along with metadata.</p> required <p>Returns:</p> Type Description <code>Callable</code> <p>A wrapped function that returns a <code>FitResult</code> object containing: - <code>params</code> : list     Optimized parameter values. - <code>std_err</code> : list or None     Standard errors of the fitted parameters. - <code>metrics</code> : dict or None     Dictionary of fit quality metrics (e.g., reduced chi-squared). - <code>predict</code> : Callable or None     A function that predicts values using the optimized parameters. - <code>output</code> : object     The raw optimizer output from the fitting process. - <code>param_names</code> : list or None     Names of the fitted parameters. - <code>metadata</code> : dict     A dictionary containing extra information. Advanced uses include passing     functions that get evaluated after fit result has been processed.     See the documentation, Notebooks/The fit_output decorator</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the fitting function's output format is not recognized.</p> Notes <ul> <li>If the fit function returns a tuple <code>(raw_output, metadata)</code>,   the metadata is extracted and applied to enhance the fit results.   In case of any conflicts, the metadata overrides the computed values.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; @fit_output\n... def my_fitting_function(x, y):\n...     return some_raw_fit_output\n...\n&gt;&gt;&gt; fit_result = my_fitting_function(x_data, y_data)\n&gt;&gt;&gt; print(fit_result.params)\n</code></pre> Source code in <code>sqil_core/fit/_core.py</code> <pre><code>def fit_output(fit_func):\n    \"\"\"\n    Decorator to standardize the output of fitting functions.\n\n    This decorator processes the raw output of various fitting libraries\n    (such as SciPy's curve_fit, least_squares leastsq, and minimize, as well as lmfit)\n    and converts it into a unified `FitResult` object. It extracts\n    optimized parameters, their standard errors, fit quality metrics,\n    and a prediction function.\n\n    Parameters\n    ----------\n    fit_func : Callable\n        A function that performs fitting and returns raw fit output,\n        possibly along with metadata.\n\n    Returns\n    -------\n    Callable\n        A wrapped function that returns a `FitResult` object containing:\n        - `params` : list\n            Optimized parameter values.\n        - `std_err` : list or None\n            Standard errors of the fitted parameters.\n        - `metrics` : dict or None\n            Dictionary of fit quality metrics (e.g., reduced chi-squared).\n        - `predict` : Callable or None\n            A function that predicts values using the optimized parameters.\n        - `output` : object\n            The raw optimizer output from the fitting process.\n        - `param_names` : list or None\n            Names of the fitted parameters.\n        - `metadata` : dict\n            A dictionary containing extra information. Advanced uses include passing\n            functions that get evaluated after fit result has been processed.\n            See the documentation, Notebooks/The fit_output decorator\n\n    Raises\n    ------\n    TypeError\n        If the fitting function's output format is not recognized.\n\n    Notes\n    -----\n    - If the fit function returns a tuple `(raw_output, metadata)`,\n      the metadata is extracted and applied to enhance the fit results.\n      In case of any conflicts, the metadata overrides the computed values.\n\n    Examples\n    --------\n    &gt;&gt;&gt; @fit_output\n    ... def my_fitting_function(x, y):\n    ...     return some_raw_fit_output\n    ...\n    &gt;&gt;&gt; fit_result = my_fitting_function(x_data, y_data)\n    &gt;&gt;&gt; print(fit_result.params)\n    \"\"\"\n\n    @wraps(fit_func)\n    def wrapper(*args, **kwargs):\n        # Perform the fit\n        fit_result = fit_func(*args, **kwargs)\n\n        # Extract information from function arguments\n        x_data, y_data = _get_xy_data_from_fit_args(*args, **kwargs)\n        sigma = kwargs.get(\"sigma\", None)\n        has_sigma = isinstance(sigma, (list, np.ndarray))\n\n        # Initilize variables\n        sqil_keys = [\"params\", \"std_err\", \"metrics\", \"predict\", \"output\", \"param_names\"]\n        sqil_dict = {key: None for key in sqil_keys}\n        metadata = {}\n        formatted = None\n        # Set the default parameters to an empty array instead of None\n        sqil_dict[\"params\"] = []\n\n        # Check if the fit output is a tuple and separate it into raw_fit_ouput and metadata\n        if (\n            isinstance(fit_result, tuple)\n            and (len(fit_result) == 2)\n            and isinstance(fit_result[1], dict)\n        ):\n            raw_fit_output, metadata = fit_result\n        else:\n            raw_fit_output = fit_result\n        sqil_dict[\"output\"] = raw_fit_output\n\n        # Check if there are variables to override in metadata before continuing\n        if \"fit_output_vars\" in metadata:\n            overrides = metadata[\"fit_output_vars\"]\n            x_data = overrides.get(\"x_data\", x_data)\n            y_data = overrides.get(\"y_data\", y_data)\n            del metadata[\"fit_output_vars\"]\n\n        # Format the raw_fit_output into a standardized dict\n        if raw_fit_output is None:\n            raise TypeError(\"Fit didn't coverge, result is None\")\n        # Scipy tuple (curve_fit, leastsq)\n        elif _is_scipy_tuple(raw_fit_output):\n            formatted = _format_scipy_tuple(raw_fit_output, y_data, has_sigma=has_sigma)\n\n        # Scipy least squares\n        elif _is_scipy_least_squares(raw_fit_output):\n            formatted = _format_scipy_least_squares(\n                raw_fit_output, y_data, has_sigma=has_sigma\n            )\n\n        # Scipy minimize\n        elif _is_scipy_minimize(raw_fit_output):\n            residuals = None\n            predict = metadata.get(\"predict\", None)\n            if (x_data is not None) and (predict is not None) and callable(predict):\n                residuals = y_data - metadata[\"predict\"](x_data, *raw_fit_output.x)\n            formatted = _format_scipy_minimize(\n                raw_fit_output, residuals=residuals, y_data=y_data, has_sigma=has_sigma\n            )\n\n        # lmfit\n        elif _is_lmfit(raw_fit_output):\n            formatted = _format_lmfit(raw_fit_output)\n\n        # Custom fit output\n        elif isinstance(raw_fit_output, dict):\n            formatted = raw_fit_output\n\n        else:\n            raise TypeError(\n                \"Couldn't recognize the output.\\n\"\n                + \"Are you using scipy? Did you forget to set `full_output=True` in your fit method?\"\n            )\n\n        # Update sqil_dict with the formatted fit_output\n        if formatted is not None:\n            sqil_dict.update(formatted)\n\n        # Add/override fileds using metadata\n        sqil_dict.update(metadata)\n\n        # Process metadata\n        metadata = _process_metadata(metadata, sqil_dict)\n        # Remove fields already present in sqil_dict from metadata\n        filtered_metadata = {k: v for k, v in metadata.items() if k not in sqil_keys}\n\n        # Assign the optimized parameters to the prediction function\n        model_name = metadata.get(\"model_name\", None)\n        if sqil_dict[\"predict\"] is not None:\n            if model_name is None:\n                model_name = sqil_dict[\"predict\"].__name__\n            params = sqil_dict[\"params\"]\n            predict = sqil_dict[\"predict\"]\n            n_inputs = _count_function_parameters(predict)\n            if n_inputs == 1 + len(params):\n                sqil_dict[\"predict\"] = lambda x: predict(x, *params)\n\n        return FitResult(\n            params=sqil_dict.get(\"params\", []),\n            std_err=sqil_dict.get(\"std_err\", None),\n            fit_output=raw_fit_output,\n            metrics=sqil_dict.get(\"metrics\", {}),\n            predict=sqil_dict.get(\"predict\", None),\n            param_names=sqil_dict.get(\"param_names\", None),\n            model_name=model_name,\n            metadata=filtered_metadata,\n        )\n\n    return wrapper\n</code></pre>"},{"location":"API%20reference/fit/fit/","title":"Fit","text":""},{"location":"API%20reference/fit/fit/#about-fitting-with-sqil_core","title":"About fitting with <code>sqil_core</code>","text":"<p>In the <code>sqil_core</code> library, most fit functions use the <code>@fit_output</code> decorator. This means that they return a <code>FitResult</code> object, even if the <code>return</code> statement in the source code suggests otherwise.</p> <p>The <code>@fit_output</code> decorator automatically computes useful metrics like standard errors and chi squared values, while also providing tools to visualize the fit result.</p> <p>Fit functions can also use the <code>@fit_input</code> decorator, which helps format input parameters (like bounds) in a more readable way, and allows to keep selected parameters fixed during the optimizaion.</p> <p>You can also apply these decorators to your own custom fit functions. Simply import <code>@fit_output</code> or <code>@fit_input</code> from <code>sqil_core</code>, apply them to your function, and enjoy the same benefits.</p> <p>For more information checkout the core section.</p>"},{"location":"API%20reference/fit/fit/#sqil_core.fit._fit.fit_circle_algebraic","title":"<code>fit_circle_algebraic(x_data, y_data)</code>","text":"<p>Fits a circle in the xy plane and returns the radius and the position of the center.</p> <p>Reference: https://arxiv.org/abs/1410.3365 This function uses an algebraic method to fit a circle to the provided data points. The algebraic approach is generally faster and more precise than iterative methods, but it can be more sensitive to noise in the data.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>Array of x-coordinates of the data points.</p> required <code>y</code> <code>ndarray</code> <p>Array of y-coordinates of the data points.</p> required <p>Returns:</p> Type Description <code>FitResult</code> <p>A <code>FitResult</code> object containing: - Fitted parameters (<code>params</code>). - Standard errors (<code>std_err</code>). - Goodness-of-fit metrics (<code>rmse</code>, root mean squared error). - A callable <code>predict</code> function for generating fitted responses.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; fit_result = fit_circle_algebraic(x_data, y_data)\n&gt;&gt;&gt; fit_result.summary()\n</code></pre> Source code in <code>sqil_core/fit/_fit.py</code> <pre><code>@fit_output\ndef fit_circle_algebraic(x_data: np.ndarray, y_data: np.ndarray) -&gt; FitResult:\n    \"\"\"Fits a circle in the xy plane and returns the radius and the position of the center.\n\n    Reference: https://arxiv.org/abs/1410.3365\n    This function uses an algebraic method to fit a circle to the provided data points.\n    The algebraic approach is generally faster and more precise than iterative methods,\n    but it can be more sensitive to noise in the data.\n\n    Parameters\n    ----------\n    x : np.ndarray\n        Array of x-coordinates of the data points.\n    y : np.ndarray\n        Array of y-coordinates of the data points.\n\n    Returns\n    -------\n    FitResult\n        A `FitResult` object containing:\n        - Fitted parameters (`params`).\n        - Standard errors (`std_err`).\n        - Goodness-of-fit metrics (`rmse`, root mean squared error).\n        - A callable `predict` function for generating fitted responses.\n\n    Examples\n    --------\n    &gt;&gt;&gt; fit_result = fit_circle_algebraic(x_data, y_data)\n    &gt;&gt;&gt; fit_result.summary()\n    \"\"\"\n    z_data = x_data + 1j * y_data\n\n    def calc_moments(z_data):\n        xi = z_data.real\n        xi_sqr = xi * xi\n        yi = z_data.imag\n        yi_sqr = yi * yi\n        zi = xi_sqr + yi_sqr\n        Nd = float(len(xi))\n        xi_sum = xi.sum()\n        yi_sum = yi.sum()\n        zi_sum = zi.sum()\n        xiyi_sum = (xi * yi).sum()\n        xizi_sum = (xi * zi).sum()\n        yizi_sum = (yi * zi).sum()\n        return np.array(\n            [\n                [(zi * zi).sum(), xizi_sum, yizi_sum, zi_sum],\n                [xizi_sum, xi_sqr.sum(), xiyi_sum, xi_sum],\n                [yizi_sum, xiyi_sum, yi_sqr.sum(), yi_sum],\n                [zi_sum, xi_sum, yi_sum, Nd],\n            ]\n        )\n\n    M = calc_moments(z_data)\n\n    a0 = (\n        (\n            (M[2][0] * M[3][2] - M[2][2] * M[3][0]) * M[1][1]\n            - M[1][2] * M[2][0] * M[3][1]\n            - M[1][0] * M[2][1] * M[3][2]\n            + M[1][0] * M[2][2] * M[3][1]\n            + M[1][2] * M[2][1] * M[3][0]\n        )\n        * M[0][3]\n        + (\n            M[0][2] * M[2][3] * M[3][0]\n            - M[0][2] * M[2][0] * M[3][3]\n            + M[0][0] * M[2][2] * M[3][3]\n            - M[0][0] * M[2][3] * M[3][2]\n        )\n        * M[1][1]\n        + (\n            M[0][1] * M[1][3] * M[3][0]\n            - M[0][1] * M[1][0] * M[3][3]\n            - M[0][0] * M[1][3] * M[3][1]\n        )\n        * M[2][2]\n        + (-M[0][1] * M[1][2] * M[2][3] - M[0][2] * M[1][3] * M[2][1]) * M[3][0]\n        + (\n            (M[2][3] * M[3][1] - M[2][1] * M[3][3]) * M[1][2]\n            + M[2][1] * M[3][2] * M[1][3]\n        )\n        * M[0][0]\n        + (\n            M[1][0] * M[2][3] * M[3][2]\n            + M[2][0] * (M[1][2] * M[3][3] - M[1][3] * M[3][2])\n        )\n        * M[0][1]\n        + (\n            (M[2][1] * M[3][3] - M[2][3] * M[3][1]) * M[1][0]\n            + M[1][3] * M[2][0] * M[3][1]\n        )\n        * M[0][2]\n    )\n    a1 = (\n        (\n            (M[3][0] - 2.0 * M[2][2]) * M[1][1]\n            - M[1][0] * M[3][1]\n            + M[2][2] * M[3][0]\n            + 2.0 * M[1][2] * M[2][1]\n            - M[2][0] * M[3][2]\n        )\n        * M[0][3]\n        + (\n            2.0 * M[2][0] * M[3][2]\n            - M[0][0] * M[3][3]\n            - 2.0 * M[2][2] * M[3][0]\n            + 2.0 * M[0][2] * M[2][3]\n        )\n        * M[1][1]\n        + (-M[0][0] * M[3][3] + 2.0 * M[0][1] * M[1][3] + 2.0 * M[1][0] * M[3][1])\n        * M[2][2]\n        + (-M[0][1] * M[1][3] + 2.0 * M[1][2] * M[2][1] - M[0][2] * M[2][3]) * M[3][0]\n        + (M[1][3] * M[3][1] + M[2][3] * M[3][2]) * M[0][0]\n        + (M[1][0] * M[3][3] - 2.0 * M[1][2] * M[2][3]) * M[0][1]\n        + (M[2][0] * M[3][3] - 2.0 * M[1][3] * M[2][1]) * M[0][2]\n        - 2.0 * M[1][2] * M[2][0] * M[3][1]\n        - 2.0 * M[1][0] * M[2][1] * M[3][2]\n    )\n    a2 = (\n        (2.0 * M[1][1] - M[3][0] + 2.0 * M[2][2]) * M[0][3]\n        + (2.0 * M[3][0] - 4.0 * M[2][2]) * M[1][1]\n        - 2.0 * M[2][0] * M[3][2]\n        + 2.0 * M[2][2] * M[3][0]\n        + M[0][0] * M[3][3]\n        + 4.0 * M[1][2] * M[2][1]\n        - 2.0 * M[0][1] * M[1][3]\n        - 2.0 * M[1][0] * M[3][1]\n        - 2.0 * M[0][2] * M[2][3]\n    )\n    a3 = -2.0 * M[3][0] + 4.0 * M[1][1] + 4.0 * M[2][2] - 2.0 * M[0][3]\n    a4 = -4.0\n\n    def func(x):\n        return a0 + a1 * x + a2 * x * x + a3 * x * x * x + a4 * x * x * x * x\n\n    def d_func(x):\n        return a1 + 2 * a2 * x + 3 * a3 * x * x + 4 * a4 * x * x * x\n\n    x0 = fsolve(func, 0.0, fprime=d_func)\n\n    def solve_eq_sys(val, M):\n        # prepare\n        M[3][0] = M[3][0] + 2 * val\n        M[0][3] = M[0][3] + 2 * val\n        M[1][1] = M[1][1] - val\n        M[2][2] = M[2][2] - val\n        return np.linalg.svd(M)\n\n    U, s, Vt = solve_eq_sys(x0[0], M)\n\n    A_vec = Vt[np.argmin(s), :]\n\n    xc = -A_vec[1] / (2.0 * A_vec[0])\n    yc = -A_vec[2] / (2.0 * A_vec[0])\n    # the term *sqrt term corrects for the constraint, because it may be altered due to numerical inaccuracies during calculation\n    r0 = (\n        1.0\n        / (2.0 * np.absolute(A_vec[0]))\n        * np.sqrt(A_vec[1] * A_vec[1] + A_vec[2] * A_vec[2] - 4.0 * A_vec[0] * A_vec[3])\n    )\n\n    std_err = _compute_circle_fit_errors(x_data, y_data, xc, yc, r0)\n    return {\n        \"params\": [xc, yc, r0],\n        \"std_err\": std_err,\n        \"metrics\": _compute_circle_fit_metrics(x_data, y_data, xc, yc, r0),\n        \"predict\": lambda theta: (xc + r0 * np.cos(theta), yc + r0 * np.sin(theta)),\n        \"output\": {},\n        \"param_names\": [\"xc\", \"yc\", \"r0\"],\n    }\n</code></pre>"},{"location":"API%20reference/fit/fit/#sqil_core.fit._fit.fit_decaying_exp","title":"<code>fit_decaying_exp(x_data, y_data, guess=None, bounds=(-np.inf, np.inf))</code>","text":"<p>Fits a decaying exponential function to the provided data. The function estimates the amplitude (A), decay time constant (tau), and baseline (y0) of the decaying exponential function.</p> <p>f(x) = A * exp(-x / \u03c4) + y0</p> \\[f(x) = A \\exp\\left( -\\frac{x}{\\tau} \\right) + y_0\\] <p>Parameters:</p> Name Type Description Default <code>x_data</code> <code>ndarray</code> <p>The independent variable (e.g., x values of the data).</p> required <code>y_data</code> <code>ndarray</code> <p>The dependent variable (e.g., y values of the data).</p> required <code>guess</code> <code>list</code> <p>Initial guesses for the fit parameters [A, tau, y0]. If not provided, defaults are calculated based on the data.</p> <code>None</code> <code>bounds</code> <code>list[tuple[float]]</code> <p>The bounds for the fit parameters in the format [(min, max), ...]. If not provided, defaults are calculated.</p> <code>(-inf, inf)</code> <code>fixed_params</code> <code>list[int]</code> <p>A list of indices representing parameters in the initial guess that should remain unchanged during the fitting process.</p> <code>None</code> <p>Returns:</p> Type Description <code>FitResult</code> <p>A <code>FitResult</code> object containing: - Fitted parameters (<code>params</code>). - Standard errors (<code>std_err</code>). - Goodness-of-fit metrics (<code>rmse</code>, root mean squared error). - A callable <code>predict</code> function for generating fitted responses.</p> Source code in <code>sqil_core/fit/_fit.py</code> <pre><code>@fit_input\n@fit_output\ndef fit_decaying_exp(\n    x_data: np.ndarray,\n    y_data: np.ndarray,\n    guess: list = None,\n    bounds: list[tuple[float]] | tuple = (-np.inf, np.inf),\n) -&gt; FitResult:\n    r\"\"\"\n    Fits a decaying exponential function to the provided data. The function estimates\n    the amplitude (A), decay time constant (tau), and baseline (y0) of the decaying\n    exponential function.\n\n    f(x) = A * exp(-x / \u03c4) + y0\n\n    $$f(x) = A \\exp\\left( -\\frac{x}{\\tau} \\right) + y_0$$\n\n    Parameters\n    ----------\n    x_data : np.ndarray\n        The independent variable (e.g., x values of the data).\n\n    y_data : np.ndarray\n        The dependent variable (e.g., y values of the data).\n\n    guess : list, optional\n        Initial guesses for the fit parameters [A, tau, y0]. If not provided,\n        defaults are calculated based on the data.\n\n    bounds : list[tuple[float]], optional\n        The bounds for the fit parameters in the format [(min, max), ...].\n        If not provided, defaults are calculated.\n\n    fixed_params : list[int], optional, default: None\n        A list of indices representing parameters in the initial guess that should\n        remain unchanged during the fitting process.\n\n    Returns\n    -------\n    FitResult\n        A `FitResult` object containing:\n        - Fitted parameters (`params`).\n        - Standard errors (`std_err`).\n        - Goodness-of-fit metrics (`rmse`, root mean squared error).\n        - A callable `predict` function for generating fitted responses.\n    \"\"\"\n    x, y = x_data, y_data\n\n    # Default intial guess if not provided\n    if has_at_least_one(guess, None):\n        guess = fill_gaps(guess, decaying_exp_guess(x_data, y_data))\n\n    # Default bounds if not provided\n    if bounds is None:\n        span_y = np.max(y) - np.min(y)\n        c0_min = np.min(y) - 100.0 * span_y\n        c0_max = np.max(y) + 100.0 * span_y\n        bounds = (\n            [-100.0 * span_y, 0.0, c0_min],\n            [100.0 * span_y, 100.0 * (np.max(x) - np.min(x)), c0_max],\n        )\n\n    res = curve_fit(\n        _models.decaying_exp, x, y, p0=guess, bounds=bounds, full_output=True\n    )\n\n    return res, {\n        \"param_names\": [\"A\", \"tau\", \"y0\"],\n        \"predict\": _models.decaying_exp,\n    }\n</code></pre>"},{"location":"API%20reference/fit/fit/#sqil_core.fit._fit.fit_decaying_oscillations","title":"<code>fit_decaying_oscillations(x_data, y_data, guess=None, bounds=None, num_init=10)</code>","text":"<p>Fits a decaying oscillation model to data. The function estimates key features like the oscillation period and phase, and tries multiple initial guesses for the optimization process.</p> <p>f(x) = A * exp(-x / \u03c4) * cos(2\u03c0 * (x - \u03c6) / T) + y0</p> \\[f(x) = A \\exp\\left( -\\frac{x}{\\tau} \\right) \\cos\\left( 2\\pi \\frac{x - \\phi}{T} \\right) + y_0\\] <p>Parameters:</p> Name Type Description Default <code>x_data</code> <code>ndarray</code> <p>Independent variable array (e.g., time or frequency).</p> required <code>y_data</code> <code>ndarray</code> <p>Dependent variable array representing the measured signal.</p> required <code>guess</code> <code>list[float] or None</code> <p>Initial parameter estimates [A, tau, y0, phi, T]. Missing values are automatically filled.</p> <code>None</code> <code>bounds</code> <code>list[tuple[float]] or tuple</code> <p>Lower and upper bounds for parameters during fitting, by default no bounds.</p> <code>None</code> <code>num_init</code> <code>int</code> <p>Number of phase values to try when guessing, by default 10.</p> <code>10</code> <p>Returns:</p> Type Description <code>FitResult</code> <p>A <code>FitResult</code> object containing: - Fitted parameters (<code>params</code>). - Standard errors (<code>std_err</code>). - Goodness-of-fit metrics (<code>rmse</code>, root mean squared error). - A callable <code>predict</code> function for generating fitted responses. - A metadata dictionary containing the pi_time and its standard error.</p> Source code in <code>sqil_core/fit/_fit.py</code> <pre><code>@fit_input\n@fit_output\ndef fit_decaying_oscillations(\n    x_data: np.ndarray,\n    y_data: np.ndarray,\n    guess: list[float] | None = None,\n    bounds: list[tuple[float]] | tuple = None,\n    num_init: int = 10,\n) -&gt; FitResult:\n    r\"\"\"\n    Fits a decaying oscillation model to data. The function estimates key features\n    like the oscillation period and phase, and tries multiple initial guesses for\n    the optimization process.\n\n    f(x) = A * exp(-x / \u03c4) * cos(2\u03c0 * (x - \u03c6) / T) + y0\n\n    $$f(x) = A \\exp\\left( -\\frac{x}{\\tau} \\right) \\cos\\left( 2\\pi \\frac{x - \\phi}{T} \\right) + y_0$$\n\n    Parameters\n    ----------\n    x_data : np.ndarray\n        Independent variable array (e.g., time or frequency).\n    y_data : np.ndarray\n        Dependent variable array representing the measured signal.\n    guess : list[float] or None, optional\n        Initial parameter estimates [A, tau, y0, phi, T]. Missing values are automatically filled.\n    bounds : list[tuple[float]] or tuple, optional\n        Lower and upper bounds for parameters during fitting, by default no bounds.\n    num_init : int, optional\n        Number of phase values to try when guessing, by default 10.\n\n    Returns\n    -------\n    FitResult\n        A `FitResult` object containing:\n        - Fitted parameters (`params`).\n        - Standard errors (`std_err`).\n        - Goodness-of-fit metrics (`rmse`, root mean squared error).\n        - A callable `predict` function for generating fitted responses.\n        - A metadata dictionary containing the pi_time and its standard error.\n    \"\"\"\n    # Default intial guess if not provided\n    if has_at_least_one(guess, None):\n        guess = fill_gaps(guess, decaying_oscillations_guess(x_data, y_data, num_init))\n\n    # Default bounds if not provided\n    if bounds is None:\n        bounds = ([None] * len(guess), [None] * len(guess))\n    if has_at_least_one(bounds[0], None) or has_at_least_one(bounds[1], None):\n        lower, upper = bounds\n        lower_guess, upper_guess = decaying_oscillations_bounds(x_data, y_data, guess)\n        bounds = (fill_gaps(lower, lower_guess), fill_gaps(upper, upper_guess))\n\n    A, tau, y0, phi, T = guess\n    phi = make_iterable(phi)\n    y0 = make_iterable(y0)\n\n    best_fit = None\n    best_popt = None\n    best_nrmse = np.inf\n\n    @fit_output\n    def _curve_fit_osc(x_data, y_data, p0, bounds):\n        return curve_fit(\n            _models.decaying_oscillations,\n            x_data,\n            y_data,\n            p0,\n            bounds=bounds,\n            full_output=True,\n        )\n\n    # Try multiple initializations\n    for phi_guess in phi:\n        for offset in y0:\n            p0 = [A, tau, offset, phi_guess, T]\n\n            try:\n                with warnings.catch_warnings():\n                    warnings.simplefilter(\"ignore\")\n                    fit_res = _curve_fit_osc(x_data, y_data, p0, bounds)\n                if fit_res.metrics[\"nrmse\"] &lt; best_nrmse:\n                    best_fit, best_popt = fit_res.output, fit_res.params\n                    best_nrmse = fit_res.metrics[\"nrmse\"]\n            except:\n                if best_fit is None:\n\n                    def _decaying_osc_res(p, x, y):\n                        return _models.decaying_oscillations(x, *p) - y\n\n                    result = least_squares(\n                        _decaying_osc_res,\n                        p0,\n                        loss=\"soft_l1\",\n                        f_scale=0.1,\n                        bounds=bounds,\n                        args=(x_data, y_data),\n                    )\n                    best_fit, best_popt = result, result.x\n\n    if best_fit is None:\n        return None\n\n    # Compute pi-time (half-period + phase offset)\n    pi_time_raw = 0.5 * best_popt[4] + best_popt[3]\n    while pi_time_raw &gt; 0.75 * np.abs(best_popt[4]):\n        pi_time_raw -= 0.5 * np.abs(best_popt[4])\n    while pi_time_raw &lt; 0.25 * np.abs(best_popt[4]):\n        pi_time_raw += 0.5 * np.abs(best_popt[4])\n\n    def _get_pi_time_std_err(sqil_dict):\n        if sqil_dict[\"std_err\"] is not None:\n            phi_err = sqil_dict[\"std_err\"][3]\n            T_err = sqil_dict[\"std_err\"][4]\n            if np.isfinite(T_err) and np.isfinite(phi_err):\n                return np.sqrt((T_err / 2) ** 2 + phi_err**2)\n        return np.nan\n\n    # Metadata dictionary\n    metadata = {\n        \"param_names\": [\"A\", \"tau\", \"y0\", \"phi\", \"T\"],\n        \"predict\": _models.decaying_oscillations,\n        \"pi_time\": pi_time_raw,\n        \"@pi_time_std_err\": _get_pi_time_std_err,\n    }\n\n    return best_fit, metadata\n</code></pre>"},{"location":"API%20reference/fit/fit/#sqil_core.fit._fit.fit_gaussian","title":"<code>fit_gaussian(x_data, y_data, guess=None, bounds=None)</code>","text":"<p>Fits a Gaussian function to the provided data. The function estimates the amplitude, mean, standard deviation (sigma), and baseline of the Gaussian function, and computes the full width at half maximum (FWHM).</p> <p>G(x) = A / (|\u03c3| * sqrt(2\u03c0)) * exp(- (x - x0)^2 / (2\u03c3^2)) + y0</p> \\[G(x) = A \\frac{1}{\\left| \\sigma \\right| \\sqrt{2\\pi}} \\exp\\left( -\\frac{(x - x_0)^2}{2\\sigma^2} \\right) + y_0\\] <p>Parameters:</p> Name Type Description Default <code>x_data</code> <code>ndarray</code> <p>The independent variable (e.g., x values of the data).</p> required <code>y_data</code> <code>ndarray</code> <p>The dependent variable (e.g., y values of the data).</p> required <code>guess</code> <code>list</code> <p>Initial guesses for the fit parameters [A, x0, sigma, y0]. If not provided, defaults are calculated based on the data.</p> <code>None</code> <code>bounds</code> <code>list[tuple[float]]</code> <p>The bounds for the fit parameters in the format [(min, max), ...]. If not provided, defaults are calculated.</p> <code>None</code> <code>fixed_params</code> <code>list[int]</code> <p>A list of indices representing parameters in the initial guess that should remain unchanged during the fitting process.</p> <code>None</code> <p>Returns:</p> Type Description <code>FitResult</code> <p>A <code>FitResult</code> object containing: - Fitted parameters (<code>params</code>). - Standard errors (<code>std_err</code>). - Goodness-of-fit metrics (<code>rmse</code>, root mean squared error). - A callable <code>predict</code> function for generating fitted responses. - A metadata dictionary containing the FWHM.</p> Source code in <code>sqil_core/fit/_fit.py</code> <pre><code>@fit_input\n@fit_output\ndef fit_gaussian(\n    x_data: np.ndarray,\n    y_data: np.ndarray,\n    guess: list = None,\n    bounds: list[tuple[float]] | tuple = None,\n) -&gt; FitResult:\n    r\"\"\"\n    Fits a Gaussian function to the provided data. The function estimates the\n    amplitude, mean, standard deviation (sigma), and baseline of the Gaussian\n    function, and computes the full width at half maximum (FWHM).\n\n    G(x) = A / (|\u03c3| * sqrt(2\u03c0)) * exp(- (x - x0)^2 / (2\u03c3^2)) + y0\n\n    $$G(x) = A \\frac{1}{\\left| \\sigma \\right| \\sqrt{2\\pi}} \\exp\\left( -\\frac{(x - x_0)^2}{2\\sigma^2} \\right) + y_0$$\n\n    Parameters\n    ----------\n    x_data : np.ndarray\n        The independent variable (e.g., x values of the data).\n\n    y_data : np.ndarray\n        The dependent variable (e.g., y values of the data).\n\n    guess : list, optional\n        Initial guesses for the fit parameters [A, x0, sigma, y0]. If not provided,\n        defaults are calculated based on the data.\n\n    bounds : list[tuple[float]], optional\n        The bounds for the fit parameters in the format [(min, max), ...].\n        If not provided, defaults are calculated.\n\n    fixed_params : list[int], optional, default: None\n        A list of indices representing parameters in the initial guess that should\n        remain unchanged during the fitting process.\n\n    Returns\n    -------\n    FitResult\n        A `FitResult` object containing:\n        - Fitted parameters (`params`).\n        - Standard errors (`std_err`).\n        - Goodness-of-fit metrics (`rmse`, root mean squared error).\n        - A callable `predict` function for generating fitted responses.\n        - A metadata dictionary containing the FWHM.\n    \"\"\"\n\n    x, y = x_data, y_data\n\n    # Default initial guess if not provided\n    if has_at_least_one(guess, None):\n        guess = fill_gaps(guess, gaussian_guess(x_data, y_data))\n    # Default bounds if not provided\n    if bounds is None:\n        bounds = ([None] * len(guess), [None] * len(guess))\n    if has_at_least_one(bounds[0], None) or has_at_least_one(bounds[1], None):\n        lower, upper = bounds\n        lower_guess, upper_guess = gaussian_bounds(x_data, y_data, guess)\n        bounds = (fill_gaps(lower, lower_guess), fill_gaps(upper, upper_guess))\n\n    res = curve_fit(_models.gaussian, x, y, p0=guess, bounds=bounds, full_output=True)\n\n    # Compute FWHM from sigma\n    _, _, sigma, _ = res[0]\n    fwhm = 2 * np.sqrt(2 * np.log(2)) * sigma\n\n    return res, {\n        \"param_names\": [\"A\", \"x0\", \"sigma\", \"y0\"],\n        \"predict\": _models.gaussian,\n        \"fwhm\": fwhm,\n    }\n</code></pre>"},{"location":"API%20reference/fit/fit/#sqil_core.fit._fit.fit_lorentzian","title":"<code>fit_lorentzian(x_data, y_data, guess=None, bounds=None)</code>","text":"<p>Fits a Lorentzian function to the provided data. The function estimates the amplitude (A), center (x0), full width at half maximum (FWHM), and baseline (y0) of the Lorentzian function.</p> <p>L(x) = A * (|FWHM| / 2) / ((x - x0)^2 + (FWHM^2 / 4)) + y0</p> \\[L(x) = A \\frac{\\left| \\text{FWHM} \\right|}{2} \\frac{1}{(x - x_0)^2 + \\frac{\\text{FWHM}^2}{4}} + y_0\\] <p>Parameters:</p> Name Type Description Default <code>x_data</code> <code>ndarray</code> <p>The independent variable (e.g., x values of the data).</p> required <code>y_data</code> <code>ndarray</code> <p>The dependent variable (e.g., y values of the data).</p> required <code>guess</code> <code>list</code> <p>Initial guesses for the fit parameters [A, x0, fwhm, y0]. If not provided, defaults are calculated based on the data.</p> <code>None</code> <code>bounds</code> <code>list[tuple[float]]</code> <p>The bounds for the fit parameters in the format [(min, max), ...]. If not provided, defaults are calculated.</p> <code>None</code> <code>fixed_params</code> <code>list[int]</code> <p>A list of indices representing parameters in the initial guess that should remain unchanged during the fitting process.</p> <code>None</code> <p>Returns:</p> Type Description <code>FitResult</code> <p>A <code>FitResult</code> object containing: - Fitted parameters (<code>params</code>). - Standard errors (<code>std_err</code>). - Goodness-of-fit metrics (<code>rmse</code>, root mean squared error). - A callable <code>predict</code> function for generating fitted responses.</p> Source code in <code>sqil_core/fit/_fit.py</code> <pre><code>@fit_input\n@fit_output\ndef fit_lorentzian(\n    x_data: np.ndarray,\n    y_data: np.ndarray,\n    guess: list = None,\n    bounds: list[tuple[float]] | tuple = None,\n) -&gt; FitResult:\n    r\"\"\"\n    Fits a Lorentzian function to the provided data. The function estimates the\n    amplitude (A), center (x0), full width at half maximum (FWHM), and baseline (y0)\n    of the Lorentzian function.\n\n    L(x) = A * (|FWHM| / 2) / ((x - x0)^2 + (FWHM^2 / 4)) + y0\n\n    $$L(x) = A \\frac{\\left| \\text{FWHM} \\right|}{2} \\frac{1}{(x - x_0)^2 + \\frac{\\text{FWHM}^2}{4}} + y_0$$\n\n    Parameters\n    ----------\n    x_data : np.ndarray\n        The independent variable (e.g., x values of the data).\n\n    y_data : np.ndarray\n        The dependent variable (e.g., y values of the data).\n\n    guess : list, optional\n        Initial guesses for the fit parameters [A, x0, fwhm, y0]. If not provided,\n        defaults are calculated based on the data.\n\n    bounds : list[tuple[float]], optional\n        The bounds for the fit parameters in the format [(min, max), ...].\n        If not provided, defaults are calculated.\n\n    fixed_params : list[int], optional, default: None\n        A list of indices representing parameters in the initial guess that should\n        remain unchanged during the fitting process.\n\n    Returns\n    -------\n    FitResult\n        A `FitResult` object containing:\n        - Fitted parameters (`params`).\n        - Standard errors (`std_err`).\n        - Goodness-of-fit metrics (`rmse`, root mean squared error).\n        - A callable `predict` function for generating fitted responses.\n    \"\"\"\n\n    x, y = x_data, y_data\n\n    # Default intial guess if not provided\n    if has_at_least_one(guess, None):\n        guess = fill_gaps(guess, lorentzian_guess(x_data, y_data))\n\n    # Default bounds if not provided\n    if bounds is None:\n        bounds = ([None] * len(guess), [None] * len(guess))\n    if has_at_least_one(bounds[0], None) or has_at_least_one(bounds[1], None):\n        lower, upper = bounds\n        lower_guess, upper_guess = lorentzian_bounds(x_data, y_data, guess)\n        bounds = (fill_gaps(lower, lower_guess), fill_gaps(upper, upper_guess))\n\n    res = curve_fit(_models.lorentzian, x, y, p0=guess, bounds=bounds, full_output=True)\n\n    return res, {\n        \"param_names\": [\"A\", \"x0\", \"fwhm\", \"y0\"],\n        \"predict\": _models.lorentzian,\n    }\n</code></pre>"},{"location":"API%20reference/fit/fit/#sqil_core.fit._fit.fit_many_decaying_oscillations","title":"<code>fit_many_decaying_oscillations(x_data, y_data, n, guess=None)</code>","text":"<p>Fits a sum of <code>n</code> exponentially decaying oscillations to the given data.</p> <p>Each component of the model is of the form:     A_i * exp(-x / tau_i) * cos(2\u03c0 * T_i * x + phi_i)</p> <p>Parameters:</p> Name Type Description Default <code>x_data</code> <code>ndarray</code> <p>1D array of x-values (e.g., time).</p> required <code>y_data</code> <code>ndarray</code> <p>1D array of y-values (e.g., signal amplitude).</p> required <code>n</code> <code>int</code> <p>Number of decaying oscillation components to fit.</p> required <code>guess</code> <code>list or None</code> <p>Optional initial parameter guess. If None, a guess is automatically generated using <code>many_decaying_oscillations_guess</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>FitResult</code> Source code in <code>sqil_core/fit/_fit.py</code> <pre><code>@fit_output\ndef fit_many_decaying_oscillations(\n    x_data: np.ndarray, y_data: np.ndarray, n: int, guess=None\n):\n    \"\"\"\n    Fits a sum of `n` exponentially decaying oscillations to the given data.\n\n    Each component of the model is of the form:\n        A_i * exp(-x / tau_i) * cos(2\u03c0 * T_i * x + phi_i)\n\n    Parameters\n    ----------\n    x_data : np.ndarray\n        1D array of x-values (e.g., time).\n    y_data : np.ndarray\n        1D array of y-values (e.g., signal amplitude).\n    n : int\n        Number of decaying oscillation components to fit.\n    guess : list or None, optional\n        Optional initial parameter guess. If None, a guess is automatically generated\n        using `many_decaying_oscillations_guess`.\n\n    Returns\n    -------\n    FitResult\n    \"\"\"\n\n    if has_at_least_one(guess, None):\n        guess = fill_gaps(guess, many_decaying_oscillations_guess(x_data, y_data, n))\n\n    res = curve_fit(\n        _models.many_decaying_oscillations,\n        x_data,\n        y_data,\n        p0=guess,\n        # maxfev=10000,\n        full_output=True,\n    )\n\n    metadata = {\n        \"param_names\": [f\"{p}{i}\" for i in range(n) for p in (\"A\", \"tau\", \"phi\", \"T\")]\n        + [\"y0\"],\n        \"predict\": lambda x: _models.many_decaying_oscillations(x, *res[0]),\n        \"model_name\": f\"many_decaying_oscillations({n})\",\n    }\n\n    return res, metadata\n</code></pre>"},{"location":"API%20reference/fit/fit/#sqil_core.fit._fit.fit_oscillations","title":"<code>fit_oscillations(x_data, y_data, guess=None, bounds=None, num_init=10)</code>","text":"<p>Fits an oscillation model to data. The function estimates key features like the oscillation period and phase, and tries multiple initial guesses for the optimization process.</p> <p>f(x) = A * cos(2\u03c0 * (x - \u03c6) / T) + y0</p> \\[f(x) = A \\cos\\left( 2\\pi \\frac{x - \\phi}{T} \\right) + y_0\\] <p>Parameters:</p> Name Type Description Default <code>x_data</code> <code>ndarray</code> <p>Independent variable array (e.g., time or frequency).</p> required <code>y_data</code> <code>ndarray</code> <p>Dependent variable array representing the measured signal.</p> required <code>guess</code> <code>list[float] or None</code> <p>Initial parameter estimates [A, y0, phi, T]. Missing values are automatically filled.</p> <code>None</code> <code>bounds</code> <code>list[tuple[float]] or tuple</code> <p>Lower and upper bounds for parameters during fitting, by default no bounds.</p> <code>None</code> <code>num_init</code> <code>int</code> <p>Number of phase values to try when guessing, by default 10.</p> <code>10</code> <p>Returns:</p> Type Description <code>FitResult</code> <p>A <code>FitResult</code> object containing: - Fitted parameters (<code>params</code>). - Standard errors (<code>std_err</code>). - Goodness-of-fit metrics (<code>rmse</code>, root mean squared error). - A callable <code>predict</code> function for generating fitted responses. - A metadata dictionary containing the pi_time and its standard error.</p> Source code in <code>sqil_core/fit/_fit.py</code> <pre><code>@fit_input\n@fit_output\ndef fit_oscillations(\n    x_data: np.ndarray,\n    y_data: np.ndarray,\n    guess: list[float] | None = None,\n    bounds: list[tuple[float]] | tuple = None,\n    num_init: int = 10,\n) -&gt; FitResult:\n    r\"\"\"\n    Fits an oscillation model to data. The function estimates key features\n    like the oscillation period and phase, and tries multiple initial guesses for\n    the optimization process.\n\n    f(x) = A * cos(2\u03c0 * (x - \u03c6) / T) + y0\n\n    $$f(x) = A \\cos\\left( 2\\pi \\frac{x - \\phi}{T} \\right) + y_0$$\n\n    Parameters\n    ----------\n    x_data : np.ndarray\n        Independent variable array (e.g., time or frequency).\n    y_data : np.ndarray\n        Dependent variable array representing the measured signal.\n    guess : list[float] or None, optional\n        Initial parameter estimates [A, y0, phi, T]. Missing values are automatically filled.\n    bounds : list[tuple[float]] or tuple, optional\n        Lower and upper bounds for parameters during fitting, by default no bounds.\n    num_init : int, optional\n        Number of phase values to try when guessing, by default 10.\n\n    Returns\n    -------\n    FitResult\n        A `FitResult` object containing:\n        - Fitted parameters (`params`).\n        - Standard errors (`std_err`).\n        - Goodness-of-fit metrics (`rmse`, root mean squared error).\n        - A callable `predict` function for generating fitted responses.\n        - A metadata dictionary containing the pi_time and its standard error.\n    \"\"\"\n    # Default intial guess if not provided\n    if has_at_least_one(guess, None):\n        guess = fill_gaps(guess, oscillations_guess(x_data, y_data, num_init))\n\n    # Default bounds if not provided\n    if bounds is None:\n        bounds = ([None] * len(guess), [None] * len(guess))\n    if has_at_least_one(bounds[0], None) or has_at_least_one(bounds[1], None):\n        lower, upper = bounds\n        lower_guess, upper_guess = oscillations_bounds(x_data, y_data, guess)\n        bounds = (fill_gaps(lower, lower_guess), fill_gaps(upper, upper_guess))\n\n    A, y0, phi, T = guess\n    phi = make_iterable(phi)\n    y0 = make_iterable(y0)\n\n    best_fit = None\n    best_popt = None\n    best_nrmse = np.inf\n\n    @fit_output\n    def _curve_fit_osc(x_data, y_data, p0, bounds):\n        return curve_fit(\n            _models.oscillations,\n            x_data,\n            y_data,\n            p0,\n            bounds=bounds,\n            full_output=True,\n        )\n\n    # Try multiple initializations\n    for phi_guess in phi:\n        for offset in y0:\n            p0 = [A, offset, phi_guess, T]\n\n            try:\n                with warnings.catch_warnings():\n                    warnings.simplefilter(\"ignore\")\n                    fit_res = _curve_fit_osc(x_data, y_data, p0, bounds)\n                if fit_res.metrics[\"nrmse\"] &lt; best_nrmse:\n                    best_fit, best_popt = fit_res.output, fit_res.params\n                    best_nrmse = fit_res.metrics[\"nrmse\"]\n            except:\n                if best_fit is None:\n\n                    def _oscillations_res(p, x, y):\n                        return _models.oscillations(x, *p) - y\n\n                    result = least_squares(\n                        _oscillations_res,\n                        p0,\n                        loss=\"soft_l1\",\n                        f_scale=0.1,\n                        bounds=bounds,\n                        args=(x_data, y_data),\n                    )\n                    best_fit, best_popt = result, result.x\n\n    if best_fit is None:\n        return None\n\n    # Compute pi-time (half-period + phase offset)\n    pi_time_raw = 0.5 * best_popt[3] + best_popt[2]\n    while pi_time_raw &gt; 0.75 * np.abs(best_popt[3]):\n        pi_time_raw -= 0.5 * np.abs(best_popt[3])\n    while pi_time_raw &lt; 0.25 * np.abs(best_popt[3]):\n        pi_time_raw += 0.5 * np.abs(best_popt[3])\n\n    def _get_pi_time_std_err(sqil_dict):\n        if sqil_dict[\"std_err\"] is not None:\n            phi_err = sqil_dict[\"std_err\"][2]\n            T_err = sqil_dict[\"std_err\"][3]\n            if np.isfinite(T_err) and np.isfinite(phi_err):\n                return np.sqrt((T_err / 2) ** 2 + phi_err**2)\n        return np.nan\n\n    # Metadata dictionary\n    metadata = {\n        \"param_names\": [\"A\", \"y0\", \"phi\", \"T\"],\n        \"predict\": _models.oscillations,\n        \"pi_time\": pi_time_raw,\n        \"@pi_time_std_err\": _get_pi_time_std_err,\n    }\n\n    return best_fit, metadata\n</code></pre>"},{"location":"API%20reference/fit/fit/#sqil_core.fit._fit.fit_qubit_relaxation_qp","title":"<code>fit_qubit_relaxation_qp(x_data, y_data, guess=None, bounds=(-np.inf, np.inf), maxfev=10000, ftol=1e-11)</code>","text":"<p>Fits a qubit relaxation model with quasiparticle (QP) effects using a biexponential decay function. The fitting procedure starts with an initial guess derived from a single exponential fit.</p> <p>f(x) = A * exp(|nQP| * (exp(-x / T1QP) - 1)) * exp(-x / T1R) + y0</p> \\[f(x) = A \\exp\\left( |\\text{n}_{\\text{QP}}| \\left( \\exp\\left(-\\frac{x}{T_{1QP}}\\right) - 1 \\right) \\right) \\exp\\left(-\\frac{x}{T_{1R}}\\right) + y_0\\] <p>Parameters:</p> Name Type Description Default <code>x_data</code> <code>ndarray</code> <p>Time data points for the relaxation curve.</p> required <code>y_data</code> <code>ndarray</code> <p>Measured relaxation data.</p> required <code>guess</code> <code>list[float]</code> <p>Initial parameter guesses. If None, a default guess is computed using a single exponential fit.</p> <code>None</code> <code>bounds</code> <code>tuple[list[float], list[float]]</code> <p>The bounds for the fit parameters in the format [(min, max), ...]. If None, reasonable bounds based on the initial guess are applied.</p> <code>(-inf, inf)</code> <code>maxfev</code> <code>int</code> <p>Maximum number of function evaluations allowed for the curve fitting.</p> <code>10000</code> <code>ftol</code> <code>float</code> <p>Relative tolerance for convergence in the least-squares optimization.</p> <code>1e-11</code> <code>fixed_params</code> <code>list[int]</code> <p>A list of indices representing parameters in the initial guess that should remain unchanged during the fitting process.</p> <code>None</code> <p>Returns:</p> Type Description <code>FitResult</code> <p>A <code>FitResult</code> object containing: - Fitted parameters (<code>params</code>). - Standard errors (<code>std_err</code>). - Goodness-of-fit metrics (<code>rmse</code>, root mean squared error). - A callable <code>predict</code> function for generating fitted responses.</p> Source code in <code>sqil_core/fit/_fit.py</code> <pre><code>@fit_input\n@fit_output\ndef fit_qubit_relaxation_qp(\n    x_data: np.ndarray,\n    y_data: np.ndarray,\n    guess: list[float] | None = None,\n    bounds: list[tuple[float]] | tuple = (-np.inf, np.inf),\n    maxfev: int = 10000,\n    ftol: float = 1e-11,\n) -&gt; FitResult:\n    r\"\"\"\n    Fits a qubit relaxation model with quasiparticle (QP) effects using a\n    biexponential decay function. The fitting procedure starts with an initial\n    guess derived from a single exponential fit.\n\n    f(x) = A * exp(|nQP| * (exp(-x / T1QP) - 1)) * exp(-x / T1R) + y0\n\n    $$f(x) = A \\exp\\left( |\\text{n}_{\\text{QP}}| \\left( \\exp\\left(-\\frac{x}{T_{1QP}}\\right)\n    - 1 \\right) \\right) \\exp\\left(-\\frac{x}{T_{1R}}\\right) + y_0$$\n\n    Parameters\n    ----------\n    x_data : np.ndarray\n        Time data points for the relaxation curve.\n\n    y_data : np.ndarray\n        Measured relaxation data.\n\n    guess : list[float], optional\n        Initial parameter guesses. If None, a default guess is computed\n        using a single exponential fit.\n\n    bounds : tuple[list[float], list[float]], optional\n        The bounds for the fit parameters in the format [(min, max), ...].\n        If None, reasonable bounds based on the initial guess are applied.\n\n    maxfev : int, optional, default=10000\n        Maximum number of function evaluations allowed for the curve fitting.\n\n    ftol : float, optional, default=1e-11\n        Relative tolerance for convergence in the least-squares optimization.\n\n    fixed_params : list[int], optional, default: None\n        A list of indices representing parameters in the initial guess that should\n        remain unchanged during the fitting process.\n\n    Returns\n    -------\n    FitResult\n        A `FitResult` object containing:\n        - Fitted parameters (`params`).\n        - Standard errors (`std_err`).\n        - Goodness-of-fit metrics (`rmse`, root mean squared error).\n        - A callable `predict` function for generating fitted responses.\n    \"\"\"\n\n    # Use a single exponential fit for initial parameter guesses\n    from scipy.optimize import curve_fit\n\n    def single_exp(x, a, tau, c):\n        return a * np.exp(-x / tau) + c\n\n    single_guess = [y_data[0] - y_data[-1], np.mean(x_data), y_data[-1]]\n    single_popt, _ = curve_fit(single_exp, x_data, y_data, p0=single_guess)\n\n    a_guess, T1R_guess, c_guess = single_popt\n    T1QP_guess = 0.1 * T1R_guess\n    nQP_guess = 1.0\n\n    # Default initial guess\n    if guess is None:\n        guess = [a_guess * np.exp(1.0), 2.0 * T1R_guess, c_guess, T1QP_guess, nQP_guess]\n\n    # Default parameter bounds\n    if bounds is None:\n        bounds = (\n            [\n                -20.0 * np.abs(a_guess),\n                1.0e-1 * T1R_guess,\n                -10.0 * np.abs(c_guess),\n                1.0e-4 * T1R_guess,\n                0.0,\n            ],\n            [\n                20.0 * np.abs(a_guess),\n                1.0e3 * T1R_guess,\n                10.0 * np.abs(c_guess),\n                10.0 * T1R_guess,\n                1.0e3,\n            ],\n        )\n\n    res = curve_fit(\n        _models.qubit_relaxation_qp,\n        x_data,\n        y_data,\n        p0=guess,\n        bounds=bounds,\n        maxfev=maxfev,\n        ftol=ftol,\n        full_output=True,\n    )\n\n    return res, {\n        \"param_names\": [\"A\", \"T1R\", \"y0\", \"T1QP\", \"nQP\"],\n        \"predict\": _models.qubit_relaxation_qp,\n    }\n</code></pre>"},{"location":"API%20reference/fit/fit/#sqil_core.fit._fit.fit_skewed_lorentzian","title":"<code>fit_skewed_lorentzian(x_data, y_data)</code>","text":"<p>Fits a skewed Lorentzian model to the given data using least squares optimization.</p> <p>This function performs a two-step fitting process to find the best-fitting parameters for a skewed Lorentzian model. The first fitting step provides initial estimates for the parameters, and the second step refines those estimates using a full model fit.</p> <p>L(f) = A1 + A2 * (f - fr) + (A3 + A4 * (f - fr)) / [1 + (2 * Q_tot * ((f / fr) - 1))\u00b2]</p> \\[L(f) = A_1 + A_2 \\cdot (f - f_r)+ \\frac{A_3 + A_4 \\cdot (f - f_r)}{1 + 4 Q_{\\text{tot}}^2 \\left( \\frac{f - f_r}{f_r} \\right)^2}\\] <p>Parameters:</p> Name Type Description Default <code>x_data</code> <code>ndarray</code> <p>A 1D numpy array containing the x data points for the fit.</p> required <code>y_data</code> <code>ndarray</code> <p>A 1D numpy array containing the y data points for the fit.</p> required <p>Returns:</p> Type Description <code>FitResult</code> <p>A <code>FitResult</code> object containing: - Fitted parameters (<code>params</code>). - Standard errors (<code>std_err</code>). - Goodness-of-fit metrics (<code>red_chi2</code>). - A callable <code>predict</code> function for generating fitted responses.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; fit_result = fit_skewed_lorentzian(x_data, y_data)\n&gt;&gt;&gt; fit_result.summary()\n</code></pre> Source code in <code>sqil_core/fit/_fit.py</code> <pre><code>@fit_output\ndef fit_skewed_lorentzian(x_data: np.ndarray, y_data: np.ndarray):\n    r\"\"\"\n    Fits a skewed Lorentzian model to the given data using least squares optimization.\n\n    This function performs a two-step fitting process to find the best-fitting parameters for a skewed Lorentzian model.\n    The first fitting step provides initial estimates for the parameters, and the second step refines those estimates\n    using a full model fit.\n\n    L(f) = A1 + A2 * (f - fr) + (A3 + A4 * (f - fr)) / [1 + (2 * Q_tot * ((f / fr) - 1))\u00b2]\n\n    $$L(f) = A_1 + A_2 \\cdot (f - f_r)+ \\frac{A_3 + A_4 \\cdot (f - f_r)}{1\n    + 4 Q_{\\text{tot}}^2 \\left( \\frac{f - f_r}{f_r} \\right)^2}$$\n\n    Parameters\n    ----------\n    x_data : np.ndarray\n        A 1D numpy array containing the x data points for the fit.\n\n    y_data : np.ndarray\n        A 1D numpy array containing the y data points for the fit.\n\n    Returns\n    -------\n    FitResult\n        A `FitResult` object containing:\n        - Fitted parameters (`params`).\n        - Standard errors (`std_err`).\n        - Goodness-of-fit metrics (`red_chi2`).\n        - A callable `predict` function for generating fitted responses.\n\n    Examples\n    --------\n    &gt;&gt;&gt; fit_result = fit_skewed_lorentzian(x_data, y_data)\n    &gt;&gt;&gt; fit_result.summary()\n    \"\"\"\n    A1a = np.minimum(y_data[0], y_data[-1])\n    A3a = -np.max(y_data)\n    fra = x_data[np.argmin(y_data)]\n\n    # First fit to get initial estimates for the more complex fit\n    def residuals(p, x, y):\n        A2, A4, Q_tot = p\n        err = y - (\n            A1a\n            + A2 * (x - fra)\n            + (A3a + A4 * (x - fra)) / (1.0 + 4.0 * Q_tot**2 * ((x - fra) / fra) ** 2)\n        )\n        return err\n\n    p0 = [0.0, 0.0, 1e3]\n    p_final, _ = leastsq(residuals, p0, args=(np.array(x_data), np.array(y_data)))\n    A2a, A4a, Q_tota = p_final\n\n    # Full parameter fit\n    def residuals2(p, x, y):\n        A1, A2, A3, A4, fr, Q_tot = p\n        err = y - (\n            A1\n            + A2 * (x - fr)\n            + (A3 + A4 * (x - fr)) / (1.0 + 4.0 * Q_tot**2 * ((x - fr) / fr) ** 2)\n        )\n        return err\n\n    p0 = [A1a, A2a, A3a, A4a, fra, Q_tota]\n    popt, pcov, infodict, errmsg, ier = leastsq(\n        residuals2, p0, args=(np.array(x_data), np.array(y_data)), full_output=True\n    )\n    # Since Q_tot is always present as a square it may turn out negative\n    popt[-1] = np.abs(popt[-1])\n\n    return (\n        (popt, pcov, infodict, errmsg, ier),\n        {\n            \"predict\": lambda x: _models.skewed_lorentzian(x, *popt),\n            \"param_names\": [\"A1\", \"A2\", \"A3\", \"A4\", \"fr\", \"Q_tot\"],\n        },\n    )\n</code></pre>"},{"location":"API%20reference/fit/fit/#sqil_core.fit._fit.transform_data","title":"<code>transform_data(data, transform_type='optm', params=None, deg=True, inv_transform=False, full_output=False)</code>","text":"<p>Transforms complex-valued data using various transformation methods, including optimization-based alignment, real/imaginary extraction, amplitude, and phase.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>The complex-valued data to be transformed.</p> required <code>transform_type</code> <code>str</code> <p>The type of transformation to apply. Options include: - 'optm' (default): Optimized translation and rotation. - 'trrt': Translation and rotation using provided params. - 'real': Extract the real part. - 'imag': Extract the imaginary part. - 'ampl': Compute the amplitude. - 'angl': Compute the phase (in degrees if <code>deg=True</code>).</p> <code>'optm'</code> <code>params</code> <code>list</code> <p>Transformation parameters [x0, y0, phi]. If None and <code>transform_type='optm'</code>, parameters are estimated automatically.</p> <code>None</code> <code>deg</code> <code>bool</code> <p>If True, phase transformations return values in degrees (default: True).</p> <code>True</code> <code>inv_transform</code> <code>bool</code> <p>If true returns transformed data and the function to perform the inverse transform.</p> <code>False</code> <code>full_output</code> <code>bool</code> <p>If True, returns transformed data, the function to perform the inverse transform, transformation parameters, and residuals.</p> <code>False</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>The transformed data.</p> <code>tuple[np.ndarray, list, np.ndarray] (if `full_output=True`)</code> <p>Transformed data, transformation parameters, and residuals.</p> Notes <ul> <li>The function applies different transformations based on <code>transform_type</code>.</li> <li>If <code>optm</code> is selected and <code>params</code> is not provided, an optimization routine   is used to determine the best transformation parameters.</li> </ul> Example <p>data = np.array([1 + 1j, 2 + 2j, 3 + 3j]) transformed, params, residuals = transform_data(data, full_output=True) print(transformed, params, residuals)</p> Source code in <code>sqil_core/fit/_fit.py</code> <pre><code>def transform_data(\n    data: np.ndarray,\n    transform_type: str = \"optm\",\n    params: list = None,\n    deg: bool = True,\n    inv_transform: bool = False,\n    full_output: bool = False,\n) -&gt; (\n    np.ndarray\n    | tuple[np.ndarray, Callable]\n    | tuple[np.ndarray, Callable, list, np.ndarray]\n):\n    \"\"\"\n    Transforms complex-valued data using various transformation methods, including\n    optimization-based alignment, real/imaginary extraction, amplitude, and phase.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        The complex-valued data to be transformed.\n\n    transform_type : str, optional\n        The type of transformation to apply. Options include:\n        - 'optm' (default): Optimized translation and rotation.\n        - 'trrt': Translation and rotation using provided params.\n        - 'real': Extract the real part.\n        - 'imag': Extract the imaginary part.\n        - 'ampl': Compute the amplitude.\n        - 'angl': Compute the phase (in degrees if `deg=True`).\n\n    params : list, optional\n        Transformation parameters [x0, y0, phi]. If None and `transform_type='optm'`,\n        parameters are estimated automatically.\n\n    deg : bool, optional\n        If True, phase transformations return values in degrees (default: True).\n\n    inv_transform : bool, optional\n        If true returns transformed data and the function to perform the inverse transform.\n\n    full_output : bool, optional\n        If True, returns transformed data, the function to perform the inverse transform,\n        transformation parameters, and residuals.\n\n    Returns\n    -------\n    np.ndarray\n        The transformed data.\n\n    tuple[np.ndarray, list, np.ndarray] (if `full_output=True`)\n        Transformed data, transformation parameters, and residuals.\n\n    Notes\n    -----\n    - The function applies different transformations based on `transform_type`.\n    - If `optm` is selected and `params` is not provided, an optimization routine\n      is used to determine the best transformation parameters.\n\n    Example\n    -------\n    &gt;&gt;&gt; data = np.array([1 + 1j, 2 + 2j, 3 + 3j])\n    &gt;&gt;&gt; transformed, params, residuals = transform_data(data, full_output=True)\n    &gt;&gt;&gt; print(transformed, params, residuals)\n    \"\"\"\n\n    def transform(data, x0, y0, phi):\n        return (data - x0 - 1.0j * y0) * np.exp(1.0j * phi)\n\n    def _inv_transform(data, x0, y0, phi):\n        return data * np.exp(-1.0j * phi) + x0 + 1.0j * y0\n\n    def opt_transform(data):\n        \"\"\"Finds optimal transformation parameters.\"\"\"\n\n        def transform_err(x):\n            return np.sum((transform(data, x[0], x[1], x[2]).imag) ** 2)\n\n        res = minimize(\n            fun=transform_err,\n            method=\"Nelder-Mead\",\n            x0=[\n                np.mean(data.real),\n                np.mean(data.imag),\n                -np.arctan2(np.std(data.imag), np.std(data.real)),\n            ],\n            options={\"maxiter\": 1000},\n        )\n\n        params = res.x\n        transformed_data = transform(data, *params)\n        if transformed_data[0] &lt; transformed_data[-1]:\n            params[2] += np.pi\n        return params\n\n    # Normalize transform_type\n    transform_type = str(transform_type).lower()\n    if transform_type.startswith((\"op\", \"pr\")):\n        transform_type = \"optm\"\n    elif transform_type.startswith(\"translation+rotation\"):\n        transform_type = \"trrt\"\n    elif transform_type.startswith((\"re\", \"qu\")):\n        transform_type = \"real\"\n    elif transform_type.startswith((\"im\", \"in\")):\n        transform_type = \"imag\"\n    elif transform_type.startswith(\"am\"):\n        transform_type = \"ampl\"\n    elif transform_type.startswith((\"ph\", \"an\")):\n        transform_type = \"angl\"\n\n    # Compute parameters if needed\n    if transform_type == \"optm\" and params is None:\n        params = opt_transform(data)\n\n    # Apply transformation\n    if transform_type in [\"optm\", \"trrt\"]:\n        transformed_data = transform(data, *params).real\n        residual = transform(data, *params).imag\n    elif transform_type == \"real\":\n        transformed_data = data.real\n        residual = data.imag\n    elif transform_type == \"imag\":\n        transformed_data = data.imag\n        residual = data.real\n    elif transform_type == \"ampl\":\n        transformed_data = np.abs(data)\n        residual = np.unwrap(np.angle(data))\n        if deg:\n            residual = np.degrees(residual)\n    elif transform_type == \"angl\":\n        transformed_data = np.unwrap(np.angle(data))\n        residual = np.abs(data)\n        if deg:\n            transformed_data = np.degrees(transformed_data)\n\n    inv_transform_fun = lambda data: _inv_transform(data, *params)\n\n    if full_output:\n        return np.array(transformed_data), inv_transform_fun, params, residual\n    if inv_transform:\n        return np.array(transformed_data), inv_transform_fun\n    return np.array(transformed_data)\n</code></pre>"},{"location":"API%20reference/fit/guess/","title":"Guess","text":""},{"location":"API%20reference/fit/guess/#sqil_core.fit._guess.decaying_exp_guess","title":"<code>decaying_exp_guess(x_data, y_data)</code>","text":"<p>Robust initial guess for decaying exponential even if the full decay isn't captured.</p> Source code in <code>sqil_core/fit/_guess.py</code> <pre><code>def decaying_exp_guess(x_data: np.ndarray, y_data: np.ndarray) -&gt; list[float]:\n    \"\"\"\n    Robust initial guess for decaying exponential even if the full decay isn't captured.\n    \"\"\"\n    x = np.asarray(x_data)\n    y = np.asarray(y_data)\n\n    # Rough estimate of y0 with a local min or mean of last N points\n    N_tail = max(3, int(0.1 * len(y)))\n    tail_mean = np.mean(y[-N_tail:])\n    y0 = min(np.min(y), tail_mean)\n\n    # Amplitude\n    A = y[0] - y0\n    A = np.clip(A, 1e-12, None)\n\n    # Ensure sign consistency\n    if np.abs(np.max(y) - y0) &gt; np.abs(A):\n        A = np.max(y) - y0\n\n    # Estimate tau using log-linear fit of the first ~30% of data\n    N_fit = max(5, int(0.3 * len(x)))\n    y_fit = y[:N_fit] - y0\n    mask = y_fit &gt; 0  # log() only valid on positive values\n\n    if np.count_nonzero(mask) &gt; 1:\n        x_fit = x[:N_fit][mask]\n        log_y = np.log(y_fit[mask])\n        slope, intercept = np.polyfit(x_fit, log_y, 1)\n        tau = -1 / slope if slope &lt; 0 else (x[-1] - x[0]) / 3\n    else:\n        tau = (x[-1] - x[0]) / 3\n\n    tau = max(tau, np.finfo(float).eps)\n\n    return [A, tau, y0]\n</code></pre>"},{"location":"API%20reference/fit/guess/#sqil_core.fit._guess.decaying_oscillations_bounds","title":"<code>decaying_oscillations_bounds(x_data, y_data, guess)</code>","text":"<p>Generate realistic bounds for decaying oscillation parameters.</p> Source code in <code>sqil_core/fit/_guess.py</code> <pre><code>def decaying_oscillations_bounds(x_data, y_data, guess):\n    \"\"\"Generate realistic bounds for decaying oscillation parameters.\"\"\"\n    x_data = np.asarray(x_data)\n    y_data = np.asarray(y_data)\n\n    A, tau, y0, phi, T = guess\n    lower, upper = oscillations_bounds(x_data, y_data, [A, y0, phi, T])\n\n    tau_min = 0.01 * tau\n    tau_max = 10 * tau\n\n    lower.insert(1, tau_min)\n    upper.insert(1, tau_max)\n    return (lower, upper)\n</code></pre>"},{"location":"API%20reference/fit/guess/#sqil_core.fit._guess.decaying_oscillations_guess","title":"<code>decaying_oscillations_guess(x_data, y_data, num_init=10)</code>","text":"<p>Generate robust initial guesses for decaying oscillation parameters.</p> Source code in <code>sqil_core/fit/_guess.py</code> <pre><code>def decaying_oscillations_guess(x_data, y_data, num_init=10):\n    \"\"\"Generate robust initial guesses for decaying oscillation parameters.\"\"\"\n    x_data = np.asarray(x_data)\n    y_data = np.asarray(y_data)\n    dx = np.mean(np.diff(x_data))\n\n    # Oscillations params\n    A, y0_candidates, phi_candidates, T = oscillations_guess(x_data, y_data, num_init)\n\n    # Decay time (tau) from log-envelope\n    try:\n        y_demeaned = y_data - np.mean(y_data)\n        envelope = np.abs(hilbert(y_demeaned))\n        log_env = np.log(np.clip(envelope, 1e-10, None))\n        slope, _ = np.polyfit(x_data, log_env, 1)\n        tau = -1 / slope if slope &lt; 0 else np.ptp(x_data)\n    except Exception:\n        tau = np.ptp(x_data)\n\n    # Rough estimate of y0 with a local min or mean of last N points\n    N_tail = max(3, int(0.1 * len(y_data)))\n    tail_mean = np.mean(y_data[-N_tail:])\n    y0_decay = min(np.min(y_data), tail_mean)\n    y0_candidates.append(y0_decay)\n\n    return [A, tau, y0_candidates, phi_candidates, T]\n</code></pre>"},{"location":"API%20reference/fit/guess/#sqil_core.fit._guess.estimate_peak","title":"<code>estimate_peak(x_data, y_data)</code>","text":"<p>Estimates the key properties of a peak or dip in 1D data.</p> <p>This function analyzes a one-dimensional dataset to identify whether the dominant feature is a peak or dip and then estimates the following parameters: - The position of the peak/dip (x0) - The full width at half maximum (FWHM) - The peak/dip height - The baseline value (y0) - A flag indicating if it is a peak (True) or a dip (False)</p> <p>Parameters:</p> Name Type Description Default <code>x_data</code> <code>ndarray</code> <p>Array of x-values.</p> required <code>y_data</code> <code>ndarray</code> <p>Array of y-values corresponding to <code>x_data</code>.</p> required <p>Returns:</p> Name Type Description <code>x0</code> <code>float</code> <p>The x-position of the peak or dip.</p> <code>fwhm</code> <code>float</code> <p>Estimated full width at half maximum.</p> <code>peak_height</code> <code>float</code> <p>Height (or depth) of the peak or dip relative to the baseline.</p> <code>y0</code> <code>float</code> <p>Baseline level from which the peak/dip is measured.</p> <code>is_peak</code> <code>bool</code> <p>True if the feature is a peak; False if it is a dip.</p> Notes <ul> <li>The function uses the median of <code>y_data</code> to determine whether the dominant   feature is a peak or a dip.</li> <li>FWHM is estimated using the positions where the signal crosses the half-max level.</li> <li>If fewer than two crossings are found, a fallback FWHM is estimated as 1/10th   of the x-range.</li> </ul> Source code in <code>sqil_core/fit/_guess.py</code> <pre><code>def estimate_peak(\n    x_data: np.ndarray, y_data: np.ndarray\n) -&gt; tuple[float, float, float, float, bool]:\n    \"\"\"\n    Estimates the key properties of a peak or dip in 1D data.\n\n    This function analyzes a one-dimensional dataset to identify whether the dominant\n    feature is a peak or dip and then estimates the following parameters:\n    - The position of the peak/dip (x0)\n    - The full width at half maximum (FWHM)\n    - The peak/dip height\n    - The baseline value (y0)\n    - A flag indicating if it is a peak (True) or a dip (False)\n\n    Parameters\n    ----------\n    x_data : np.ndarray\n        Array of x-values.\n    y_data : np.ndarray\n        Array of y-values corresponding to `x_data`.\n\n    Returns\n    -------\n    x0 : float\n        The x-position of the peak or dip.\n    fwhm : float\n        Estimated full width at half maximum.\n    peak_height : float\n        Height (or depth) of the peak or dip relative to the baseline.\n    y0 : float\n        Baseline level from which the peak/dip is measured.\n    is_peak : bool\n        True if the feature is a peak; False if it is a dip.\n\n    Notes\n    -----\n    - The function uses the median of `y_data` to determine whether the dominant\n      feature is a peak or a dip.\n    - FWHM is estimated using the positions where the signal crosses the half-max level.\n    - If fewer than two crossings are found, a fallback FWHM is estimated as 1/10th\n      of the x-range.\n    \"\"\"\n\n    x, y = x_data, y_data\n    y_median = np.median(y)\n    y_max, y_min = np.max(y), np.min(y)\n\n    # Determine if it's a peak or dip\n    if y_max - y_median &gt;= y_median - y_min:\n        idx = np.argmax(y)\n        is_peak = True\n        y0 = y_min\n        peak_height = y_max - y0\n    else:\n        idx = np.argmin(y)\n        is_peak = False\n        y0 = y_max\n        peak_height = y0 - y_min\n\n    x0 = x[idx]\n\n    # Estimate FWHM using half-max crossings\n    half_max = y0 + (peak_height / 2.0 if is_peak else -peak_height / 2.0)\n    crossings = np.where(np.diff(np.sign(y - half_max)))[0]\n    if len(crossings) &gt;= 2:\n        fwhm = np.abs(x[crossings[-1]] - x[crossings[0]])\n    else:\n        fwhm = (x[-1] - x[0]) / 10.0\n\n    return x0, fwhm, peak_height, y0, is_peak\n</code></pre>"},{"location":"API%20reference/fit/guess/#sqil_core.fit._guess.gaussian_bounds","title":"<code>gaussian_bounds(x_data, y_data, guess)</code>","text":"<p>Guess gaussian fit bounds.</p> Source code in <code>sqil_core/fit/_guess.py</code> <pre><code>def gaussian_bounds(x_data, y_data, guess):\n    \"\"\"Guess gaussian fit bounds.\"\"\"\n    x, y = x_data, y_data\n    A, *_ = guess\n\n    x_span = np.max(x) - np.min(x)\n    sigma_min = (x[1] - x[0]) / 10 if len(x) &gt; 1 else x_span / 100\n    sigma_max = x_span\n    A_abs = np.abs(A)\n\n    bounds = (\n        [-10 * A_abs, np.min(x) - 0.1 * x_span, sigma_min, np.min(y) - 0.5 * A_abs],\n        [10 * A_abs, np.max(x) + 0.1 * x_span, sigma_max, np.max(y) + 0.5 * A_abs],\n    )\n    return bounds\n</code></pre>"},{"location":"API%20reference/fit/guess/#sqil_core.fit._guess.gaussian_guess","title":"<code>gaussian_guess(x_data, y_data)</code>","text":"<p>Guess gaussian fit parameters.</p> Source code in <code>sqil_core/fit/_guess.py</code> <pre><code>def gaussian_guess(x_data, y_data):\n    \"\"\"Guess gaussian fit parameters.\"\"\"\n    x0, fwhm, peak_height, y0, is_peak = estimate_peak(x_data, y_data)\n\n    sigma = fwhm / (2 * np.sqrt(2 * np.log(2)))  # Convert FWHM to \u03c3\n\n    A = peak_height * sigma * np.sqrt(2 * np.pi)\n    if not is_peak:\n        A = -A\n\n    guess = [A, x0, sigma, y0]\n    return guess\n</code></pre>"},{"location":"API%20reference/fit/guess/#sqil_core.fit._guess.lorentzian_bounds","title":"<code>lorentzian_bounds(x_data, y_data, guess)</code>","text":"<p>Guess lorentzian fit bounds.</p> Source code in <code>sqil_core/fit/_guess.py</code> <pre><code>def lorentzian_bounds(x_data, y_data, guess):\n    \"\"\"Guess lorentzian fit bounds.\"\"\"\n    x, y = x_data, y_data\n    A, *_ = guess\n\n    x_span = np.max(x) - np.min(x)\n    A_abs = np.abs(A) if A != 0 else 1.0\n    fwhm_min = (x[1] - x[0]) if len(x) &gt; 1 else x_span / 10\n\n    bounds = (\n        [-10 * A_abs, np.min(x) - 0.1 * x_span, fwhm_min, np.min(y) - 0.5 * A_abs],\n        [+10 * A_abs, np.max(x) + 0.1 * x_span, x_span, np.max(y) + 0.5 * A_abs],\n    )\n    return bounds\n</code></pre>"},{"location":"API%20reference/fit/guess/#sqil_core.fit._guess.lorentzian_guess","title":"<code>lorentzian_guess(x_data, y_data)</code>","text":"<p>Guess lorentzian fit parameters.</p> Source code in <code>sqil_core/fit/_guess.py</code> <pre><code>def lorentzian_guess(x_data, y_data):\n    \"\"\"Guess lorentzian fit parameters.\"\"\"\n    x0, fwhm, peak_height, y0, is_peak = estimate_peak(x_data, y_data)\n\n    # Compute A from peak height = 2A / FWHM\n    A = (peak_height * fwhm) / 2.0\n    if not is_peak:\n        A = -A\n\n    guess = [A, x0, fwhm, y0]\n    return guess\n</code></pre>"},{"location":"API%20reference/fit/guess/#sqil_core.fit._guess.oscillations_bounds","title":"<code>oscillations_bounds(x_data, y_data, guess)</code>","text":"<p>Generate realistic bounds for oscillation parameters.</p> Source code in <code>sqil_core/fit/_guess.py</code> <pre><code>def oscillations_bounds(x_data, y_data, guess):\n    \"\"\"Generate realistic bounds for oscillation parameters.\"\"\"\n    x_data = np.asarray(x_data)\n    y_data = np.asarray(y_data)\n\n    A, y0, phi, T = guess\n\n    # Add small offset to ensure bounds don't collaps\n    eps = 1e-12\n\n    A_min = 0.1 * A - eps\n    A_max = 10 * A\n\n    y0_min = np.min(y_data) - eps\n    y0_max = np.max(y_data)\n\n    phi_min = 0.0 - eps\n    phi_max = T  # reasonable 1-period wrap\n\n    T_min = 0.1 * T - eps\n    T_max = 10 * T\n\n    lower = [A_min, y0_min, phi_min, T_min]\n    upper = [A_max, y0_max, phi_max, T_max]\n    return (lower, upper)\n</code></pre>"},{"location":"API%20reference/fit/guess/#sqil_core.fit._guess.oscillations_guess","title":"<code>oscillations_guess(x_data, y_data, num_init=10)</code>","text":"<p>Generate robust initial guesses for oscillation parameters.</p> Source code in <code>sqil_core/fit/_guess.py</code> <pre><code>def oscillations_guess(x_data, y_data, num_init=10):\n    \"\"\"Generate robust initial guesses for oscillation parameters.\"\"\"\n    x_data = np.asarray(x_data)\n    y_data = np.asarray(y_data)\n    dx = np.mean(np.diff(x_data))\n\n    # Amplitude guess (robust against outliers)\n    A = (np.percentile(y_data, 95) - np.percentile(y_data, 5)) / 2\n\n    # Offset guess (tail median + mean)\n    y0_tail = np.median(y_data[-max(5, len(y_data) // 10) :])\n    y0_mean = np.mean(y_data)\n    y0_candidates = [y0_tail, y0_mean]\n\n    # FFT-based T (period)\n    y_demeaned = y_data - np.mean(y_data)\n    freqs = rfftfreq(len(x_data), d=dx)\n    spectrum = np.abs(rfft(y_demeaned))\n    peak_idx = np.argmax(spectrum[1:]) + 1  # Ignore DC\n    freq_peak = freqs[peak_idx]\n    T = 1 / freq_peak if freq_peak &gt; 0 else np.ptp(x_data)  # fallback to range\n\n    # Phase estimate from cross-correlation\n    cos_wave = np.cos(2 * np.pi * x_data / T)\n    lag = np.argmax(np.correlate(y_demeaned, cos_wave, mode=\"full\")) - len(x_data) + 1\n    phi_base = x_data[0] + lag * dx\n    phi_candidates = np.linspace(phi_base - T, phi_base + T, num_init)\n    phi_candidates = np.mod(phi_candidates, T)\n\n    return [A, y0_candidates, phi_candidates, T]\n</code></pre>"},{"location":"API%20reference/fit/quality/","title":"Quality","text":""},{"location":"API%20reference/fit/quality/#sqil_core.fit._quality.evaluate_fit_quality","title":"<code>evaluate_fit_quality(fit_metrics, recipe='nrmse')</code>","text":"<p>Evaluates the quality category of a fit based on a specified metric recipe.</p> <p>This function maps a numeric fit metric (e.g., NRMSE or AIC) to a qualitative fit quality category (GREAT, GOOD, ACCEPTABLE, BAD) using predefined thresholds. These thresholds are stored in the <code>FIT_QUALITY_THRESHOLDS</code> dictionary and must be provided for each supported recipe.</p> <p>Parameters:</p> Name Type Description Default <code>fit_metrics</code> <code>dict</code> <p>Dictionary containing computed metrics from a fit. Must include the key specified by <code>recipe</code>.</p> required <code>recipe</code> <code>str</code> <p>The name of the metric to evaluate quality against. Default is \"nrmse\".</p> <code>'nrmse'</code> <p>Returns:</p> Type Description <code>FitQuality</code> <p>A qualitative classification of the fit (GREAT, GOOD, ACCEPTABLE, BAD), represented by an enum or constant defined in <code>FitQuality</code>.</p> Source code in <code>sqil_core/fit/_quality.py</code> <pre><code>def evaluate_fit_quality(fit_metrics: dict, recipe: str = \"nrmse\") -&gt; FitQuality:\n    \"\"\"\n    Evaluates the quality category of a fit based on a specified metric recipe.\n\n    This function maps a numeric fit metric (e.g., NRMSE or AIC) to a qualitative\n    fit quality category (GREAT, GOOD, ACCEPTABLE, BAD) using predefined thresholds. These\n    thresholds are stored in the `FIT_QUALITY_THRESHOLDS` dictionary and must be\n    provided for each supported recipe.\n\n    Parameters\n    ----------\n    fit_metrics : dict\n        Dictionary containing computed metrics from a fit. Must include the key\n        specified by `recipe`.\n    recipe : str, optional\n        The name of the metric to evaluate quality against. Default is \"nrmse\".\n\n    Returns\n    -------\n    FitQuality\n        A qualitative classification of the fit (GREAT, GOOD, ACCEPTABLE, BAD), represented\n        by an enum or constant defined in `FitQuality`.\n    \"\"\"\n\n    value = fit_metrics.get(recipe)\n    if value is None:\n        raise KeyError(\n            f\"The metrics provided aren't sufficient to use recipe '{recipe}'\"\n        )\n\n    thresholds = FIT_QUALITY_THRESHOLDS.get(recipe)\n    if thresholds is None:\n        raise NotImplementedError(\n            f\"No fit quality threshold available for '{recipe}'.\"\n            + \" You can add them to 'FIT_QUALITY_THRESHOLDS'\"\n        )\n\n    for threshold, quality in thresholds:\n        if value &lt;= threshold:\n            return quality\n\n    return FitQuality.BAD\n</code></pre>"},{"location":"API%20reference/fit/quality/#sqil_core.fit._quality.format_fit_metrics","title":"<code>format_fit_metrics(fit_metrics, keys=None)</code>","text":"<p>Formats and summarizes selected fit metrics with qualitative evaluations.</p> <p>This function generates a human-readable table that reports selected fit metrics (e.g., reduced \u03c7\u00b2, R\u00b2, NRMSE) alongside their numerical values and qualitative quality assessments. Quality categories are determined using <code>evaluate_fit_quality</code>.</p> <p>Parameters:</p> Name Type Description Default <code>fit_metrics</code> <code>dict</code> <p>Dictionary of fit metrics to display. Should contain values for keys like \"red_chi2\", \"r2\", \"nrmse\", etc.</p> required <code>keys</code> <code>list of str</code> <p>Subset of metric keys to include in the output. If None, all available keys in <code>fit_metrics</code> are considered.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>A plain-text table summarizing the selected metrics with their values and associated quality labels.</p> Notes <ul> <li>Complex-valued R\u00b2 metrics are skipped.</li> <li>Keys are optionally renamed for output formatting (e.g., \"red_chi2\" \u2192 \"reduced \u03c7\u00b2\").</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; metrics = {\"red_chi2\": 1.2, \"r2\": 0.97, \"nrmse\": 0.05}\n&gt;&gt;&gt; print(format_fit_metrics(metrics))\nreduced \u03c7\u00b2   1.200e+00   GOOD\nR\u00b2           9.700e-01   GOOD\nnrmse        5.000e-02   GOOD\n</code></pre> Source code in <code>sqil_core/fit/_quality.py</code> <pre><code>def format_fit_metrics(fit_metrics: dict, keys: list[str] | None = None) -&gt; str:\n    \"\"\"\n    Formats and summarizes selected fit metrics with qualitative evaluations.\n\n    This function generates a human-readable table that reports selected fit metrics\n    (e.g., reduced \u03c7\u00b2, R\u00b2, NRMSE) alongside their numerical values and qualitative\n    quality assessments. Quality categories are determined using `evaluate_fit_quality`.\n\n    Parameters\n    ----------\n    fit_metrics : dict\n        Dictionary of fit metrics to display. Should contain values for keys like\n        \"red_chi2\", \"r2\", \"nrmse\", etc.\n    keys : list of str, optional\n        Subset of metric keys to include in the output. If None, all available keys\n        in `fit_metrics` are considered.\n\n    Returns\n    -------\n    str\n        A plain-text table summarizing the selected metrics with their values and\n        associated quality labels.\n\n    Notes\n    -----\n    - Complex-valued R\u00b2 metrics are skipped.\n    - Keys are optionally renamed for output formatting (e.g., \"red_chi2\" \u2192 \"reduced \u03c7\u00b2\").\n\n    Examples\n    --------\n    &gt;&gt;&gt; metrics = {\"red_chi2\": 1.2, \"r2\": 0.97, \"nrmse\": 0.05}\n    &gt;&gt;&gt; print(format_fit_metrics(metrics))\n    reduced \u03c7\u00b2   1.200e+00   GOOD\n    R\u00b2           9.700e-01   GOOD\n    nrmse        5.000e-02   GOOD\n    \"\"\"\n\n    table_data = []\n\n    if keys is None:\n        keys = fit_metrics.keys() if fit_metrics else []\n\n    # Print fit quality parameters\n    for key in keys:\n        value = fit_metrics[key]\n        quality = \"\"\n        # Evaluate reduced Chi-squared\n        if key == \"red_chi2\":\n            key = \"reduced \u03c7\u00b2\"\n            quality = evaluate_fit_quality(fit_metrics, \"red_chi2\")\n        # Evaluate R-squared\n        elif key == \"r2\":\n            # Skip if complex\n            if isinstance(value, complex):\n                continue\n            key = \"R\u00b2\"\n            quality = evaluate_fit_quality(fit_metrics, \"r2\")\n        # Normalized root mean square error NRMSE\n        # Normalized mean absolute error NMAE and\n        elif (key == \"nrmse\") or (key == \"nmae\"):\n            quality = evaluate_fit_quality(fit_metrics, key)\n        else:\n            continue\n\n        quality_label = str(quality)\n\n        table_data.append([key, f\"{value:.3e}\", quality_label])\n    return tabulate(table_data, tablefmt=\"plain\")\n</code></pre>"},{"location":"API%20reference/fit/quality/#sqil_core.fit._quality.get_best_fit","title":"<code>get_best_fit(fit_res_a, fit_res_b, recipe='nrmse_aic')</code>","text":"<p>Selects the better fit result according to a specified selection recipe.</p> <p>This function acts as a dispatcher to choose between two fit results using a predefined comparison strategy.</p> <p>Supported recipies:     - \"nrmse_aic\": uses NRMSE as primary metric and adjusts it with AIC if the         NRMSE are in the same quality category.</p> <p>Parameters:</p> Name Type Description Default <code>fit_res_a</code> <code>FitResult</code> <p>The first fit result object containing metrics and parameters.</p> required <code>fit_res_b</code> <code>FitResult</code> <p>The second fit result object containing metrics and parameters.</p> required <code>recipe</code> <code>Literal['nrmse_aic']</code> <p>The name of the comparison strategy to use.</p> <code>'nrmse_aic'</code> <p>Returns:</p> Type Description <code>FitResult</code> <p>The selected fit result, based on the comparison strategy.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; best_fit = get_best_fit(fit1, fit2)\n&gt;&gt;&gt; print(\"Best-fit parameters:\", best_fit.params)\n</code></pre> Source code in <code>sqil_core/fit/_quality.py</code> <pre><code>def get_best_fit(\n    fit_res_a: FitResult,\n    fit_res_b: FitResult,\n    recipe: Literal[\"nrmse_aic\"] = \"nrmse_aic\",\n):\n    \"\"\"\n    Selects the better fit result according to a specified selection recipe.\n\n    This function acts as a dispatcher to choose between two fit results using a\n    predefined comparison strategy.\n\n    Supported recipies:\n        - \"nrmse_aic\": uses NRMSE as primary metric and adjusts it with AIC if the\n            NRMSE are in the same quality category.\n\n    Parameters\n    ----------\n    fit_res_a : FitResult\n        The first fit result object containing metrics and parameters.\n    fit_res_b : FitResult\n        The second fit result object containing metrics and parameters.\n    recipe : Literal[\"nrmse_aic\"], optional\n        The name of the comparison strategy to use.\n\n    Returns\n    -------\n    FitResult\n        The selected fit result, based on the comparison strategy.\n\n    Examples\n    --------\n    &gt;&gt;&gt; best_fit = get_best_fit(fit1, fit2)\n    &gt;&gt;&gt; print(\"Best-fit parameters:\", best_fit.params)\n    \"\"\"\n\n    if recipe == \"nrmse_aic\":\n        return get_best_fit_nrmse_aic(fit_res_a, fit_res_b)\n    raise NotImplementedError(f\"Recipe {recipe} does not exist\")\n</code></pre>"},{"location":"API%20reference/fit/quality/#sqil_core.fit._quality.get_best_fit_nrmse_aic","title":"<code>get_best_fit_nrmse_aic(fit_res_a, fit_res_b, aic_rel_tol=0.01)</code>","text":"<p>Selects the better fit result based on NRMSE quality and AIC with complexity penalty.</p> <p>This function compares two fit results by first evaluating the normalized root mean squared error (NRMSE) using a quality categorization scheme. If the fits differ in NRMSE quality, the one with better quality is selected. If the qualities are equal, the function compares the Akaike Information Criterion (AIC), using a relative tolerance to determine statistical equivalence. When AIC values are within tolerance, the simpler model (with fewer parameters) is preferred.</p> <p>Parameters:</p> Name Type Description Default <code>fit_res_a</code> <code>FitResult</code> <p>The first FitResult object.</p> required <code>fit_res_b</code> <code>FitResult</code> <p>The second FitResult object.</p> required <code>aic_rel_tol</code> <code>float</code> <p>The relative tolerance for AIC comparison. If the relative difference in AIC is below this threshold, models are considered equally good, and complexity (number of parameters) is used as a tiebreaker. Default is 0.01.</p> <code>0.01</code> <p>Returns:</p> Type Description <code>FitResult</code> <p>The preferred fit result based on NRMSE category, AIC, and model simplicity.</p> Notes <ul> <li>If models are statistically equivalent in AIC and have the same complexity,   the first result is returned for consistency.</li> <li>If the minimum AIC is zero, relative delta AIC is replaced by its absolute counter   part, but still using the aic_rel_tol as tolerance.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; best_fit = get_best_fit_nrmse_aic(fit1, fit2)\n&gt;&gt;&gt; print(\"Selected model parameters:\", best_fit.params)\n</code></pre> Source code in <code>sqil_core/fit/_quality.py</code> <pre><code>def get_best_fit_nrmse_aic(\n    fit_res_a: FitResult, fit_res_b: FitResult, aic_rel_tol: float = 0.01\n):\n    \"\"\"\n    Selects the better fit result based on NRMSE quality and AIC with complexity penalty.\n\n    This function compares two fit results by first evaluating the normalized root\n    mean squared error (NRMSE) using a quality categorization scheme. If the fits\n    differ in NRMSE quality, the one with better quality is selected. If the\n    qualities are equal, the function compares the Akaike Information Criterion (AIC),\n    using a relative tolerance to determine statistical equivalence. When AIC values\n    are within tolerance, the simpler model (with fewer parameters) is preferred.\n\n    Parameters\n    ----------\n    fit_res_a : FitResult\n        The first FitResult object.\n    fit_res_b : FitResult\n        The second FitResult object.\n    aic_rel_tol : float, optional\n        The relative tolerance for AIC comparison. If the relative difference in AIC\n        is below this threshold, models are considered equally good, and complexity\n        (number of parameters) is used as a tiebreaker. Default is 0.01.\n\n    Returns\n    -------\n    FitResult\n        The preferred fit result based on NRMSE category, AIC, and model simplicity.\n\n    Notes\n    -----\n    - If models are statistically equivalent in AIC and have the same complexity,\n      the first result is returned for consistency.\n    - If the minimum AIC is zero, relative delta AIC is replaced by its absolute counter\n      part, but still using the aic_rel_tol as tolerance.\n\n    Examples\n    --------\n    &gt;&gt;&gt; best_fit = get_best_fit_nrmse_aic(fit1, fit2)\n    &gt;&gt;&gt; print(\"Selected model parameters:\", best_fit.params)\n    \"\"\"\n\n    quality_a = evaluate_fit_quality(fit_res_a.metrics)\n    quality_b = evaluate_fit_quality(fit_res_b.metrics)\n\n    # If NMRSE qualities are not in the same category, return the best one\n    if quality_a != quality_b:\n        return fit_res_a if quality_a &gt; quality_b else fit_res_b\n    aic_a = fit_res_a.metrics.get(\"aic\")\n    aic_b = fit_res_b.metrics.get(\"aic\")\n\n    # Use AIC to penalize fit complexity\n    if aic_a is None or aic_b is None:\n        raise ValueError(\"Missing AIC value in one of the fits\")\n    delta = abs(aic_a - aic_b)\n    min_aic = abs(min(aic_a, aic_b))\n    rel_delta = delta / min_aic if min_aic != 0 else delta\n    if rel_delta &lt; aic_rel_tol:\n        # Within tolerance: consider them equivalent, return simpler (fewer params)\n        len_a, len_b = len(fit_res_a.params), len(fit_res_b.params)\n        if len_a != len_b:\n            return fit_res_a if len_a &lt; len_b else fit_res_b\n        # Otherwise: arbitrary but consistent\n        return fit_res_a\n    # Outside tolerance: pick the one with lower AIC\n    return fit_res_a if aic_a &lt; aic_b else fit_res_b\n</code></pre>"},{"location":"API%20reference/resonator/resonator/","title":"Resonator","text":""},{"location":"API%20reference/resonator/resonator/#sqil_core.resonator._resonator.S11_reflection","title":"<code>S11_reflection(freq, a, alpha, tau, Q_tot, fr, Q_ext, phi, mag_bg=None)</code>","text":"<p>Calculates the S11 reflection coefficient for a superconducting resonator with an optional magnitude background.</p> <p>This function models the S11 reflection parameter, representing how much of an incident signal is reflected by a resonator. It includes both the resonator's frequency-dependent response and an optional magnitude background correction, providing a more accurate fit for experimental data.</p> <p>The S11 reflection is computed as:     S11(f) = env(f) * resonator(f) where:     - env(f) = a * mag_bg(f) * exp(i * \u03b1) * exp(2\u03c0i * (f - f\u2080) * \u03c4)       models the environmental response, including amplitude scaling, phase shifts,       time delays, and optional frequency-dependent magnitude background.     - resonator(f) = 1 - [2 * Q_tot / |Q_ext|] * exp(i * \u03c6) / [1 + 2i * Q_tot * (f / fr - 1)]       models the resonator's frequency response.</p> <p>Parameters:</p> Name Type Description Default <code>freq</code> <code>ndarray</code> <p>Array of frequency points (in Hz) at which to evaluate the S11 parameter.</p> required <code>a</code> <code>float</code> <p>Amplitude scaling factor for the environmental response.</p> required <code>alpha</code> <code>float</code> <p>Phase offset (in radians) for the environmental response.</p> required <code>tau</code> <code>float</code> <p>Time delay (in seconds) representing the signal path delay.</p> required <code>Q_tot</code> <code>float</code> <p>Total quality factor of the resonator (includes internal and external losses).</p> required <code>fr</code> <code>float</code> <p>Resonant frequency of the resonator (in Hz).</p> required <code>Q_ext</code> <code>float</code> <p>External quality factor, representing coupling losses to external circuitry.</p> required <code>phi</code> <code>float</code> <p>Additional phase shift (in radians) in the resonator response.</p> required <code>mag_bg</code> <code>ndarray or None</code> <p>Frequency-dependent magnitude background correction. If provided, it should be an array of the same shape as <code>freq</code>. Defaults to 1 (no correction).</p> <code>None</code> <p>Returns:</p> Name Type Description <code>S11</code> <code>ndarray</code> <p>Complex array representing the S11 reflection coefficient across the input frequencies.</p> Notes <ul> <li>Passing mag_bg = np.nan has the same effect of passing mag_bg = None</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; freq = np.linspace(4.9e9, 5.1e9, 500)  # Frequency sweep around 5 GHz\n&gt;&gt;&gt; mag_bg = freq**2 + 3 * freq  # Example magnitude background\n&gt;&gt;&gt; S11 = S11_reflection(freq, a=1.0, alpha=0.0, tau=1e-9,\n...                      Q_tot=5000, fr=5e9, Q_ext=10000, phi=0.0, mag_bg=mag_bg)\n&gt;&gt;&gt; import matplotlib.pyplot as plt\n&gt;&gt;&gt; plt.plot(freq, 20 * np.log10(np.abs(S11)))  # Plot magnitude in dB\n&gt;&gt;&gt; plt.xlabel(\"Frequency (Hz)\")\n&gt;&gt;&gt; plt.ylabel(\"S11 Magnitude (dB)\")\n&gt;&gt;&gt; plt.title(\"S11 Reflection Coefficient with Magnitude Background\")\n&gt;&gt;&gt; plt.show()\n</code></pre> Source code in <code>sqil_core/resonator/_resonator.py</code> <pre><code>def S11_reflection(\n    freq: np.ndarray,\n    a: float,\n    alpha: float,\n    tau: float,\n    Q_tot: float,\n    fr: float,\n    Q_ext: float,\n    phi: float,\n    mag_bg: np.ndarray | None = None,\n) -&gt; np.ndarray:\n    \"\"\"\n    Calculates the S11 reflection coefficient for a superconducting resonator with an optional magnitude background.\n\n    This function models the S11 reflection parameter, representing how much of an\n    incident signal is reflected by a resonator. It includes both the resonator's\n    frequency-dependent response and an optional magnitude background correction,\n    providing a more accurate fit for experimental data.\n\n    The S11 reflection is computed as:\n        S11(f) = env(f) * resonator(f)\n    where:\n        - env(f) = a * mag_bg(f) * exp(i * \u03b1) * exp(2\u03c0i * (f - f\u2080) * \u03c4)\n          models the environmental response, including amplitude scaling, phase shifts,\n          time delays, and optional frequency-dependent magnitude background.\n        - resonator(f) = 1 - [2 * Q_tot / |Q_ext|] * exp(i * \u03c6) / [1 + 2i * Q_tot * (f / fr - 1)]\n          models the resonator's frequency response.\n\n    Parameters\n    ----------\n    freq : np.ndarray\n        Array of frequency points (in Hz) at which to evaluate the S11 parameter.\n    a : float\n        Amplitude scaling factor for the environmental response.\n    alpha : float\n        Phase offset (in radians) for the environmental response.\n    tau : float\n        Time delay (in seconds) representing the signal path delay.\n    Q_tot : float\n        Total quality factor of the resonator (includes internal and external losses).\n    fr : float\n        Resonant frequency of the resonator (in Hz).\n    Q_ext : float\n        External quality factor, representing coupling losses to external circuitry.\n    phi : float\n        Additional phase shift (in radians) in the resonator response.\n    mag_bg : np.ndarray or None, optional\n        Frequency-dependent magnitude background correction. If provided, it should be\n        an array of the same shape as `freq`. Defaults to 1 (no correction).\n\n    Returns\n    -------\n    S11 : np.ndarray\n        Complex array representing the S11 reflection coefficient across the input frequencies.\n\n    Notes\n    -----\n    - Passing mag_bg = np.nan has the same effect of passing mag_bg = None\n\n    Examples\n    --------\n    &gt;&gt;&gt; freq = np.linspace(4.9e9, 5.1e9, 500)  # Frequency sweep around 5 GHz\n    &gt;&gt;&gt; mag_bg = freq**2 + 3 * freq  # Example magnitude background\n    &gt;&gt;&gt; S11 = S11_reflection(freq, a=1.0, alpha=0.0, tau=1e-9,\n    ...                      Q_tot=5000, fr=5e9, Q_ext=10000, phi=0.0, mag_bg=mag_bg)\n    &gt;&gt;&gt; import matplotlib.pyplot as plt\n    &gt;&gt;&gt; plt.plot(freq, 20 * np.log10(np.abs(S11)))  # Plot magnitude in dB\n    &gt;&gt;&gt; plt.xlabel(\"Frequency (Hz)\")\n    &gt;&gt;&gt; plt.ylabel(\"S11 Magnitude (dB)\")\n    &gt;&gt;&gt; plt.title(\"S11 Reflection Coefficient with Magnitude Background\")\n    &gt;&gt;&gt; plt.show()\n    \"\"\"\n    if mag_bg is None:\n        mag_bg = 1\n    elif np.isscalar(mag_bg) and np.isnan(mag_bg):\n        mag_bg = 1\n\n    env = a * mag_bg * np.exp(1j * alpha) * np.exp(2j * np.pi * (freq - freq[0]) * tau)\n    resonator = 1 - (2 * Q_tot / np.abs(Q_ext)) * np.exp(1j * phi) / (\n        1 + 2j * Q_tot * (freq / fr - 1)\n    )\n    return env * resonator\n</code></pre>"},{"location":"API%20reference/resonator/resonator/#sqil_core.resonator._resonator.S11_reflection_mesh","title":"<code>S11_reflection_mesh(freq, a, alpha, tau, Q_tot, Q_ext, fr, phi)</code>","text":"<p>Vectorized S11 reflection function.</p> <p>Parameters:</p> Name Type Description Default <code>freq</code> <code>(array, shape(N))</code> <p>Frequency points.</p> required <code>a</code> <code>scalar or array</code> <p>Parameters of the S11 model.</p> required <code>alpha</code> <code>scalar or array</code> <p>Parameters of the S11 model.</p> required <code>tau</code> <code>scalar or array</code> <p>Parameters of the S11 model.</p> required <code>Q_tot</code> <code>scalar or array</code> <p>Parameters of the S11 model.</p> required <code>Q_ext</code> <code>scalar or array</code> <p>Parameters of the S11 model.</p> required <code>fr</code> <code>scalar or array</code> <p>Parameters of the S11 model.</p> required <code>phi</code> <code>scalar or array</code> <p>Parameters of the S11 model.</p> required <p>Returns:</p> Name Type Description <code>S11</code> <code>array</code> <p>Complex reflection coefficient. Shape is (M1, M2, ..., N) where M1, M2, ... are the broadcasted shapes of the parameters.</p> Source code in <code>sqil_core/resonator/_resonator.py</code> <pre><code>def S11_reflection_mesh(freq, a, alpha, tau, Q_tot, Q_ext, fr, phi):\n    \"\"\"\n    Vectorized S11 reflection function.\n\n    Parameters\n    ----------\n    freq : array, shape (N,)\n        Frequency points.\n    a, alpha, tau, Q_tot, Q_ext, fr, phi : scalar or array\n        Parameters of the S11 model.\n\n    Returns\n    -------\n    S11 : array\n        Complex reflection coefficient. Shape is (M1, M2, ..., N) where M1, M2, ... are the broadcasted shapes of the parameters.\n    \"\"\"\n    # Ensure freq is at least 2D for broadcasting (1, N)\n    freq = np.atleast_1d(freq)  # (N,)\n\n    # Ensure all parameters are at least 1D arrays for broadcasting\n    a = np.atleast_1d(a)  # (M1,)\n    alpha = np.atleast_1d(alpha)  # (M2,)\n    tau = np.atleast_1d(tau)  # (M3,)\n    Q_tot = np.atleast_1d(Q_tot)  # (M4,)\n    Q_ext = np.atleast_1d(Q_ext)  # (M5,)\n    fr = np.atleast_1d(fr)  # (M6,)\n    phi = np.atleast_1d(phi)  # (M7,)\n\n    # Reshape frequency to (1, 1, ..., 1, N) for proper broadcasting\n    # This makes sure freq has shape (1, 1, ..., N)\n    freq = freq[np.newaxis, ...]\n\n    # Calculate the envelope part\n    env = (\n        a[..., np.newaxis]\n        * np.exp(1j * alpha[..., np.newaxis])\n        * np.exp(2j * np.pi * (freq - freq[..., 0:1]) * tau[..., np.newaxis])\n    )\n\n    # Calculate the resonator part\n    resonator = 1 - (\n        2 * Q_tot[..., np.newaxis] / np.abs(Q_ext[..., np.newaxis])\n    ) * np.exp(1j * phi[..., np.newaxis]) / (\n        1 + 2j * Q_tot[..., np.newaxis] * (freq / fr[..., np.newaxis] - 1)\n    )\n\n    return env * resonator\n</code></pre>"},{"location":"API%20reference/resonator/resonator/#sqil_core.resonator._resonator.S21_hanger","title":"<code>S21_hanger(freq, a, alpha, tau, Q_tot, fr, Q_ext, phi, mag_bg=None)</code>","text":"<p>Calculates the S21 transmission coefficient using the hanger resonator model with an optional magnitude background.</p> <p>This function models the S21 transmission parameter, which describes how much of an incident signal is transmitted through a superconducting resonator. The model combines the resonator's frequency-dependent response with an environmental background response and an optional magnitude background correction to more accurately reflect experimental data.</p> <p>The S21 transmission is computed as:     S21(f) = env(f) * resonator(f) where:     - env(f) = a * mag_bg(f) * exp(i * \u03b1) * exp(2\u03c0i * (f - f\u2080) * \u03c4)       models the environmental response, accounting for amplitude scaling, phase shifts,       and signal path delays.     - resonator(f) = 1 - [Q_tot / |Q_ext|] * exp(i * \u03c6) / [1 + 2i * Q_tot * (f / fr - 1)]       models the frequency response of the hanger-type resonator.</p> <p>Parameters:</p> Name Type Description Default <code>freq</code> <code>ndarray</code> <p>Array of frequency points (in Hz) at which to evaluate the S21 parameter.</p> required <code>a</code> <code>float</code> <p>Amplitude scaling factor for the environmental response.</p> required <code>alpha</code> <code>float</code> <p>Phase offset (in radians) for the environmental response.</p> required <code>tau</code> <code>float</code> <p>Time delay (in seconds) representing the signal path delay.</p> required <code>Q_tot</code> <code>float</code> <p>Total quality factor of the resonator (includes internal and external losses).</p> required <code>fr</code> <code>float</code> <p>Resonant frequency of the resonator (in Hz).</p> required <code>Q_ext</code> <code>float</code> <p>External quality factor, representing coupling losses to external circuitry.</p> required <code>phi</code> <code>float</code> <p>Additional phase shift (in radians) in the resonator response.</p> required <code>mag_bg</code> <code>ndarray or None</code> <p>Frequency-dependent magnitude background correction. If provided, it should be an array of the same shape as <code>freq</code>. Defaults to 1 (no correction).</p> <code>None</code> <p>Returns:</p> Name Type Description <code>S21</code> <code>ndarray</code> <p>Complex array representing the S21 transmission coefficient across the input frequencies.</p> Notes <ul> <li>Passing mag_bg = np.nan has the same effect of passing mag_bg = None</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; freq = np.linspace(4.9e9, 5.1e9, 500)  # Frequency sweep around 5 GHz\n&gt;&gt;&gt; mag_bg = freq**2 + 3 * freq  # Example magnitude background\n&gt;&gt;&gt; S21 = S21_hanger(freq, a=1.0, alpha=0.0, tau=1e-9,\n...                  Q_tot=5000, fr=5e9, Q_ext=10000, phi=0.0, mag_bg=mag_bg)\n&gt;&gt;&gt; import matplotlib.pyplot as plt\n&gt;&gt;&gt; plt.plot(freq, 20 * np.log10(np.abs(S21)))  # Plot magnitude in dB\n&gt;&gt;&gt; plt.xlabel(\"Frequency (Hz)\")\n&gt;&gt;&gt; plt.ylabel(\"S21 Magnitude (dB)\")\n&gt;&gt;&gt; plt.title(\"S21 Transmission Coefficient with Magnitude Background\")\n&gt;&gt;&gt; plt.show()\n</code></pre> Source code in <code>sqil_core/resonator/_resonator.py</code> <pre><code>def S21_hanger(\n    freq: np.ndarray,\n    a: float,\n    alpha: float,\n    tau: float,\n    Q_tot: float,\n    fr: float,\n    Q_ext: float,\n    phi: float,\n    mag_bg: np.ndarray | None = None,\n) -&gt; np.ndarray:\n    \"\"\"\n    Calculates the S21 transmission coefficient using the hanger resonator model with an optional magnitude background.\n\n    This function models the S21 transmission parameter, which describes how much of an\n    incident signal is transmitted through a superconducting resonator. The model combines\n    the resonator's frequency-dependent response with an environmental background response\n    and an optional magnitude background correction to more accurately reflect experimental data.\n\n    The S21 transmission is computed as:\n        S21(f) = env(f) * resonator(f)\n    where:\n        - env(f) = a * mag_bg(f) * exp(i * \u03b1) * exp(2\u03c0i * (f - f\u2080) * \u03c4)\n          models the environmental response, accounting for amplitude scaling, phase shifts,\n          and signal path delays.\n        - resonator(f) = 1 - [Q_tot / |Q_ext|] * exp(i * \u03c6) / [1 + 2i * Q_tot * (f / fr - 1)]\n          models the frequency response of the hanger-type resonator.\n\n    Parameters\n    ----------\n    freq : np.ndarray\n        Array of frequency points (in Hz) at which to evaluate the S21 parameter.\n    a : float\n        Amplitude scaling factor for the environmental response.\n    alpha : float\n        Phase offset (in radians) for the environmental response.\n    tau : float\n        Time delay (in seconds) representing the signal path delay.\n    Q_tot : float\n        Total quality factor of the resonator (includes internal and external losses).\n    fr : float\n        Resonant frequency of the resonator (in Hz).\n    Q_ext : float\n        External quality factor, representing coupling losses to external circuitry.\n    phi : float\n        Additional phase shift (in radians) in the resonator response.\n    mag_bg : np.ndarray or None, optional\n        Frequency-dependent magnitude background correction. If provided, it should be\n        an array of the same shape as `freq`. Defaults to 1 (no correction).\n\n    Returns\n    -------\n    S21 : np.ndarray\n        Complex array representing the S21 transmission coefficient across the input frequencies.\n\n    Notes\n    -----\n    - Passing mag_bg = np.nan has the same effect of passing mag_bg = None\n\n    Examples\n    --------\n    &gt;&gt;&gt; freq = np.linspace(4.9e9, 5.1e9, 500)  # Frequency sweep around 5 GHz\n    &gt;&gt;&gt; mag_bg = freq**2 + 3 * freq  # Example magnitude background\n    &gt;&gt;&gt; S21 = S21_hanger(freq, a=1.0, alpha=0.0, tau=1e-9,\n    ...                  Q_tot=5000, fr=5e9, Q_ext=10000, phi=0.0, mag_bg=mag_bg)\n    &gt;&gt;&gt; import matplotlib.pyplot as plt\n    &gt;&gt;&gt; plt.plot(freq, 20 * np.log10(np.abs(S21)))  # Plot magnitude in dB\n    &gt;&gt;&gt; plt.xlabel(\"Frequency (Hz)\")\n    &gt;&gt;&gt; plt.ylabel(\"S21 Magnitude (dB)\")\n    &gt;&gt;&gt; plt.title(\"S21 Transmission Coefficient with Magnitude Background\")\n    &gt;&gt;&gt; plt.show()\n    \"\"\"\n    if mag_bg is None:\n        mag_bg = 1\n    elif np.isscalar(mag_bg) and np.isnan(mag_bg):\n        mag_bg = 1\n\n    env = a * mag_bg * np.exp(1j * alpha) * np.exp(2j * np.pi * (freq - freq[0]) * tau)\n    resonator = 1 - (Q_tot / np.abs(Q_ext)) * np.exp(1j * phi) / (\n        1 + 2j * Q_tot * (freq / fr - 1)\n    )\n    return env * resonator\n</code></pre>"},{"location":"API%20reference/resonator/resonator/#sqil_core.resonator._resonator.S21_transmission","title":"<code>S21_transmission(freq, a, alpha, tau, Q_tot, fr, mag_bg=None)</code>","text":"<p>Computes the complex S21 transmission for a single-pole resonator model.</p> <p>This model describes the transmission response of a resonator. The total response includes both the resonator and a complex background envelope with a possible linear phase delay.</p> <p>Parameters:</p> Name Type Description Default <code>freq</code> <code>ndarray</code> <p>Frequency array (in Hz) over which the S21 transmission is evaluated.</p> required <code>a</code> <code>float</code> <p>Amplitude scaling factor of the background envelope.</p> required <code>alpha</code> <code>float</code> <p>Phase offset of the background envelope (in radians).</p> required <code>tau</code> <code>float</code> <p>Time delay in the background response (in seconds).</p> required <code>Q_tot</code> <code>float</code> <p>Total quality factor of the resonator.</p> required <code>fr</code> <code>float</code> <p>Resonant frequency of the resonator (in Hz).</p> required <code>mag_bg</code> <code>float or None</code> <p>Optional background magnitude scaling. If <code>None</code> or <code>NaN</code>, it defaults to 1.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Complex-valued S21 transmission array over the specified frequency range.</p> Source code in <code>sqil_core/resonator/_resonator.py</code> <pre><code>def S21_transmission(\n    freq: np.ndarray,\n    a: float,\n    alpha: float,\n    tau: float,\n    Q_tot: float,\n    fr: float,\n    mag_bg: float | None = None,\n) -&gt; np.ndarray:\n    \"\"\"\n    Computes the complex S21 transmission for a single-pole resonator model.\n\n    This model describes the transmission response of a resonator. The total response\n    includes both the resonator and a complex background envelope with a possible linear phase delay.\n\n    Parameters\n    ----------\n    freq : np.ndarray\n        Frequency array (in Hz) over which the S21 transmission is evaluated.\n    a : float\n        Amplitude scaling factor of the background envelope.\n    alpha : float\n        Phase offset of the background envelope (in radians).\n    tau : float\n        Time delay in the background response (in seconds).\n    Q_tot : float\n        Total quality factor of the resonator.\n    fr : float\n        Resonant frequency of the resonator (in Hz).\n    mag_bg : float or None, optional\n        Optional background magnitude scaling. If `None` or `NaN`, it defaults to 1.\n\n    Returns\n    -------\n    np.ndarray\n        Complex-valued S21 transmission array over the specified frequency range.\n    \"\"\"\n\n    if mag_bg is None:\n        mag_bg = 1\n    elif np.isscalar(mag_bg) and np.isnan(mag_bg):\n        mag_bg = 1\n\n    env = a * np.exp(1j * alpha) * np.exp(2j * np.pi * (freq - freq[0]) * tau)\n    resonator = 1 / (1 + 2j * Q_tot * (freq / fr - 1))\n    return env * resonator\n</code></pre>"},{"location":"API%20reference/resonator/resonator/#sqil_core.resonator._resonator.compute_Q_int","title":"<code>compute_Q_int(Q_tot, Q_ext_mag, Q_ext_phase)</code>","text":"<p>Compute Q_internal given Q_total and the manitude and phase of Q_external.</p> Source code in <code>sqil_core/resonator/_resonator.py</code> <pre><code>def compute_Q_int(Q_tot, Q_ext_mag, Q_ext_phase):\n    \"\"\"Compute Q_internal given Q_total and the manitude and phase of Q_external.\"\"\"\n    return 1 / (1 / Q_tot - np.real(1 / (Q_ext_mag * np.exp(-1j * Q_ext_phase))))\n</code></pre>"},{"location":"API%20reference/resonator/resonator/#sqil_core.resonator._resonator.fit_phase_vs_freq","title":"<code>fit_phase_vs_freq(freq, phase, theta0, Q_tot, fr)</code>","text":"<p>Fits the phase response of a superconducting resonator using an arctangent model.</p> <p>Reference: https://arxiv.org/abs/1410.3365 This function models the phase response as:     \u03c6(f) = \u03b8\u2080 + 2 * arctan(2 * Q_tot * (1 - f / f_r))</p> <p>where:     - \u03c6(f) is the measured phase response (in radians),     - \u03b8\u2080 is the phase offset,     - Q_tot is the total (loaded) quality factor,     - f_r is the resonant frequency.</p> <p>The fitting is performed using a stepwise least-squares optimization to accurately estimate the parameters \u03b8\u2080, Q_tot, and f_r from experimental data.</p> <p>Parameters:</p> Name Type Description Default <code>freq</code> <code>array - like</code> <p>Frequency data (in Hz) at which the phase response was measured.</p> required <code>phase</code> <code>array - like</code> <p>Unwrapped phase response data (in radians) corresponding to <code>freq</code>.</p> required <code>theta0</code> <code>float</code> <p>Initial guess for the phase offset \u03b8\u2080 (in radians).</p> required <code>Q_tot</code> <code>float</code> <p>Initial guess for the total (loaded) quality factor Q_tot.</p> required <code>fr</code> <code>float</code> <p>Initial guess for the resonant frequency f_r (in Hz).</p> required <p>Returns:</p> Type Description <code>FitResult</code> <p>A <code>FitResult</code> object containing: - Fitted parameters (<code>params</code>). - Standard errors (<code>std_err</code>). - Goodness-of-fit metrics (<code>red_chi2</code>). - A callable <code>predict</code> function for generating fitted responses.</p> Notes <ul> <li>The fitting is performed in multiple stages for improved stability:<ol> <li>Optimize \u03b8\u2080 and f_r (fixing Q_tot).</li> <li>Optimize Q_tot and f_r (fixing \u03b8\u2080).</li> <li>Optimize f_r alone.</li> <li>Optimize Q_tot alone.</li> <li>Joint optimization of \u03b8\u2080, Q_tot, and f_r.</li> </ol> </li> <li>This stepwise optimization handles parameter coupling and improves convergence.</li> </ul> Example <p>fitted_params, percent_errors = fit_phase_vs_freq(freq, phase, 0.0, 1000, 5e9) print(f\"Fitted Parameters: \u03b8\u2080 = {fitted_params[0]}, Q_tot = {fitted_params[1]}, f_r = {fitted_params[2]}\") print(f\"Percentage Errors: \u03b8\u2080 = {percent_errors[0]}%, Q_tot = {percent_errors[1]}%, f_r = {percent_errors[2]}%\")</p> Source code in <code>sqil_core/resonator/_resonator.py</code> <pre><code>@fit_output\ndef fit_phase_vs_freq(freq, phase, theta0, Q_tot, fr):\n    \"\"\"\n    Fits the phase response of a superconducting resonator using an arctangent model.\n\n    Reference: https://arxiv.org/abs/1410.3365\n    This function models the phase response as:\n        \u03c6(f) = \u03b8\u2080 + 2 * arctan(2 * Q_tot * (1 - f / f_r))\n\n    where:\n        - \u03c6(f) is the measured phase response (in radians),\n        - \u03b8\u2080 is the phase offset,\n        - Q_tot is the total (loaded) quality factor,\n        - f_r is the resonant frequency.\n\n    The fitting is performed using a stepwise least-squares optimization to accurately\n    estimate the parameters \u03b8\u2080, Q_tot, and f_r from experimental data.\n\n    Parameters\n    ----------\n    freq : array-like\n        Frequency data (in Hz) at which the phase response was measured.\n    phase : array-like\n        Unwrapped phase response data (in radians) corresponding to `freq`.\n    theta0 : float\n        Initial guess for the phase offset \u03b8\u2080 (in radians).\n    Q_tot : float\n        Initial guess for the total (loaded) quality factor Q_tot.\n    fr : float\n        Initial guess for the resonant frequency f_r (in Hz).\n\n    Returns\n    -------\n    FitResult\n        A `FitResult` object containing:\n        - Fitted parameters (`params`).\n        - Standard errors (`std_err`).\n        - Goodness-of-fit metrics (`red_chi2`).\n        - A callable `predict` function for generating fitted responses.\n\n    Notes\n    -----\n    - The fitting is performed in multiple stages for improved stability:\n        1. Optimize \u03b8\u2080 and f_r (fixing Q_tot).\n        2. Optimize Q_tot and f_r (fixing \u03b8\u2080).\n        3. Optimize f_r alone.\n        4. Optimize Q_tot alone.\n        5. Joint optimization of \u03b8\u2080, Q_tot, and f_r.\n    - This stepwise optimization handles parameter coupling and improves convergence.\n\n    Example\n    -------\n    &gt;&gt;&gt; fitted_params, percent_errors = fit_phase_vs_freq(freq, phase, 0.0, 1000, 5e9)\n    &gt;&gt;&gt; print(f\"Fitted Parameters: \u03b8\u2080 = {fitted_params[0]}, Q_tot = {fitted_params[1]}, f_r = {fitted_params[2]}\")\n    &gt;&gt;&gt; print(f\"Percentage Errors: \u03b8\u2080 = {percent_errors[0]}%, Q_tot = {percent_errors[1]}%, f_r = {percent_errors[2]}%\")\n    \"\"\"\n    # Unwrap the phase of the complex data to avoid discontinuities\n    phase = np.unwrap(phase)\n\n    # Define the distance function to handle phase wrapping\n    def dist(x):\n        np.absolute(x, x)\n        c = (x &gt; np.pi).astype(int)\n        return x + c * (-2.0 * x + 2.0 * np.pi)\n\n    # Step 1: Optimize \u03b8\u2080 and fr with Q_tot fixed\n    def residuals_1(p, x, y, Q_tot):\n        theta0, fr = p\n        err = dist(y - (theta0 + 2.0 * np.arctan(2.0 * Q_tot * (1.0 - x / fr))))\n        return err\n\n    p0 = [theta0, fr]\n    p_final = leastsq(\n        lambda a, b, c: residuals_1(a, b, c, Q_tot), p0, args=(freq, phase)\n    )\n    theta0, fr = p_final[0]\n\n    # Step 2: Optimize Q_tot and fr with \u03b8\u2080 fixed\n    def residuals_2(p, x, y, theta0):\n        Q_tot, fr = p\n        err = dist(y - (theta0 + 2.0 * np.arctan(2.0 * Q_tot * (1.0 - x / fr))))\n        return err\n\n    p0 = [Q_tot, fr]\n    p_final = leastsq(\n        lambda a, b, c: residuals_2(a, b, c, theta0), p0, args=(freq, phase)\n    )\n    Q_tot, fr = p_final[0]\n\n    # Step 3: Optimize fr alone\n    def residuals_3(p, x, y, theta0, Q_tot):\n        fr = p\n        err = dist(y - (theta0 + 2.0 * np.arctan(2.0 * Q_tot * (1.0 - x / fr))))\n        return err\n\n    p0 = fr\n    p_final = leastsq(\n        lambda a, b, c: residuals_3(a, b, c, theta0, Q_tot), p0, args=(freq, phase)\n    )\n    fr = float(p_final[0].item())\n\n    # Step 4: Optimize Q_tot alone\n    def residuals_4(p, x, y, theta0, fr):\n        Q_tot = p\n        err = dist(y - (theta0 + 2.0 * np.arctan(2.0 * Q_tot * (1.0 - x / fr))))\n        return err\n\n    p0 = Q_tot\n    p_final = leastsq(\n        lambda a, b, c: residuals_4(a, b, c, theta0, fr), p0, args=(freq, phase)\n    )\n    Q_tot = float(p_final[0].item())\n\n    # Step 5: Joint optimization of \u03b8\u2080, Q_tot, and fr\n    def residuals_5(p, x, y):\n        theta0, Q_tot, fr = p\n        err = dist(y - (theta0 + 2.0 * np.arctan(2.0 * Q_tot * (1.0 - x / fr))))\n        return err\n\n    p0 = [theta0, Q_tot, fr]\n    final_result = leastsq(residuals_5, p0, args=(freq, phase), full_output=True)\n\n    return (\n        final_result,\n        {\n            \"predict\": lambda f: theta0 + 2 * np.arctan(2 * Q_tot * (1 - f / fr)),\n            \"param_names\": [\"\u03b8\u2080\", \"Q_tot\", \"fr\"],\n        },\n    )\n</code></pre>"},{"location":"API%20reference/resonator/resonator/#sqil_core.resonator._resonator.fit_phase_vs_freq_global","title":"<code>fit_phase_vs_freq_global(freq, phase, theta0=None, Q_tot=None, fr=None, disp=False)</code>","text":"<p>Fits phase response data as a function of frequency using an arctangent model.</p> <p>This function models the phase response of a superconducting resonator or circuit as a function of frequency. It fits the data using the model:     \u03b8(f) = \u03b8\u2080 + 2 * arctan(2 * Q_tot * (1 - f / fr)) where \u03b8\u2080 is the phase offset, Q_tot is the total quality factor, and fr is the resonant frequency. The fitting is performed using the Nelder-Mead optimization method to minimize the sum of squared residuals between the measured and modeled phase.</p> <p>Parameters:</p> Name Type Description Default <code>freq</code> <code>ndarray</code> <p>Array of frequency data points (in Hz).</p> required <code>phase</code> <code>ndarray</code> <p>Array of measured phase data (in radians).</p> required <code>theta0</code> <code>float</code> <p>Initial guess for the phase offset \u03b8\u2080. If not provided, defaults to the mean of <code>phase</code>.</p> <code>None</code> <code>Q_tot</code> <code>float</code> <p>Initial guess for the total quality factor. If not provided, defaults to 0.01.</p> <code>None</code> <code>fr</code> <code>float</code> <p>Initial guess for the resonant frequency. If not provided, defaults to the mean of <code>freq</code>.</p> <code>None</code> <code>disp</code> <code>bool</code> <p>If True, displays optimization progress. Default is True.</p> <code>False</code> <p>Returns:</p> Type Description <code>FitResult</code> <p>A <code>FitResult</code> object containing: - Fitted parameters (<code>params</code>). - Standard errors (<code>std_err</code>). - Goodness-of-fit metrics (<code>red_chi2</code>). - A callable <code>predict</code> function for generating fitted responses.</p> Notes <ul> <li>The model assumes the phase response follows the arctangent behavior typical in   superconducting resonators near resonance.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; freq = np.linspace(5e9, 6e9, 1000)  # Frequency in Hz\n&gt;&gt;&gt; phase = np.random.normal(0, 0.1, size=freq.size)  # Simulated noisy phase data\n&gt;&gt;&gt; popt, perr = fit_phase_vs_freq(freq, phase)\n&gt;&gt;&gt; print(\"Fitted Parameters (\u03b8\u2080, Q_tot, fr):\", popt)\n&gt;&gt;&gt; print(\"Percentage Errors:\", perr)\n</code></pre> Source code in <code>sqil_core/resonator/_resonator.py</code> <pre><code>@fit_output\ndef fit_phase_vs_freq_global(\n    freq: np.ndarray,\n    phase: np.ndarray,\n    theta0: float | None = None,\n    Q_tot: float | None = None,\n    fr: float | None = None,\n    disp: bool = False,\n) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Fits phase response data as a function of frequency using an arctangent model.\n\n    This function models the phase response of a superconducting resonator or circuit\n    as a function of frequency. It fits the data using the model:\n        \u03b8(f) = \u03b8\u2080 + 2 * arctan(2 * Q_tot * (1 - f / fr))\n    where \u03b8\u2080 is the phase offset, Q_tot is the total quality factor, and fr is the\n    resonant frequency. The fitting is performed using the Nelder-Mead optimization\n    method to minimize the sum of squared residuals between the measured and modeled phase.\n\n    Parameters\n    ----------\n    freq : np.ndarray\n        Array of frequency data points (in Hz).\n    phase : np.ndarray\n        Array of measured phase data (in radians).\n    theta0 : float, optional\n        Initial guess for the phase offset \u03b8\u2080. If not provided, defaults to the mean of `phase`.\n    Q_tot : float, optional\n        Initial guess for the total quality factor. If not provided, defaults to 0.01.\n    fr : float, optional\n        Initial guess for the resonant frequency. If not provided, defaults to the mean of `freq`.\n    disp : bool, optional\n        If True, displays optimization progress. Default is True.\n\n    Returns\n    -------\n    FitResult\n        A `FitResult` object containing:\n        - Fitted parameters (`params`).\n        - Standard errors (`std_err`).\n        - Goodness-of-fit metrics (`red_chi2`).\n        - A callable `predict` function for generating fitted responses.\n\n    Notes\n    -----\n    - The model assumes the phase response follows the arctangent behavior typical in\n      superconducting resonators near resonance.\n\n    Examples\n    --------\n    &gt;&gt;&gt; freq = np.linspace(5e9, 6e9, 1000)  # Frequency in Hz\n    &gt;&gt;&gt; phase = np.random.normal(0, 0.1, size=freq.size)  # Simulated noisy phase data\n    &gt;&gt;&gt; popt, perr = fit_phase_vs_freq(freq, phase)\n    &gt;&gt;&gt; print(\"Fitted Parameters (\u03b8\u2080, Q_tot, fr):\", popt)\n    &gt;&gt;&gt; print(\"Percentage Errors:\", perr)\n    \"\"\"\n    if theta0 is None:\n        theta0 = np.mean(phase)\n    if Q_tot is None:\n        Q_tot = 0.01\n    if fr is None:\n        fr = np.mean(freq)  # freq[np.argmin(np.abs(phase - np.mean(phase)))]\n\n    def objective(x):\n        theta0, Q_tot, fr = x\n        model = theta0 + 2 * np.arctan(2 * Q_tot * (1 - freq / fr))\n        residuals = phase - model\n        return np.square(residuals).sum()\n\n    res = minimize(\n        fun=objective,\n        x0=[theta0, Q_tot, fr],\n        method=\"Nelder-Mead\",\n        options={\"maxiter\": 3000000, \"disp\": disp},\n    )\n\n    return res, {\n        \"predict\": lambda f: theta0 + 2 * np.arctan(2 * Q_tot * (1 - f / fr)),\n        \"param_names\": [\"\u03b8\u2080\", \"Q_tot\", \"fr\"],\n    }\n</code></pre>"},{"location":"API%20reference/resonator/resonator/#sqil_core.resonator._resonator.full_fit","title":"<code>full_fit(freq, data, measurement, a, alpha, tau, Q_tot, fr, Q_ext=1, phi0=0, mag_bg=None)</code>","text":"<p>Performs a full fit of the measured resonator data using a selected model (either reflection or hanger-type measurement). The fitting is handled using the lmfit Model framework.</p> <p>IMPORTANT: This fitting function should only be used to refine already good initial guesses!</p> <p>Parameters:</p> Name Type Description Default <code>freq</code> <code>ndarray</code> <p>A 1D array of frequency values in Hz.</p> required <code>data</code> <code>ndarray</code> <p>A 1D array of complex-valued measured resonator data.</p> required <code>measurement</code> <code>str</code> <p>Type of measurement. Should be either: - <code>\"reflection\"</code>: Uses the <code>S11_reflection</code> model. - <code>\"hanger\"</code>: Uses the <code>S21_hanger</code> model.</p> required <code>a</code> <code>float</code> <p>Amplitude scaling factor.</p> required <code>alpha</code> <code>float</code> <p>Phase offset parameter.</p> required <code>tau</code> <code>float</code> <p>Cable delay or propagation time.</p> required <code>Q_tot</code> <code>float</code> <p>Total quality factor of the resonator.</p> required <code>fr</code> <code>float</code> <p>Resonant frequency.</p> required <code>Q_ext</code> <code>float</code> <p>External quality factor (coupling quality factor). Only for reflection and hanger.</p> <code>1</code> <code>phi0</code> <code>float</code> <p>Phase offset at resonance. Only for relfection and hanger.</p> <code>0</code> <code>mag_bg</code> <code>ndarray</code> <p>A 1D array representing the magnitude background response, if available. Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>FitResult</code> <p>A <code>FitResult</code> object containing: - Fitted parameters (<code>params</code>). - Standard errors (<code>std_err</code>). - Goodness-of-fit metrics (<code>red_chi2</code>). - A callable <code>predict</code> function for generating fitted responses.</p> Example <p>freq = np.linspace(1e9, 2e9, 1000)  # Example frequency range data = np.exp(1j * freq * 2 * np.pi / 1e9)  # Example complex response fit_result = full_fit(freq, data, \"reflection\", 1, 0, 0, 1e4, 2e4, 1.5e9, 0) fit_result.summary()</p> Source code in <code>sqil_core/resonator/_resonator.py</code> <pre><code>@fit_output\ndef full_fit(\n    freq, data, measurement, a, alpha, tau, Q_tot, fr, Q_ext=1, phi0=0, mag_bg=None\n) -&gt; FitResult:\n    \"\"\"\n    Performs a full fit of the measured resonator data using a selected model\n    (either reflection or hanger-type measurement). The fitting is handled\n    using the lmfit Model framework.\n\n    IMPORTANT: This fitting function should only be used to refine already\n    good initial guesses!\n\n    Parameters\n    ----------\n    freq : np.ndarray\n        A 1D array of frequency values in Hz.\n\n    data : np.ndarray\n        A 1D array of complex-valued measured resonator data.\n\n    measurement : str\n        Type of measurement. Should be either:\n        - `\"reflection\"`: Uses the `S11_reflection` model.\n        - `\"hanger\"`: Uses the `S21_hanger` model.\n\n    a : float\n        Amplitude scaling factor.\n\n    alpha : float\n        Phase offset parameter.\n\n    tau : float\n        Cable delay or propagation time.\n\n    Q_tot : float\n        Total quality factor of the resonator.\n\n    fr : float\n        Resonant frequency.\n\n    Q_ext : float\n        External quality factor (coupling quality factor).\n        Only for reflection and hanger.\n\n    phi0 : float\n        Phase offset at resonance. Only for relfection and hanger.\n\n    mag_bg : np.ndarray, optional\n        A 1D array representing the magnitude background response, if available.\n        Default is None.\n\n    Returns\n    -------\n    FitResult\n        A `FitResult` object containing:\n        - Fitted parameters (`params`).\n        - Standard errors (`std_err`).\n        - Goodness-of-fit metrics (`red_chi2`).\n        - A callable `predict` function for generating fitted responses.\n\n    Example\n    -------\n    &gt;&gt;&gt; freq = np.linspace(1e9, 2e9, 1000)  # Example frequency range\n    &gt;&gt;&gt; data = np.exp(1j * freq * 2 * np.pi / 1e9)  # Example complex response\n    &gt;&gt;&gt; fit_result = full_fit(freq, data, \"reflection\", 1, 0, 0, 1e4, 2e4, 1.5e9, 0)\n    &gt;&gt;&gt; fit_result.summary()\n    \"\"\"\n    model_name = None\n\n    if measurement == \"reflection\":\n\n        def S11_reflection_fixed(freq, a, alpha, tau, Q_tot, fr, Q_ext_mag, phi):\n            return S11_reflection(\n                freq, a, alpha, tau, Q_tot, fr, Q_ext_mag, phi, mag_bg\n            )\n\n        model = Model(S11_reflection_fixed)\n        params = model.make_params(\n            a=a, alpha=alpha, tau=tau, Q_tot=Q_tot, fr=fr, Q_ext_mag=Q_ext, phi=phi0\n        )\n        model_name = \"S11_reflection\"\n\n    elif measurement == \"hanger\":\n\n        def S21_hanger_fixed(freq, a, alpha, tau, Q_tot, fr, Q_ext_mag, phi):\n            return S21_hanger(freq, a, alpha, tau, Q_tot, fr, Q_ext_mag, phi, mag_bg)\n\n        model = Model(S21_hanger_fixed)\n        params = model.make_params(\n            a=a, alpha=alpha, tau=tau, Q_tot=Q_tot, fr=fr, Q_ext_mag=Q_ext, phi=phi0\n        )\n        model_name = \"S21_hanger\"\n\n    elif measurement == \"transmission\":\n\n        def S21_transmission_fixed(freq, a, alpha, tau, Q_tot, fr):\n            return S21_transmission(freq, a, alpha, tau, Q_tot, fr, mag_bg)\n\n        model = Model(S21_transmission_fixed)\n        params = model.make_params(a=a, alpha=alpha, tau=tau, Q_tot=Q_tot, fr=fr)\n        model_name = \"S21_transmission\"\n\n    res = model.fit(data, params, freq=freq)\n    return res, {\"model_name\": model_name}\n</code></pre>"},{"location":"API%20reference/resonator/resonator/#sqil_core.resonator._resonator.linmag_fit","title":"<code>linmag_fit(freq, data)</code>","text":"<p>Fits the magnitude squared of complex data to a Lorentzian profile.</p> <p>This function computes the normalized magnitude of the input complex data and fits its squared value to a Lorentzian function to characterize resonance features. If the initial Lorentzian fit quality is poor (based on NRMSE), it attempts a fit using a skewed Lorentzian model and returns the better fit.</p> <p>Parameters:</p> Name Type Description Default <code>freq</code> <code>ndarray</code> <p>Frequency values corresponding to the data points.</p> required <code>data</code> <code>ndarray</code> <p>Complex-valued data to be fitted.</p> required <p>Returns:</p> Type Description <code>FitResult</code> <p>The best fit result from either the Lorentzian or skewed Lorentzian fit, selected based on fit quality.</p> Source code in <code>sqil_core/resonator/_resonator.py</code> <pre><code>def linmag_fit(freq: np.ndarray, data: np.ndarray) -&gt; FitResult:\n    \"\"\"\n    Fits the magnitude squared of complex data to a Lorentzian profile.\n\n    This function computes the normalized magnitude of the input complex data and fits\n    its squared value to a Lorentzian function to characterize resonance features.\n    If the initial Lorentzian fit quality is poor (based on NRMSE), it attempts a fit\n    using a skewed Lorentzian model and returns the better fit.\n\n    Parameters\n    ----------\n    freq : np.ndarray\n        Frequency values corresponding to the data points.\n    data : np.ndarray\n        Complex-valued data to be fitted.\n\n    Returns\n    -------\n    FitResult\n        The best fit result from either the Lorentzian or skewed Lorentzian fit, selected\n        based on fit quality.\n    \"\"\"\n\n    linmag = np.abs(data)\n    norm_linmag = linmag / np.max(linmag)\n    # Lorentzian fit\n    fit_res = fit_lorentzian(freq, norm_linmag**2)\n    # If the lorentzian fit is bad, try a skewed lorentzian\n    if not fit_res.is_acceptable(\"nrmse\"):\n        fit_res_skewed = fit_skewed_lorentzian(freq, norm_linmag**2)\n        fit_res = get_best_fit(fit_res, fit_res_skewed)\n\n    return fit_res\n</code></pre>"},{"location":"API%20reference/resonator/resonator/#sqil_core.resonator._resonator.plot_resonator","title":"<code>plot_resonator(freq, data, x_fit=None, y_fit=None, mag_bg=None, title='')</code>","text":"<p>Plots the resonator response in three different representations: - Complex plane (Re vs. Im) - Magnitude response (Amplitude vs. Frequency) - Phase response (Phase vs. Frequency)</p> <p>Parameters:</p> Name Type Description Default <code>freq</code> <code>ndarray</code> <p>A 1D array representing the frequency values.</p> required <code>data</code> <code>ndarray</code> <p>A 1D array of complex-valued data points corresponding to the resonator response.</p> required <code>fit</code> <code>ndarray</code> <p>A 1D array of complex values representing the fitted model response. Default is None.</p> required <code>mag_bg</code> <code>ndarray</code> <p>A 1D array representing the background magnitude response, if available. Default is None.</p> <code>None</code> <code>title</code> <code>str</code> <p>The title of the plot. Default is an empty string.</p> <code>''</code> Source code in <code>sqil_core/resonator/_resonator.py</code> <pre><code>def plot_resonator(\n    freq, data, x_fit=None, y_fit=None, mag_bg: np.ndarray | None = None, title=\"\"\n):\n    \"\"\"\n    Plots the resonator response in three different representations:\n    - Complex plane (Re vs. Im)\n    - Magnitude response (Amplitude vs. Frequency)\n    - Phase response (Phase vs. Frequency)\n\n    Parameters\n    ----------\n    freq : np.ndarray\n        A 1D array representing the frequency values.\n\n    data : np.ndarray\n        A 1D array of complex-valued data points corresponding to the resonator response.\n\n    fit : np.ndarray, optional\n        A 1D array of complex values representing the fitted model response. Default is None.\n\n    mag_bg : np.ndarray, optional\n        A 1D array representing the background magnitude response, if available. Default is None.\n\n    title : str, optional\n        The title of the plot. Default is an empty string.\n    \"\"\"\n\n    set_plot_style(plt)\n\n    fig = plt.figure(figsize=(24, 10))\n    gs = fig.add_gridspec(2, 2)\n    ms = plt.rcParams.get(\"lines.markersize\")\n\n    # Subplot on the left (full height, first column)\n    ax1 = fig.add_subplot(gs[:, 0])  # Left side spans both rows\n    ax1.plot(np.real(data), np.imag(data), \"o\", color=\"tab:blue\", ms=ms + 1)\n    if y_fit is not None:\n        ax1.plot(np.real(y_fit), np.imag(y_fit), color=\"tab:orange\")\n    ax1.set_aspect(\"equal\")\n    ax1.set_xlabel(\"In-phase\")\n    ax1.set_ylabel(\"Quadrature\")\n    ax1.grid(True)\n\n    # Subplot on the top-right (first row, second column)\n    ax2 = fig.add_subplot(gs[0, 1])\n    ax2.plot(freq, np.abs(data), \"o\", color=\"tab:blue\", ms=ms - 1)\n    if y_fit is not None:\n        ax2.plot(x_fit, np.abs(y_fit), color=\"tab:orange\")\n    if (mag_bg is not None) and (not np.isnan(mag_bg).any()):\n        ax2.plot(freq, mag_bg, \"-.\", color=\"tab:green\")\n    ax2.set_ylabel(\"Magnitude [V]\")\n    ax2.grid(True)\n\n    # Subplot on the bottom-right (second row, second column)\n    ax3 = fig.add_subplot(gs[1, 1])\n    ax3.plot(freq, np.unwrap(np.angle(data)), \"o\", color=\"tab:blue\", ms=ms - 1)\n    if y_fit is not None:\n        ax3.plot(x_fit, np.unwrap(np.angle(y_fit)), color=\"tab:orange\")\n    ax3.set_ylabel(\"Phase [rad]\")\n    ax3.set_xlabel(\"Frequency [Hz]\")\n    ax3.grid(True)\n\n    fig.suptitle(title)\n    fig.tight_layout()\n\n    return fig, (ax1, ax2, ax3)\n</code></pre>"},{"location":"API%20reference/resonator/resonator/#sqil_core.resonator._resonator.quick_fit","title":"<code>quick_fit(freq, data, measurement, tau=None, Q_tot=None, fr=None, mag_bg=None, fit_range=None, bias_toward_fr=False, verbose=False, do_plot=False)</code>","text":"<p>Extracts resonator parameters from complex S-parameter data using circle fitting for reflection or hanger measurements.</p> <p>This function analyzes complex-valued resonator data by fitting a circle in the complex plane and refining key resonator parameters. It estimates or refines the total quality factor (Q_tot), resonance frequency (fr). For reflection and hanger it also estimates the external quality factor (Q_ext), while correcting for impedance mismatch.</p> <p>Parameters:</p> Name Type Description Default <code>freq</code> <code>ndarray</code> <p>Frequency data.</p> required <code>data</code> <code>ndarray</code> <p>Complex-valued S-parameter data (e.g., S11 for reflection or S21 for hanger configuration).</p> required <code>measurement</code> <code>(reflection, hanger)</code> <p>Type of measurement setup. Use 'reflection' for S11 or 'hanger' for S21.</p> <code>'reflection'</code> <code>tau</code> <code>float</code> <p>Initial guess for the cable delay IN RADIANS. If you are passing a value obtained from a linear fit divide it by 2pi. If None, it is estimated from a linear fit.</p> <code>None</code> <code>Q_tot</code> <code>float</code> <p>Initial guess for the total quality factor. If None, it is estimated from a skewed Lorentzian fit.</p> <code>None</code> <code>fr</code> <code>float</code> <p>Initial guess for the resonance frequency. If None, it is estimated from a skewed Lorentzian fit.</p> <code>None</code> <code>mag_bg</code> <code>ndarray</code> <p>Magnitude background correction data. Defaults to NaN if not provided.</p> <code>None</code> <code>fit_range</code> <code>float | None</code> <p>The number x that defines the range [fr-x, fr+x] in which the fitting should be performed. The estimation of cable delay and amplitude background will be perfomed on the full data. Defaults to <code>3 * fr / Q_tot</code>.</p> <code>None</code> <code>bias_toward_fr</code> <code>bool</code> <p>If true performs circle fits using the 50% of points closest to the resonance. Defaults is False.</p> <code>False</code> <code>verbose</code> <code>bool</code> <p>If True, detailed fitting results and progress are printed. Default is False.</p> <code>False</code> <code>do_plot</code> <code>bool</code> <p>If True, plots the fitted circle and off-resonant point for visualization. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing: - a (float): Amplitude scaling factor from the off-resonant point. - alpha (float): Phase offset from the off-resonant point (in radians). - tau (float): Estimated cable delay (in radians). - Q_tot (float): Total quality factor. - fr (float): Resonance frequency. - Q_ext (complex): External quality factor, accounting for impedance mismatch. - phi0 (float): Phase shift due to impedance mismatch (in radians).</p> Notes <ul> <li>If <code>tau</code> is not provided it is estimated by fitting a line through the last 5% of phase points.</li> <li>If <code>Q_tot</code> or <code>fr</code> is not provided, they are estimated by fitting a skewed Lorentzian model.</li> <li>Visualization helps assess the quality of intermediate steps.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; data = np.random.rand(100) + 1j * np.random.rand(100)\n&gt;&gt;&gt; a, alpha, Q_tot, Q_ext, fr, phi0, theta0 = quick_fit(data, measurement='reflection', verbose=True, do_plot=True)\n&gt;&gt;&gt; print(f\"Resonance Frequency: {fr} Hz, Q_tot: {Q_tot}, Q_ext: {Q_ext}\")\n</code></pre> Source code in <code>sqil_core/resonator/_resonator.py</code> <pre><code>def quick_fit(\n    freq: np.ndarray,\n    data: np.ndarray,\n    measurement: Literal[\"reflection\", \"hanger\", \"transmission\"],\n    tau: float | None = None,\n    Q_tot: float | None = None,\n    fr: float | None = None,\n    mag_bg: np.ndarray | None = None,\n    fit_range: float | None = None,\n    bias_toward_fr: bool = False,\n    verbose: bool = False,\n    do_plot: bool = False,\n) -&gt; tuple[float, float, float, complex, float, float, float]:\n    \"\"\"\n    Extracts resonator parameters from complex S-parameter data using circle fitting for reflection or hanger measurements.\n\n    This function analyzes complex-valued resonator data by fitting a circle in the complex plane and\n    refining key resonator parameters. It estimates or refines the total quality factor (Q_tot),\n    resonance frequency (fr). For reflection and hanger it also estimates the external quality\n    factor (Q_ext), while correcting for impedance mismatch.\n\n    Parameters\n    ----------\n    freq : np.ndarray\n        Frequency data.\n    data : np.ndarray\n        Complex-valued S-parameter data (e.g., S11 for reflection or S21 for hanger configuration).\n    measurement : {'reflection', 'hanger'}\n        Type of measurement setup. Use 'reflection' for S11 or 'hanger' for S21.\n    tau : float, optional\n        Initial guess for the cable delay IN RADIANS. If you are passing a value obtained from a linear fit\n        divide it by 2pi. If None, it is estimated from a linear fit.\n    Q_tot : float, optional\n        Initial guess for the total quality factor. If None, it is estimated from a skewed Lorentzian fit.\n    fr : float, optional\n        Initial guess for the resonance frequency. If None, it is estimated from a skewed Lorentzian fit.\n    mag_bg : np.ndarray, optional\n        Magnitude background correction data. Defaults to NaN if not provided.\n    fit_range: float, optional\n        The number x that defines the range [fr-x, fr+x] in which the fitting should be performed. The estimation of\n        cable delay and amplitude background will be perfomed on the full data. Defaults to `3 * fr / Q_tot`.\n    bias_toward_fr : bool, optional\n        If true performs circle fits using the 50% of points closest to the resonance. Defaults is False.\n    verbose : bool, optional\n        If True, detailed fitting results and progress are printed. Default is False.\n    do_plot : bool, optional\n        If True, plots the fitted circle and off-resonant point for visualization. Default is False.\n\n    Returns\n    -------\n    tuple\n        A tuple containing:\n        - a (float): Amplitude scaling factor from the off-resonant point.\n        - alpha (float): Phase offset from the off-resonant point (in radians).\n        - tau (float): Estimated cable delay (in radians).\n        - Q_tot (float): Total quality factor.\n        - fr (float): Resonance frequency.\n        - Q_ext (complex): External quality factor, accounting for impedance mismatch.\n        - phi0 (float): Phase shift due to impedance mismatch (in radians).\n\n    Notes\n    -----\n    - If `tau` is not provided it is estimated by fitting a line through the last 5% of phase points.\n    - If `Q_tot` or `fr` is not provided, they are estimated by fitting a skewed Lorentzian model.\n    - Visualization helps assess the quality of intermediate steps.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; data = np.random.rand(100) + 1j * np.random.rand(100)\n    &gt;&gt;&gt; a, alpha, Q_tot, Q_ext, fr, phi0, theta0 = quick_fit(data, measurement='reflection', verbose=True, do_plot=True)\n    &gt;&gt;&gt; print(f\"Resonance Frequency: {fr} Hz, Q_tot: {Q_tot}, Q_ext: {Q_ext}\")\n    \"\"\"\n    # Sanitize inputs\n    if (\n        measurement != \"reflection\"\n        and measurement != \"hanger\"\n        and measurement != \"transmission\"\n    ):\n        raise Exception(\n            f\"Invalid measurement type {measurement}. Must be either 'reflection', 'hanger' or 'transmission'\"\n        )\n    if mag_bg is None:\n        mag_bg = np.nan\n\n    # Define amplitude and phase\n    linmag = np.abs(data)\n    phase = np.unwrap(np.angle(data))\n\n    # Inital estimate for Q_tot and fr by fitting a lorentzian on\n    # the squared manitude data\n    if (Q_tot is None) or (fr is None):\n        if verbose:\n            print(\"* Lorentzian estimation of fr and Q_tot\")\n        norm_linmag = linmag / np.max(linmag)\n        fit_res = fit_lorentzian(freq, norm_linmag**2)\n        _, efr, fwhm, _ = fit_res.params\n        eQ_tot = efr / fwhm\n        # If the lorentzian fit is bad, try a skewed lorentzian\n        if fit_res.metrics[\"nrmse\"] &gt; 0.5:\n            fit_res_skewed = fit_skewed_lorentzian(freq, norm_linmag**2)\n            (A1, A2, A3, A4, efr, eQ_tot) = fit_res.params\n            delta_aic = fit_res[\"aic\"] - fit_res_skewed[\"aic\"]\n            fit_res = fit_res_skewed if delta_aic &gt;= 0 else fit_res\n\n        # Assign only parameters for which no initial guess was provided\n        Q_tot = Q_tot or eQ_tot\n        fr = fr or efr\n        if verbose:\n            print(fit_res.model_name)\n            fit_res.summary()\n            if fr != efr:\n                print(f\" -&gt; Still considering fr = {fr}\\n\")\n            elif Q_tot != eQ_tot:\n                print(f\" -&gt; Still considering Q_tot = {Q_tot}\\n\")\n\n    # Initial estimate for tau by fitting a line through the last 5% of phase points\n    if tau is None:\n        if verbose:\n            print(\"* Linear estimation of cable delay from the last 5% of phase points\")\n        [_, tau] = estimate_linear_background(\n            freq, phase, points_cut=0.05, cut_from_back=True\n        )\n        tau /= 2 * np.pi\n        if verbose:\n            print(f\" -&gt; tau [rad]: {tau}\\n\")\n\n    # Remove cable delay\n    phase1 = phase - 2 * np.pi * tau * (freq - freq[0])\n    data1 = linmag * np.exp(1j * phase1)\n\n    # Cut data around the estimated resonance frequency\n    if fit_range is None:\n        fit_range = 3 * fr / Q_tot\n    mask = (freq &gt; fr - fit_range) &amp; (freq &lt; fr + fit_range)\n    freq, linmag, phase, data = freq[mask], linmag[mask], phase[mask], data[mask]\n    phase1, data1 = phase1[mask], data1[mask]\n    if not np.isscalar(mag_bg):\n        mag_bg = mag_bg[mask]\n\n    # Move cirle to center\n    if bias_toward_fr:\n        fr_idx = np.abs(freq - fr).argmin()\n        idx_range = int(len(freq) / 4)\n        re, im = np.real(data1), np.imag(data1)\n        fit_res = fit_circle_algebraic(\n            re[fr_idx - idx_range : fr_idx + idx_range],\n            im[fr_idx - idx_range : fr_idx + idx_range],\n        )\n    else:\n        fit_res = fit_circle_algebraic(np.real(data1), np.imag(data1))\n    (xc, yc, r0) = fit_res.params\n    data3 = data1 - xc - 1j * yc\n    phase3 = np.unwrap(np.angle(data3))\n\n    # Fit phase vs frequency\n    if verbose:\n        print(\"* Phase vs frequency fit\")\n    fit_res = fit_phase_vs_freq(freq, phase3, theta0=0, Q_tot=Q_tot, fr=fr)\n    (theta0, Q_tot, fr) = fit_res.params\n    if verbose:\n        fit_res.summary()\n\n    # Find the off-resonant point\n    p_offres = (xc + 1j * yc) + r0 * np.exp(1j * (theta0 + np.pi))\n    a = np.abs(p_offres)\n    alpha = np.angle(p_offres)\n    # Rescale data\n    linmag5 = linmag / a\n    phase5 = phase1 - alpha\n    data5 = linmag5 * np.exp(1j * phase5)\n\n    Q_ext = None\n    phi0 = None\n    if measurement == \"reflection\" or measurement == \"hanger\":\n        # Find impedence mismatch\n        if bias_toward_fr:\n            fr_idx = np.abs(freq - fr).argmin()\n            re, im = np.real(data5), np.imag(data5)\n            fit_res = fit_circle_algebraic(\n                re[fr_idx - idx_range : fr_idx + idx_range],\n                im[fr_idx - idx_range : fr_idx + idx_range],\n            )\n        else:\n            fit_res = fit_circle_algebraic(np.real(data5), np.imag(data5))\n        (xc6, yc6, r06) = fit_res.params\n        phi0 = -np.arcsin(yc6 / r06)\n\n        # Q_ext and Q_int\n        if measurement == \"reflection\":\n            Q_ext = Q_tot / (r06 * np.exp(-1j * phi0))\n        elif measurement == \"hanger\":\n            Q_ext = Q_tot / (2 * r06 * np.exp(-1j * phi0))\n\n    # Refine phase offset and amplitude scaling\n    if measurement == \"reflection\":\n        res6 = S11_reflection(freq, a, alpha, tau, Q_tot, Q_ext, fr, phi0, mag_bg / a)\n    elif measurement == \"hanger\":\n        res6 = S21_hanger(freq, a, alpha, tau, Q_tot, Q_ext, fr, phi0, mag_bg / a)\n    elif measurement == \"transmission\":\n        res6 = S21_transmission(freq, a, alpha, tau, Q_tot, fr)\n        a *= (np.max(linmag) - np.min(linmag)) / (\n            np.max(np.abs(res6)) - np.min(np.abs(res6))\n        )\n        alpha += phase[0] - np.unwrap(np.angle(res6))[0]\n\n    # Plot small summary\n    if do_plot:\n        v = np.linspace(0, 2 * np.pi, 100)\n        fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n        ax.plot(\n            np.real(data1),\n            np.imag(data1),\n            \"o\",\n            label=\"Data cut (without cable delay)\",\n            zorder=1,\n        )\n        ax.plot(xc + r0 * np.cos(v), yc + r0 * np.sin(v), label=\"Circle fit\", zorder=2)\n        ax.scatter(\n            np.real(p_offres),\n            np.imag(p_offres),\n            color=\"tab:cyan\",\n            label=\"Off-resonant point\",\n            zorder=3,\n            s=120,\n            marker=\"*\",\n        )\n        ax.scatter(xc, yc, color=\"tab:orange\")\n        ax.set_xlabel(\"Re\")\n        ax.set_ylabel(\"Im\")\n        ax.axis(\"equal\")\n        ax.margins(x=0.25, y=0.25)\n        ax.grid(True)\n        ax.set_title(\"Data fit and off-resonant point\")\n        ax.legend()\n        # Show\n        fig.tight_layout()\n        plt.show()\n\n    return a, alpha, tau, Q_tot, fr, Q_ext, phi0\n</code></pre>"},{"location":"API%20reference/utils/analysis/","title":"Analysis","text":""},{"location":"API%20reference/utils/analysis/#sqil_core.utils._analysis.compute_fft","title":"<code>compute_fft(x_data, y_data)</code>","text":"<p>Computes the Fast Fourier Transform (FFT) of a signal and returns the positive frequency spectrum.</p> <p>Parameters:</p> Name Type Description Default <code>x_data</code> <code>ndarray</code> <p>Time or independent variable array, assumed to be uniformly spaced.</p> required <code>y_data</code> <code>ndarray</code> <p>Signal data corresponding to <code>x_data</code>. Can be real or complex.</p> required <p>Returns:</p> Name Type Description <code>positive_freqs</code> <code>ndarray</code> <p>Array of positive frequency components corresponding to the FFT.</p> <code>fft_magnitude</code> <code>ndarray</code> <p>Magnitude of the FFT at the positive frequencies.</p> Notes <ul> <li>The signal is centered by subtracting its mean before computing the FFT, which removes the DC component.</li> <li>Only the positive frequency half of the FFT spectrum is returned, assuming a real-valued input signal.</li> <li>If <code>y_data</code> is complex, returned FFT values still reflect magnitude only.</li> <li>The input <code>x_data</code> must be uniformly spaced for the frequency axis to be accurate.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; t = np.linspace(0, 1, 1000)\n&gt;&gt;&gt; y = np.sin(2 * np.pi * 50 * t)\n&gt;&gt;&gt; freqs, spectrum = compute_fft(t, y)\n</code></pre> Source code in <code>sqil_core/utils/_analysis.py</code> <pre><code>def compute_fft(x_data, y_data):\n    \"\"\"\n    Computes the Fast Fourier Transform (FFT) of a signal and returns the positive frequency spectrum.\n\n    Parameters\n    ----------\n    x_data : np.ndarray\n        Time or independent variable array, assumed to be uniformly spaced.\n    y_data : np.ndarray\n        Signal data corresponding to `x_data`. Can be real or complex.\n\n    Returns\n    -------\n    positive_freqs : np.ndarray\n        Array of positive frequency components corresponding to the FFT.\n    fft_magnitude : np.ndarray\n        Magnitude of the FFT at the positive frequencies.\n\n    Notes\n    -----\n    - The signal is centered by subtracting its mean before computing the FFT, which removes the DC component.\n    - Only the positive frequency half of the FFT spectrum is returned, assuming a real-valued input signal.\n    - If `y_data` is complex, returned FFT values still reflect magnitude only.\n    - The input `x_data` must be uniformly spaced for the frequency axis to be accurate.\n\n    Examples\n    --------\n    &gt;&gt;&gt; t = np.linspace(0, 1, 1000)\n    &gt;&gt;&gt; y = np.sin(2 * np.pi * 50 * t)\n    &gt;&gt;&gt; freqs, spectrum = compute_fft(t, y)\n    \"\"\"\n\n    # Subtract DC offset to focus on oscillations\n    y_data_centered = y_data - np.mean(y_data)\n\n    # Calculate time step (assumes uniform spacing)\n    dt = x_data[1] - x_data[0]\n    N = len(x_data)\n\n    # Compute FFT and frequency axis\n    fft_vals = np.fft.fft(y_data_centered)\n    freqs = np.fft.fftfreq(N, dt)\n\n    # Take only positive frequencies\n    positive_freqs = freqs[: N // 2]\n    fft_magnitude = np.abs(fft_vals[: N // 2])\n\n    return positive_freqs, fft_magnitude\n</code></pre>"},{"location":"API%20reference/utils/analysis/#sqil_core.utils._analysis.compute_snr_peaked","title":"<code>compute_snr_peaked(x_data, y_data, x0, fwhm, noise_region_factor=2.5, min_points=20)</code>","text":"<p>Computes the Signal-to-Noise Ratio (SNR) for a peaked function (e.g., Lorentzian, Gaussian) based on the provided fit parameters. The SNR is calculated by comparing the signal strength at the peak (x0) with the noise level estimated from a region outside the peak.</p> <p>Parameters:</p> Name Type Description Default <code>x_data</code> <code>ndarray</code> <p>Array of x values (independent variable), typically representing frequency or position.</p> required <code>y_data</code> <code>ndarray</code> <p>Array of y values (dependent variable), representing the measured values (e.g., intensity, amplitude).</p> required <code>x0</code> <code>float</code> <p>The location of the peak (center of the distribution), often the resonance frequency or peak position.</p> required <code>fwhm</code> <code>float</code> <p>The Full Width at Half Maximum (FWHM) of the peak. This defines the width of the peak and helps determine the region for noise estimation.</p> required <code>noise_region_factor</code> <code>float</code> <p>The factor used to define the width of the noise region as a multiple of the FWHM. The noise region is considered outside the interval <code>(x0 - noise_region_factor * fwhm, x0 + noise_region_factor * fwhm)</code>.</p> <code>2.5</code> <code>min_points</code> <code>int</code> <p>The minimum number of data points required in the noise region to estimate the noise level. If the number of points in the noise region is smaller than this threshold, a warning is issued.</p> <code>20</code> <p>Returns:</p> Type Description <code>float</code> <p>The computed Signal-to-Noise Ratio (SNR), which is the ratio of the signal strength at <code>x0</code> to the standard deviation of the noise. If the noise standard deviation is zero, the SNR is set to infinity.</p> Notes <ul> <li>The function assumes that the signal has a clear peak at <code>x0</code> and that the surrounding data represents noise.</li> <li>If the noise region contains fewer than <code>min_points</code> data points, a warning is raised suggesting the adjustment of <code>noise_region_factor</code>.</li> </ul> Example <p>x_data = np.linspace(-10, 10, 1000) y_data = np.exp(-(x_data**2))  # Example Gaussian x0 = 0 fwhm = 2.0 snr = compute_snr_peaked(x_data, y_data, x0, fwhm) print(snr)</p> Source code in <code>sqil_core/utils/_analysis.py</code> <pre><code>def compute_snr_peaked(\n    x_data: np.ndarray,\n    y_data: np.ndarray,\n    x0: float,\n    fwhm: float,\n    noise_region_factor: float = 2.5,\n    min_points: int = 20,\n) -&gt; float:\n    \"\"\"\n    Computes the Signal-to-Noise Ratio (SNR) for a peaked function (e.g., Lorentzian, Gaussian)\n    based on the provided fit parameters. The SNR is calculated by comparing the signal strength\n    at the peak (x0) with the noise level estimated from a region outside the peak.\n\n    Parameters\n    ----------\n    x_data : np.ndarray\n        Array of x values (independent variable), typically representing frequency or position.\n\n    y_data : np.ndarray\n        Array of y values (dependent variable), representing the measured values (e.g., intensity, amplitude).\n\n    x0 : float\n        The location of the peak (center of the distribution), often the resonance frequency or peak position.\n\n    fwhm : float\n        The Full Width at Half Maximum (FWHM) of the peak. This defines the width of the peak and helps determine\n        the region for noise estimation.\n\n    noise_region_factor : float, optional, default=2.5\n        The factor used to define the width of the noise region as a multiple of the FWHM. The noise region is\n        considered outside the interval `(x0 - noise_region_factor * fwhm, x0 + noise_region_factor * fwhm)`.\n\n    min_points : int, optional, default=20\n        The minimum number of data points required in the noise region to estimate the noise level. If the number\n        of points in the noise region is smaller than this threshold, a warning is issued.\n\n    Returns\n    -------\n    float\n        The computed Signal-to-Noise Ratio (SNR), which is the ratio of the signal strength at `x0` to the\n        standard deviation of the noise. If the noise standard deviation is zero, the SNR is set to infinity.\n\n    Notes\n    -----\n    - The function assumes that the signal has a clear peak at `x0` and that the surrounding data represents noise.\n    - If the noise region contains fewer than `min_points` data points, a warning is raised suggesting the adjustment of `noise_region_factor`.\n\n    Example\n    -------\n    &gt;&gt;&gt; x_data = np.linspace(-10, 10, 1000)\n    &gt;&gt;&gt; y_data = np.exp(-(x_data**2))  # Example Gaussian\n    &gt;&gt;&gt; x0 = 0\n    &gt;&gt;&gt; fwhm = 2.0\n    &gt;&gt;&gt; snr = compute_snr_peaked(x_data, y_data, x0, fwhm)\n    &gt;&gt;&gt; print(snr)\n    \"\"\"\n\n    # Signal strength at x0\n    signal = y_data[np.argmin(np.abs(x_data - x0))]\n\n    # Define noise region (outside noise_region_factor * FWHM)\n    noise_mask = (x_data &lt; (x0 - noise_region_factor * fwhm)) | (\n        x_data &gt; (x0 + noise_region_factor * fwhm)\n    )\n    noise_data = y_data[noise_mask]\n\n    # Check if there are enough data points for noise estimation\n    if len(noise_data) &lt; min_points:\n        Warning(\n            f\"Only {len(noise_data)} points found in the noise region. Consider reducing noise_region_factor.\"\n        )\n\n    # Compute noise standard deviation\n    noise_std = np.std(noise_data)\n\n    # Compute SNR\n    snr = signal / noise_std if noise_std &gt; 0 else np.inf  # Avoid division by zero\n\n    return snr\n</code></pre>"},{"location":"API%20reference/utils/analysis/#sqil_core.utils._analysis.estimate_linear_background","title":"<code>estimate_linear_background(x, data, points_cut=0.1, cut_from_back=False)</code>","text":"<p>Estimates the linear background for a given data set by fitting a linear model to a subset of the data.</p> <p>This function performs a linear regression to estimate the background (offset and slope) from the given data by selecting a portion of the data as specified by the <code>points_cut</code> parameter. The linear fit is applied to either the first or last <code>points_cut</code> fraction of the data, depending on the <code>cut_from_back</code> flag. The estimated background is returned as the coefficients of the linear fit.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>The independent variable data.</p> required <code>data</code> <code>ndarray</code> <p>The dependent variable data, which can be 1D or 2D (e.g., multiple measurements or data points).</p> required <code>points_cut</code> <code>float</code> <p>The fraction of the data to be considered for the linear fit. Default is 0.1 (10% of the data).</p> <code>0.1</code> <code>cut_from_back</code> <code>bool</code> <p>Whether to use the last <code>points_cut</code> fraction of the data (True) or the first fraction (False). Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>list</code> <p>The coefficients of the linear fit: a list with two elements, where the first is the offset (intercept) and the second is the slope.</p> Notes <ul> <li>If <code>data</code> is 2D, the fit is performed on each column of the data separately.</li> <li>The function assumes that <code>x</code> and <code>data</code> have compatible shapes.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; x = np.linspace(0, 10, 100)\n&gt;&gt;&gt; data = 3 * x + 2 + np.random.normal(0, 1, size=(100,))\n&gt;&gt;&gt; coefficients = estimate_linear_background(x, data, points_cut=0.2)\n&gt;&gt;&gt; print(\"Estimated coefficients:\", coefficients)\n</code></pre> Source code in <code>sqil_core/utils/_analysis.py</code> <pre><code>def estimate_linear_background(\n    x: np.ndarray,\n    data: np.ndarray,\n    points_cut: float = 0.1,\n    cut_from_back: bool = False,\n) -&gt; list:\n    \"\"\"\n    Estimates the linear background for a given data set by fitting a linear model to a subset of the data.\n\n    This function performs a linear regression to estimate the background (offset and slope) from the\n    given data by selecting a portion of the data as specified by the `points_cut` parameter. The linear\n    fit is applied to either the first or last `points_cut` fraction of the data, depending on the `cut_from_back`\n    flag. The estimated background is returned as the coefficients of the linear fit.\n\n    Parameters\n    ----------\n    x : np.ndarray\n        The independent variable data.\n    data : np.ndarray\n        The dependent variable data, which can be 1D or 2D (e.g., multiple measurements or data points).\n    points_cut : float, optional\n        The fraction of the data to be considered for the linear fit. Default is 0.1 (10% of the data).\n    cut_from_back : bool, optional\n        Whether to use the last `points_cut` fraction of the data (True) or the first fraction (False).\n        Default is False.\n\n    Returns\n    -------\n    list\n        The coefficients of the linear fit: a list with two elements, where the first is the offset (intercept)\n        and the second is the slope.\n\n    Notes\n    -----\n    - If `data` is 2D, the fit is performed on each column of the data separately.\n    - The function assumes that `x` and `data` have compatible shapes.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; x = np.linspace(0, 10, 100)\n    &gt;&gt;&gt; data = 3 * x + 2 + np.random.normal(0, 1, size=(100,))\n    &gt;&gt;&gt; coefficients = estimate_linear_background(x, data, points_cut=0.2)\n    &gt;&gt;&gt; print(\"Estimated coefficients:\", coefficients)\n    \"\"\"\n    is1D = len(data.shape) == 1\n    points = data.shape[0] if is1D else data.shape[1]\n    cut = int(points * points_cut)\n\n    # Consider just the cut points\n    if not cut_from_back:\n        x_data = x[0:cut] if is1D else x[:, 0:cut]\n        y_data = data[0:cut] if is1D else data[:, 0:cut]\n    else:\n        x_data = x[-cut:] if is1D else x[:, -cut:]\n        y_data = data[-cut:] if is1D else data[:, -cut:]\n\n    ones_column = np.ones_like(x_data[0, :]) if not is1D else np.ones_like(x_data)\n    X = np.vstack([ones_column, x_data[0, :] if not is1D else x_data]).T\n    # Linear fit\n    coefficients, residuals, _, _ = np.linalg.lstsq(\n        X, y_data if is1D else y_data.T, rcond=None\n    )\n\n    return coefficients.T\n</code></pre>"},{"location":"API%20reference/utils/analysis/#sqil_core.utils._analysis.find_closest_index","title":"<code>find_closest_index(arr, target)</code>","text":"<p>Find the index of the element in <code>arr</code> closest to the <code>target</code> value.</p> Source code in <code>sqil_core/utils/_analysis.py</code> <pre><code>def find_closest_index(arr, target):\n    \"\"\"\n    Find the index of the element in `arr` closest to the `target` value.\n    \"\"\"\n\n    return np.abs(arr - target).argmin()\n</code></pre>"},{"location":"API%20reference/utils/analysis/#sqil_core.utils._analysis.find_first_minima_idx","title":"<code>find_first_minima_idx(data)</code>","text":"<p>Find the index of the first local minimum in a 1D array.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>array - like</code> <p>1D sequence of numerical values.</p> required <p>Returns:</p> Type Description <code>int or None</code> <p>Index of the first local minimum, or None if no local minimum is found.</p> Notes <p>A local minimum is defined as a point that is smaller than its immediate neighbors. Uses <code>scipy.signal.argrelextrema</code> to detect local minima.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; data = [3, 2, 4, 1, 5]\n&gt;&gt;&gt; find_first_minima_idx(data)\n1\n</code></pre> Source code in <code>sqil_core/utils/_analysis.py</code> <pre><code>def find_first_minima_idx(data):\n    \"\"\"\n    Find the index of the first local minimum in a 1D array.\n\n    Parameters\n    ----------\n    data : array-like\n        1D sequence of numerical values.\n\n    Returns\n    -------\n    int or None\n        Index of the first local minimum, or None if no local minimum is found.\n\n    Notes\n    -----\n    A local minimum is defined as a point that is smaller than its immediate neighbors.\n    Uses `scipy.signal.argrelextrema` to detect local minima.\n\n    Examples\n    --------\n    &gt;&gt;&gt; data = [3, 2, 4, 1, 5]\n    &gt;&gt;&gt; find_first_minima_idx(data)\n    1\n    \"\"\"\n    data = np.array(data)\n    minima_indices = argrelextrema(data, np.less)[0]\n\n    # Check boundaries for minima (optional)\n    if data.size &lt; 2:\n        return None\n\n    if len(minima_indices) &gt; 0:\n        return minima_indices[0]\n\n    return None\n</code></pre>"},{"location":"API%20reference/utils/analysis/#sqil_core.utils._analysis.get_peaks","title":"<code>get_peaks(x_data, y_data, prominence=None, sort=True)</code>","text":"<p>Detects and returns peaks in a 1D signal based on prominence.</p> <p>Parameters:</p> Name Type Description Default <code>x_data</code> <code>ndarray</code> <p>1D array of x-values corresponding to <code>y_data</code> (e.g., frequency or time axis).</p> required <code>y_data</code> <code>ndarray</code> <p>1D array of y-values representing the signal in which to find peaks.</p> required <code>prominence</code> <code>float or None</code> <p>Minimum prominence of peaks to detect. If None, defaults to 5% of the maximum value in <code>y_data</code>.</p> <code>None</code> <code>sort</code> <code>bool</code> <p>If True, peaks are sorted in descending order of magnitude. Default is True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>peak_freqs</code> <code>ndarray</code> <p>x-values at which peaks occur.</p> <code>peak_magnitudes</code> <code>ndarray</code> <p>y-values (magnitudes) at the detected peaks.</p> Notes <ul> <li>Uses <code>scipy.signal.find_peaks</code> for detection.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; x = np.linspace(0, 10, 1000)\n&gt;&gt;&gt; y = np.sin(2 * np.pi * x) + 0.1 * np.random.randn(1000)\n&gt;&gt;&gt; freqs, mags = get_peaks(x, np.abs(y))\n&gt;&gt;&gt; print(freqs[:3], mags[:3])  # Show top 3 peak locations and magnitudes\n</code></pre> Source code in <code>sqil_core/utils/_analysis.py</code> <pre><code>def get_peaks(x_data, y_data, prominence: float | None = None, sort=True):\n    \"\"\"\n    Detects and returns peaks in a 1D signal based on prominence.\n\n    Parameters\n    ----------\n    x_data : np.ndarray\n        1D array of x-values corresponding to `y_data` (e.g., frequency or time axis).\n    y_data : np.ndarray\n        1D array of y-values representing the signal in which to find peaks.\n    prominence : float or None, optional\n        Minimum prominence of peaks to detect. If None, defaults to 5% of the maximum\n        value in `y_data`.\n    sort : bool, optional\n        If True, peaks are sorted in descending order of magnitude. Default is True.\n\n    Returns\n    -------\n    peak_freqs : np.ndarray\n        x-values at which peaks occur.\n    peak_magnitudes : np.ndarray\n        y-values (magnitudes) at the detected peaks.\n\n    Notes\n    -----\n    - Uses `scipy.signal.find_peaks` for detection.\n\n    Examples\n    --------\n    &gt;&gt;&gt; x = np.linspace(0, 10, 1000)\n    &gt;&gt;&gt; y = np.sin(2 * np.pi * x) + 0.1 * np.random.randn(1000)\n    &gt;&gt;&gt; freqs, mags = get_peaks(x, np.abs(y))\n    &gt;&gt;&gt; print(freqs[:3], mags[:3])  # Show top 3 peak locations and magnitudes\n    \"\"\"\n\n    if prominence is None:\n        prominence = 0.05 * np.max(y_data)\n\n    # Find peaks\n    peaks, properties = find_peaks(y_data, prominence=prominence)\n\n    # Get the corresponding frequencies and magnitudes\n    peak_freqs = x_data[peaks]\n    peak_magnitudes = y_data[peaks]\n\n    if sort:\n        sorted_indices = np.argsort(peak_magnitudes)[::-1]\n        peak_freqs = peak_freqs[sorted_indices]\n        peak_magnitudes = peak_magnitudes[sorted_indices]\n\n    return peak_freqs, peak_magnitudes\n</code></pre>"},{"location":"API%20reference/utils/analysis/#sqil_core.utils._analysis.line_between_2_points","title":"<code>line_between_2_points(x1, y1, x2, y2)</code>","text":"<p>Computes the equation of a line passing through two points.</p> <p>Given two points (x1, y1) and (x2, y2), this function returns the y-intercept and slope of the line connecting them. If x1 and x2 are the same, the function returns y1 as the intercept and a slope of 0 to avoid division by zero.</p> <p>Parameters:</p> Name Type Description Default <code>x1</code> <code>float</code> <p>The x-coordinate of the first point.</p> required <code>y1</code> <code>float</code> <p>The y-coordinate of the first point.</p> required <code>x2</code> <code>float</code> <p>The x-coordinate of the second point.</p> required <code>y2</code> <code>float</code> <p>The y-coordinate of the second point.</p> required <p>Returns:</p> Type Description <code>tuple[float, float]</code> <p>A tuple containing: - The y-intercept (float), which is y1. - The slope (float) of the line passing through the points.</p> Notes <ul> <li>If x1 and x2 are the same, the function assumes a vertical line and returns a slope of 0.</li> <li>The returned y-intercept is based on y1 for consistency in edge cases.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; line_between_2_points(1, 2, 3, 4)\n(2, 1.0)\n&gt;&gt;&gt; line_between_2_points(2, 5, 2, 10)\n(5, 0)\n</code></pre> Source code in <code>sqil_core/utils/_analysis.py</code> <pre><code>def line_between_2_points(\n    x1: float, y1: float, x2: float, y2: float\n) -&gt; tuple[float, float]:\n    \"\"\"\n    Computes the equation of a line passing through two points.\n\n    Given two points (x1, y1) and (x2, y2), this function returns the y-intercept and slope of the line\n    connecting them. If x1 and x2 are the same, the function returns y1 as the intercept and a slope of 0\n    to avoid division by zero.\n\n    Parameters\n    ----------\n    x1 : float\n        The x-coordinate of the first point.\n    y1 : float\n        The y-coordinate of the first point.\n    x2 : float\n        The x-coordinate of the second point.\n    y2 : float\n        The y-coordinate of the second point.\n\n    Returns\n    -------\n    tuple[float, float]\n        A tuple containing:\n        - The y-intercept (float), which is y1.\n        - The slope (float) of the line passing through the points.\n\n    Notes\n    -----\n    - If x1 and x2 are the same, the function assumes a vertical line and returns a slope of 0.\n    - The returned y-intercept is based on y1 for consistency in edge cases.\n\n    Examples\n    --------\n    &gt;&gt;&gt; line_between_2_points(1, 2, 3, 4)\n    (2, 1.0)\n    &gt;&gt;&gt; line_between_2_points(2, 5, 2, 10)\n    (5, 0)\n    \"\"\"\n    if x1 == x2:\n        return np.inf, y1\n    slope = (y2 - y1) / (x2 - x1)\n    intercept = y1 - slope * x1\n    return slope, intercept\n</code></pre>"},{"location":"API%20reference/utils/analysis/#sqil_core.utils._analysis.linear_interpolation","title":"<code>linear_interpolation(x, x1, y1, x2, y2)</code>","text":"<p>Performs linear interpolation to estimate the value of y at a given x.</p> <p>This function computes the interpolated y-value for a given x using two known points (x1, y1) and (x2, y2) on a straight line. It supports both scalar and array inputs for x, enabling vectorized operations.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>float or ndarray</code> <p>The x-coordinate(s) at which to interpolate.</p> required <code>x1</code> <code>float</code> <p>The x-coordinate of the first known point.</p> required <code>y1</code> <code>float</code> <p>The y-coordinate of the first known point.</p> required <code>x2</code> <code>float</code> <p>The x-coordinate of the second known point.</p> required <code>y2</code> <code>float</code> <p>The y-coordinate of the second known point.</p> required <p>Returns:</p> Type Description <code>float or ndarray</code> <p>The interpolated y-value(s) at x.</p> Notes <ul> <li>If x1 and x2 are the same, the function returns y1 to prevent division by zero.</li> <li>Assumes that x lies between x1 and x2 for meaningful interpolation.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; linear_interpolation(3, 2, 4, 6, 8)\n5.0\n&gt;&gt;&gt; x_vals = np.array([3, 4, 5])\n&gt;&gt;&gt; linear_interpolation(x_vals, 2, 4, 6, 8)\narray([5., 6., 7.])\n</code></pre> Source code in <code>sqil_core/utils/_analysis.py</code> <pre><code>def linear_interpolation(\n    x: float | np.ndarray, x1: float, y1: float, x2: float, y2: float\n) -&gt; float | np.ndarray:\n    \"\"\"\n    Performs linear interpolation to estimate the value of y at a given x.\n\n    This function computes the interpolated y-value for a given x using two known points (x1, y1) and (x2, y2)\n    on a straight line. It supports both scalar and array inputs for x, enabling vectorized operations.\n\n    Parameters\n    ----------\n    x : float or np.ndarray\n        The x-coordinate(s) at which to interpolate.\n    x1 : float\n        The x-coordinate of the first known point.\n    y1 : float\n        The y-coordinate of the first known point.\n    x2 : float\n        The x-coordinate of the second known point.\n    y2 : float\n        The y-coordinate of the second known point.\n\n    Returns\n    -------\n    float or np.ndarray\n        The interpolated y-value(s) at x.\n\n    Notes\n    -----\n    - If x1 and x2 are the same, the function returns y1 to prevent division by zero.\n    - Assumes that x lies between x1 and x2 for meaningful interpolation.\n\n    Examples\n    --------\n    &gt;&gt;&gt; linear_interpolation(3, 2, 4, 6, 8)\n    5.0\n    &gt;&gt;&gt; x_vals = np.array([3, 4, 5])\n    &gt;&gt;&gt; linear_interpolation(x_vals, 2, 4, 6, 8)\n    array([5., 6., 7.])\n    \"\"\"\n    if x1 == x2:\n        return y1\n    return y1 + (x - x1) * (y2 - y1) / (x2 - x1)\n</code></pre>"},{"location":"API%20reference/utils/analysis/#sqil_core.utils._analysis.remove_linear_background","title":"<code>remove_linear_background(x, data, points_cut=0.1)</code>","text":"<p>Removes a linear background from the input data (e.g. the phase background of a spectroscopy).</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>Input data. Can be a 1D vector or a 2D matrix.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>The input data with the linear background removed. The shape of the returned array matches the input <code>data</code>.</p> Source code in <code>sqil_core/utils/_analysis.py</code> <pre><code>def remove_linear_background(\n    x: np.ndarray, data: np.ndarray, points_cut=0.1\n) -&gt; np.ndarray:\n    \"\"\"Removes a linear background from the input data (e.g. the phase background\n    of a spectroscopy).\n\n\n    Parameters\n    ----------\n    data : np.ndarray\n        Input data. Can be a 1D vector or a 2D matrix.\n\n    Returns\n    -------\n    np.ndarray\n        The input data with the linear background removed. The shape of the\n        returned array matches the input `data`.\n    \"\"\"\n    coefficients = estimate_linear_background(x, data, points_cut)\n\n    # Remove background over the whole array\n    is1D = len(data.shape) == 1\n    ones_column = np.ones_like(x[0, :]) if not is1D else np.ones_like(x)\n    X = np.vstack([ones_column, x[0, :] if not is1D else x]).T\n    return data - (X @ coefficients.T).T\n</code></pre>"},{"location":"API%20reference/utils/analysis/#sqil_core.utils._analysis.remove_offset","title":"<code>remove_offset(data, avg=3)</code>","text":"<p>Removes the initial offset from a data matrix or vector by subtracting the average of the first <code>avg</code> points. After applying this function, the first point of each column of the data will be shifted to (about) 0.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>Input data, either a 1D vector or a 2D matrix</p> required <code>avg</code> <code>int</code> <p>The number of initial points to average when calculating the offset, by default 3</p> <code>3</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>The input data with the offset removed</p> Source code in <code>sqil_core/utils/_analysis.py</code> <pre><code>def remove_offset(data: np.ndarray, avg: int = 3) -&gt; np.ndarray:\n    \"\"\"Removes the initial offset from a data matrix or vector by subtracting\n    the average of the first `avg` points. After applying this function,\n    the first point of each column of the data will be shifted to (about) 0.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        Input data, either a 1D vector or a 2D matrix\n    avg : int, optional\n        The number of initial points to average when calculating\n        the offset, by default 3\n\n    Returns\n    -------\n    np.ndarray\n       The input data with the offset removed\n    \"\"\"\n    is1D = len(data.shape) == 1\n    if is1D:\n        return data - np.mean(data[0:avg])\n    return data - np.mean(data[:, 0:avg], axis=1).reshape(data.shape[0], 1)\n</code></pre>"},{"location":"API%20reference/utils/analysis/#sqil_core.utils._analysis.soft_normalize","title":"<code>soft_normalize(data)</code>","text":"<p>Apply soft normalization to a 1D or 2D array with optional NaNs.</p> <p>This function performs z-score normalization followed by a smooth non-linear compression using a hyperbolic tangent (tanh) function. It is designed to reduce the effect of outliers while preserving the dynamic range of typical data values. The result is rescaled to [0, 1].</p> <p>For 2D arrays, normalization is done row-wise, but compression is based on a global threshold across all non-NaN entries.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>Input data, must be a 1D or 2D NumPy array. Can contain NaNs.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Normalized data, same shape as input, with values scaled to [0, 1]. NaNs are preserved.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>data</code> is not 1D or 2D.</p> Notes <ul> <li>Z-score normalization is done using nanmean and nanstd.</li> <li>Outliers are compressed using a tanh centered at a scaled threshold.</li> <li>Output values are guaranteed to be in [0, 1] range, except NaNs.</li> <li>Rows with zero standard deviation are flattened to 0.5.</li> </ul> Source code in <code>sqil_core/utils/_analysis.py</code> <pre><code>def soft_normalize(data: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Apply soft normalization to a 1D or 2D array with optional NaNs.\n\n    This function performs z-score normalization followed by a smooth\n    non-linear compression using a hyperbolic tangent (tanh) function.\n    It is designed to reduce the effect of outliers while preserving\n    the dynamic range of typical data values. The result is rescaled to [0, 1].\n\n    For 2D arrays, normalization is done row-wise, but compression is\n    based on a global threshold across all non-NaN entries.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        Input data, must be a 1D or 2D NumPy array. Can contain NaNs.\n\n    Returns\n    -------\n    np.ndarray\n        Normalized data, same shape as input, with values scaled to [0, 1].\n        NaNs are preserved.\n\n    Raises\n    ------\n    ValueError\n        If `data` is not 1D or 2D.\n\n    Notes\n    -----\n    - Z-score normalization is done using nanmean and nanstd.\n    - Outliers are compressed using a tanh centered at a scaled threshold.\n    - Output values are guaranteed to be in [0, 1] range, except NaNs.\n    - Rows with zero standard deviation are flattened to 0.5.\n    \"\"\"\n\n    if data.ndim not in [1, 2]:\n        raise ValueError(\"Input must be 1D or 2D\")\n\n    data = np.array(data, dtype=np.float64)\n    nan_mask = np.isnan(data)\n\n    if data.ndim == 1:\n        mean = np.nanmean(data)\n        std = np.nanstd(data)\n        std = 1.0 if std == 0 else std\n        abs_z = np.abs((data - mean) / std)\n    else:\n        mean = np.nanmean(data, axis=1, keepdims=True)\n        std = np.nanstd(data, axis=1, keepdims=True)\n        std = np.where(std == 0, 1.0, std)\n        abs_z = np.abs((data - mean) / std)\n\n    # Flatten over all values for global thresholding\n    flat_abs_z = abs_z[~nan_mask]\n    if flat_abs_z.size == 0:\n        return np.full_like(data, 0.5)\n\n    threshold = 4.0 * np.mean(flat_abs_z)\n    alpha = 1.0 / (4.0 * np.std(flat_abs_z)) if np.std(flat_abs_z) != 0 else 1.0\n\n    compressed = np.tanh(alpha * (abs_z - threshold))\n\n    # Rescale to [0, 1]\n    compressed[nan_mask] = np.nan\n    min_val = np.nanmin(compressed)\n    max_val = np.nanmax(compressed)\n    if max_val == min_val:\n        rescaled = np.full_like(compressed, 0.5)\n    else:\n        rescaled = (compressed - min_val) / (max_val - min_val)\n\n    rescaled[nan_mask] = np.nan\n    return rescaled\n</code></pre>"},{"location":"API%20reference/utils/formatter/","title":"Formatter","text":"<p>Format text for printing in a readable way</p>"},{"location":"API%20reference/utils/formatter/#sqil_core.utils._formatter.ParamInfo","title":"<code>ParamInfo</code>","text":"<p>Parameter information for items of param_dict</p> <p>Attributes:     id (str): QPU key     value (any): the value of the parameter     name (str): full name of the parameter (e.g. Readout frequency)     symbol (str): symbol of the parameter in Latex notation (e.g. f_{RO})     unit (str): base unit of measurement (e.g. Hz)     scale (int): the scale that should be generally applied to raw data (e.g. 1e-9 to take raw Hz to GHz)</p> Source code in <code>sqil_core/utils/_formatter.py</code> <pre><code>class ParamInfo:\n    \"\"\"Parameter information for items of param_dict\n\n    Attributes:\n        id (str): QPU key\n        value (any): the value of the parameter\n        name (str): full name of the parameter (e.g. Readout frequency)\n        symbol (str): symbol of the parameter in Latex notation (e.g. f_{RO})\n        unit (str): base unit of measurement (e.g. Hz)\n        scale (int): the scale that should be generally applied to raw data (e.g. 1e-9 to take raw Hz to GHz)\n    \"\"\"\n\n    def __init__(self, id, value=None, metadata=None):\n        self.id = id\n        self.value = value\n\n        if metadata is not None:\n            meta = metadata\n        elif id in PARAM_METADATA:\n            meta = PARAM_METADATA[id]\n        else:\n            meta = {}\n\n        self.name = meta.get(\"name\", None)\n        self.symbol = meta.get(\"symbol\", id)\n        self.unit = meta.get(\"unit\", \"\")\n        self.scale = meta.get(\"scale\", 1)\n        self.precision = meta.get(\"precision\", 3)\n\n        if self.name is None:\n            self.name = self.id[0].upper() + self.id[1:].replace(\"_\", \" \")\n\n    def to_dict(self):\n        \"\"\"Convert ParamInfo to a dictionary.\"\"\"\n        return {\n            \"id\": self.id,\n            \"value\": self.value,\n            \"name\": self.name,\n            \"symbol\": self.symbol,\n            \"unit\": self.unit,\n            \"scale\": self.scale,\n            \"precision\": self.precision,\n        }\n\n    @property\n    def name_and_unit(self, latex=True):\n        unit = f\"[{self.rescaled_unit}]\" if self.unit or self.scale != 1 else \"\"\n        if unit == \"\":\n            return unit\n        return self.name + rf\" ${unit}$\" if latex else rf\" {unit}\"\n\n    @property\n    def rescaled_unit(self):\n        # if self.unit == \"\":\n        #     return self.unit\n        exponent = -(int(f\"{self.scale:.0e}\".split(\"e\")[1]) // 3) * 3\n        unit = f\"{_EXP_UNIT_MAP[exponent]}{self.unit}\"\n        return unit\n\n    @property\n    def symbol_and_value(self, latex=True):\n        sym = f\"${self.symbol}$\" if latex else self.symbol\n        equal = f\"$=$\" if latex else \" = \"\n        val = format_number(self.value, self.precision, self.unit, latex=latex)\n        return f\"{sym}{equal}{val}\"\n\n    def __str__(self):\n        \"\"\"Return a JSON-formatted string of the object.\"\"\"\n        return json.dumps(self.to_dict())\n\n    def __eq__(self, other):\n        if isinstance(other, ParamInfo):\n            return (self.id == other.id) &amp; (self.value == other.value)\n        if isinstance(other, (int, float, complex, str)):\n            return self.value == other\n        return False\n\n    def __bool__(self):\n        return bool(self.id)\n</code></pre>"},{"location":"API%20reference/utils/formatter/#sqil_core.utils._formatter.ParamInfo.__str__","title":"<code>__str__()</code>","text":"<p>Return a JSON-formatted string of the object.</p> Source code in <code>sqil_core/utils/_formatter.py</code> <pre><code>def __str__(self):\n    \"\"\"Return a JSON-formatted string of the object.\"\"\"\n    return json.dumps(self.to_dict())\n</code></pre>"},{"location":"API%20reference/utils/formatter/#sqil_core.utils._formatter.ParamInfo.to_dict","title":"<code>to_dict()</code>","text":"<p>Convert ParamInfo to a dictionary.</p> Source code in <code>sqil_core/utils/_formatter.py</code> <pre><code>def to_dict(self):\n    \"\"\"Convert ParamInfo to a dictionary.\"\"\"\n    return {\n        \"id\": self.id,\n        \"value\": self.value,\n        \"name\": self.name,\n        \"symbol\": self.symbol,\n        \"unit\": self.unit,\n        \"scale\": self.scale,\n        \"precision\": self.precision,\n    }\n</code></pre>"},{"location":"API%20reference/utils/formatter/#sqil_core.utils._formatter.format_number","title":"<code>format_number(num, precision=3, unit='', latex=True)</code>","text":"<p>Format a number (or an array of numbers) in a nice way for printing.</p> <p>Parameters:</p> Name Type Description Default <code>num</code> <code>float | ndarray</code> <p>Input number (or array). Should not be rescaled, e.g. input values in Hz, NOT GHz</p> required <code>precision</code> <code>int</code> <p>The number of digits of the output number. Must be &gt;= 3.</p> <code>3</code> <code>unit</code> <code>str</code> <p>Unit of measurement, by default ''</p> <code>''</code> <code>latex</code> <code>bool</code> <p>Include Latex syntax, by default True</p> <code>True</code> <p>Returns:</p> Type Description <code>str</code> <p>Formatted number</p> Source code in <code>sqil_core/utils/_formatter.py</code> <pre><code>def format_number(\n    num: float | np.ndarray, precision: int = 3, unit: str = \"\", latex: bool = True\n) -&gt; str:\n    \"\"\"Format a number (or an array of numbers) in a nice way for printing.\n\n    Parameters\n    ----------\n    num : float | np.ndarray\n        Input number (or array). Should not be rescaled,\n        e.g. input values in Hz, NOT GHz\n    precision : int\n        The number of digits of the output number. Must be &gt;= 3.\n    unit : str, optional\n        Unit of measurement, by default ''\n    latex : bool, optional\n        Include Latex syntax, by default True\n\n    Returns\n    -------\n    str\n        Formatted number\n    \"\"\"\n    # Handle arrays\n    if isinstance(num, (list, np.ndarray)):\n        return [format_number(n, precision, unit, latex) for n in num]\n\n    # Return if not a number\n    if not isinstance(num, (int, float, complex)):\n        return num\n\n    # Format number\n    exp_form = f\"{num:.12e}\"\n    base, exponent = exp_form.split(\"e\")\n    # Make exponent a multiple of 3\n    base = float(base) * 10 ** (int(exponent) % 3)\n    exponent = (int(exponent) // 3) * 3\n    # Apply precision to the base\n    if precision &lt; 3:\n        precision = 3\n    base_precise = _cut_to_significant_digits(\n        base, precision + 1\n    )  # np.round(base, precision - (int(exponent) % 3))\n    base_precise = np.round(\n        base_precise, precision - len(str(base_precise).split(\".\")[0])\n    )\n    if int(base_precise) == float(base_precise):\n        base_precise = int(base_precise)\n\n    # Build string\n    if unit:\n        res = f\"{base_precise}{'~' if latex else ' '}{_EXP_UNIT_MAP[exponent]}{unit}\"\n    else:\n        res = f\"{base_precise}\" + (f\" x 10^{{{exponent}}}\" if exponent != 0 else \"\")\n    return f\"${res}$\" if latex else res\n</code></pre>"},{"location":"API%20reference/utils/formatter/#sqil_core.utils._formatter.get_name_and_unit","title":"<code>get_name_and_unit(param_id)</code>","text":"<p>Get the name and unit of measurement of a prameter, e.g. Frequency [GHz].</p> <p>Parameters:</p> Name Type Description Default <code>param</code> <code>str</code> <p>Parameter ID, as defined in the param_dict.json file.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Name and [unit]</p> Source code in <code>sqil_core/utils/_formatter.py</code> <pre><code>def get_name_and_unit(param_id: str) -&gt; str:\n    \"\"\"Get the name and unit of measurement of a prameter, e.g. Frequency [GHz].\n\n    Parameters\n    ----------\n    param : str\n        Parameter ID, as defined in the param_dict.json file.\n\n    Returns\n    -------\n    str\n        Name and [unit]\n    \"\"\"\n    meta = PARAM_METADATA[param_id]\n    scale = meta[\"scale\"] if \"scale\" in meta else 1\n    exponent = -(int(f\"{scale:.0e}\".split(\"e\")[1]) // 3) * 3\n    return f\"{meta['name']} [{_EXP_UNIT_MAP[exponent]}{meta['unit']}]\"\n</code></pre>"},{"location":"API%20reference/utils/plot/","title":"Plot","text":""},{"location":"API%20reference/utils/plot/#sqil_core.utils._plot.build_title","title":"<code>build_title(title, path, params)</code>","text":"<p>Build a plot title that includes the values of given parameters found in the params_dict.json file, e.g. One tone with I = 0.5 mA.</p> <p>Parameters:</p> Name Type Description Default <code>title</code> <code>str</code> <p>Title of the plot to which the parameters will be appended.</p> required <code>path</code> <code>str</code> <p>Path to the param_dict.json file.</p> required <code>params</code> <code>List[str]</code> <p>List of keys of parameters in the param_dict.json file.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The original title followed by parameter values.</p> Source code in <code>sqil_core/utils/_plot.py</code> <pre><code>def build_title(title: str, path: str, params: list[str]) -&gt; str:\n    \"\"\"Build a plot title that includes the values of given parameters found in\n    the params_dict.json file, e.g. One tone with I = 0.5 mA.\n\n    Parameters\n    ----------\n    title : str\n        Title of the plot to which the parameters will be appended.\n\n    path: str\n        Path to the param_dict.json file.\n\n    params : List[str]\n        List of keys of parameters in the param_dict.json file.\n\n    Returns\n    -------\n    str\n        The original title followed by parameter values.\n    \"\"\"\n    dic = read_json(f\"{path}/param_dict.json\")\n    title += \" with \"\n    for idx, param in enumerate(params):\n        if not (param in PARAM_METADATA.keys()) or not (param in dic):\n            title += f\"{param} = ? &amp; \"\n            continue\n        meta = PARAM_METADATA[param]\n        value = format_number(dic[param], 3, meta[\"unit\"])\n        title += f\"${meta['symbol']} =${value} &amp; \"\n        if idx % 2 == 0 and idx != 0:\n            title += \"\\n\"\n    return title[0:-3]\n</code></pre>"},{"location":"API%20reference/utils/plot/#sqil_core.utils._plot.finalize_plot","title":"<code>finalize_plot(fig, title, fit_res=None, qubit_params={}, updated_params={}, sweep_info={}, relevant_params=[])</code>","text":"<p>Annotates a matplotlib figure with experiment parameters, fit quality, and title.</p> <p>Parameters:</p> Name Type Description Default <code>fig</code> <code>Figure</code> <p>The figure object to annotate.</p> required <code>title</code> <code>str</code> <p>Title text to use for the plot.</p> required <code>fit_res</code> <code>FitResult</code> <p>Fit result object containing model name and quality summary.</p> <code>None</code> <code>qubit_params</code> <code>ParamDict</code> <p>Dictionary of experimental qubit parameters, indexed by parameter ID.</p> <code>{}</code> <code>updated_params</code> <code>dict</code> <p>Dictionary of updated parameters (e.g., from fitting), where keys are param IDs and values are numeric or symbolic parameter values.</p> <code>{}</code> <code>sweep_info</code> <code>dict</code> <p>Information about sweep parameters (e.g., their IDs and labels).</p> <code>{}</code> <code>relevant_params</code> <code>list</code> <p>List of parameter IDs considered relevant for display under \"Experiment\".</p> <code>[]</code> Source code in <code>sqil_core/utils/_plot.py</code> <pre><code>def finalize_plot(\n    fig,\n    title,\n    fit_res: FitResult = None,\n    qubit_params: ParamDict = {},\n    updated_params: dict = {},\n    sweep_info={},\n    relevant_params=[],\n):\n    \"\"\"\n    Annotates a matplotlib figure with experiment parameters, fit quality, and title.\n\n    Parameters\n    ----------\n    fig : matplotlib.figure.Figure\n        The figure object to annotate.\n    title : str\n        Title text to use for the plot.\n    fit_res : FitResult, optional\n        Fit result object containing model name and quality summary.\n    qubit_params : ParamDict, optional\n        Dictionary of experimental qubit parameters, indexed by parameter ID.\n    updated_params : dict, optional\n        Dictionary of updated parameters (e.g., from fitting), where keys are param IDs\n        and values are numeric or symbolic parameter values.\n    sweep_info : dict, optional\n        Information about sweep parameters (e.g., their IDs and labels).\n    relevant_params : list, optional\n        List of parameter IDs considered relevant for display under \"Experiment\".\n    \"\"\"\n    # Make a summary of relevant experimental parameters\n    exp_params_keys = get_relevant_exp_parameters(\n        qubit_params, relevant_params, [info.id for info in sweep_info]\n    )\n    params_str = \",   \".join(\n        [qubit_params[id].symbol_and_value for id in exp_params_keys]\n    )\n    # Make a summary of the updated qubit parameters\n    updated_params_info = {k: ParamInfo(k, v) for k, v in updated_params.items()}\n    update_params_str = \",   \".join(\n        [updated_params_info[id].symbol_and_value for id in updated_params_info.keys()]\n    )\n\n    # Find appropriate y_position to print text\n    bbox = fig.get_window_extent().transformed(fig.dpi_scale_trans.inverted())\n    fig_height_inches = bbox.height\n    if fig_height_inches &lt; 8:\n        y_pos = -0.05\n    elif fig_height_inches &lt; 10:\n        y_pos = -0.03\n    elif fig_height_inches &lt; 13:\n        y_pos = -0.02\n    else:\n        y_pos = -0.01\n\n    # Add text to the plot\n    fig.suptitle(f\"{title}\\n\" + update_params_str)\n    if fit_res:\n        fig.text(0.02, y_pos, f\"Model: {fit_res.model_name} - {fit_res.quality()}\")\n    if params_str:\n        fig.text(0.4, y_pos, \"Experiment:   \" + params_str, ha=\"left\")\n</code></pre>"},{"location":"API%20reference/utils/plot/#sqil_core.utils._plot.get_x_id_by_plot_dim","title":"<code>get_x_id_by_plot_dim(exp_id, plot_dim, sweep_param_id)</code>","text":"<p>Returns the param_id of the parameter that should be used as the x-axis.</p> Source code in <code>sqil_core/utils/_plot.py</code> <pre><code>def get_x_id_by_plot_dim(exp_id: str, plot_dim: str, sweep_param_id: str | None) -&gt; str:\n    \"\"\"Returns the param_id of the parameter that should be used as the x-axis.\"\"\"\n    if exp_id == \"CW_onetone\" or exp_id == \"pulsed_onetone\":\n        if plot_dim == \"1\":\n            return sweep_param_id or \"ro_freq\"\n        return \"ro_freq\"\n    elif exp_id == \"CW_twotone\" or exp_id == \"pulsed_twotone\":\n        if plot_dim == \"1\":\n            return sweep_param_id or \"qu_freq\"\n        return \"qu_freq\"\n</code></pre>"},{"location":"API%20reference/utils/plot/#sqil_core.utils._plot.guess_plot_dimension","title":"<code>guess_plot_dimension(f, sweep=[], threshold_2D=10)</code>","text":"<p>Guess if the plot should be a 1D line, a collection of 1D lines (1.5D), or a 2D color plot.</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>ndarray</code> <p>Main variable, usually frequency</p> required <code>sweep</code> <code>Union[ndarray, List]</code> <p>Sweep variable, by default []</p> <code>[]</code> <code>threshold_2D</code> <code>int</code> <p>Threshold of sweeping parameters after which the data is considered, by default 10</p> <code>10</code> <p>Returns:</p> Type Description <code>Tuple[Union['1', '1.5', '2'], ndarray]</code> <p>The plot dimension ('1', '1.5' or '2') and the vector that should be used as the x axis in the plot.</p> Source code in <code>sqil_core/utils/_plot.py</code> <pre><code>def guess_plot_dimension(\n    f: np.ndarray, sweep: np.ndarray | list = [], threshold_2D=10\n) -&gt; tuple[list[\"1\", \"1.5\", \"2\"] | np.ndarray]:\n    \"\"\"Guess if the plot should be a 1D line, a collection of 1D lines (1.5D),\n    or a 2D color plot.\n\n    Parameters\n    ----------\n    f : np.ndarray\n        Main variable, usually frequency\n    sweep : Union[np.ndarray, List], optional\n        Sweep variable, by default []\n    threshold_2D : int, optional\n        Threshold of sweeping parameters after which the data is considered, by default 10\n\n    Returns\n    -------\n    Tuple[Union['1', '1.5', '2'], np.ndarray]\n        The plot dimension ('1', '1.5' or '2') and the vector that should be used as the x\n        axis in the plot.\n    \"\"\"\n    if len(sweep) &gt; threshold_2D:\n        return \"2\"\n    elif len(f.shape) == 2 and len(sweep.shape) == 1:\n        return \"1.5\"\n    else:\n        return \"1\"\n</code></pre>"},{"location":"API%20reference/utils/plot/#sqil_core.utils._plot.plot_mag_phase","title":"<code>plot_mag_phase(path=None, datadict=None, raw=False)</code>","text":"<p>Plot the magnitude and phase of complex measurement data from an db path or in-memory dictionary.</p> <p>This function generates either a 1D or 2D plot of the magnitude and phase of complex data, depending on the presence of sweep parameters. It supports normalization and background subtraction.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str or None</code> <p>Path to the folder containing measurement data. Required if <code>datadict</code> is not provided.</p> <code>None</code> <code>datadict</code> <code>dict or None</code> <p>Pre-loaded data dictionary with schema, typically extracted using <code>extract_h5_data</code>. Required if <code>path</code> is not provided.</p> <code>None</code> <code>raw</code> <code>bool</code> <p>If True, skip normalization and background subtraction for 2D plots. Useful for viewing raw data.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>fig</code> <code>Figure</code> <p>The matplotlib Figure object containing the plot.</p> <code>axs</code> <code>matplotlib.axes.Axes or ndarray of Axes</code> <p>The Axes object(s) used for the subplot(s).</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If neither <code>path</code> nor <code>datadict</code> is provided.</p> Notes <ul> <li>Axes and units are automatically inferred from the schema in the dataset.</li> </ul> Source code in <code>sqil_core/utils/_plot.py</code> <pre><code>def plot_mag_phase(path=None, datadict=None, raw=False):\n    \"\"\"\n    Plot the magnitude and phase of complex measurement data from an db path or in-memory dictionary.\n\n    This function generates either a 1D or 2D plot of the magnitude and phase of complex data,\n    depending on the presence of sweep parameters. It supports normalization and background\n    subtraction.\n\n    Parameters\n    ----------\n    path : str or None, optional\n        Path to the folder containing measurement data. Required if `datadict` is not provided.\n    datadict : dict or None, optional\n        Pre-loaded data dictionary with schema, typically extracted using `extract_h5_data`.\n        Required if `path` is not provided.\n    raw : bool, default False\n        If True, skip normalization and background subtraction for 2D plots. Useful for viewing raw data.\n\n    Returns\n    -------\n    fig : matplotlib.figure.Figure\n        The matplotlib Figure object containing the plot.\n    axs : matplotlib.axes.Axes or ndarray of Axes\n        The Axes object(s) used for the subplot(s).\n\n    Raises\n    ------\n    Exception\n        If neither `path` nor `datadict` is provided.\n\n    Notes\n    -----\n    - Axes and units are automatically inferred from the schema in the dataset.\n    \"\"\"\n\n    all_data, all_info, _ = get_data_and_info(path=path, datadict=datadict)\n    x_data, y_data, sweeps = all_data\n    x_info, y_info, sweep_info = all_info\n\n    # Rescale data\n    x_data_scaled = x_data * x_info.scale\n    y_data_scaled = y_data * y_info.scale\n    y_unit = f\" [{y_info.rescaled_unit}]\" if y_info.unit else \"\"\n\n    set_plot_style(plt)\n\n    if len(sweeps) == 0:  # 1D plot\n        fig, axs = plt.subplots(2, 1, figsize=(20, 12), sharex=True)\n\n        axs[0].plot(x_data_scaled, np.abs(y_data_scaled), \"o\")\n        axs[0].set_ylabel(\"Magnitude\" + y_unit)\n        axs[0].tick_params(labelbottom=True)\n        axs[0].xaxis.set_tick_params(\n            which=\"both\", labelbottom=True\n        )  # Redundant for safety\n\n        axs[1].plot(x_data_scaled, np.unwrap(np.angle(y_data_scaled)), \"o\")\n        axs[1].set_xlabel(x_info.name_and_unit)\n        axs[1].set_ylabel(\"Phase [rad]\")\n    else:  # 2D plot\n        fig, axs = plt.subplots(1, 2, figsize=(24, 12), sharex=True, sharey=True)\n\n        # Process mag and phase\n        mag, phase = np.abs(y_data), np.unwrap(np.angle(y_data))\n        if not raw:\n            mag = soft_normalize(remove_offset(mag))\n            flat_phase = remove_linear_background(x_data, phase, points_cut=1)\n            phase = soft_normalize(flat_phase)\n        # Load sweep parameter\n        sweep0_info = sweep_info[0]\n        sweep0_scaled = sweeps[0] * sweep0_info.scale\n\n        c0 = axs[0].pcolormesh(\n            x_data_scaled,\n            sweep0_scaled,\n            mag,\n            shading=\"auto\",\n            cmap=\"PuBu\",\n        )\n        if raw:\n            fig.colorbar(c0, ax=axs[0])\n            axs[0].set_title(\"Magnitude\" + y_unit)\n        else:\n            axs[0].set_title(\"Magnitude (normalized)\")\n        axs[0].set_xlabel(x_info.name_and_unit)\n        axs[0].set_ylabel(sweep0_info.name_and_unit)\n\n        c1 = axs[1].pcolormesh(\n            x_data_scaled,\n            sweep0_scaled,\n            phase,\n            shading=\"auto\",\n            cmap=\"PuBu\",\n        )\n        if raw:\n            fig.colorbar(c1, ax=axs[1])\n            axs[1].set_title(\"Phase [rad]\")\n        else:\n            axs[1].set_title(\"Phase (normalized)\")\n        axs[1].set_xlabel(x_info.name_and_unit)\n        axs[1].tick_params(labelleft=True)\n        axs[1].xaxis.set_tick_params(\n            which=\"both\", labelleft=True\n        )  # Redundant for safety\n\n    fig.tight_layout()\n    return fig, axs\n</code></pre>"},{"location":"API%20reference/utils/plot/#sqil_core.utils._plot.plot_projection_IQ","title":"<code>plot_projection_IQ(path=None, datadict=None, proj_data=None, full_output=False)</code>","text":"<p>Plots the real projection of complex I/Q data versus the x-axis and the full IQ plane.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the HDF5 file containing the data. Required if <code>datadict</code> is not provided.</p> <code>None</code> <code>datadict</code> <code>dict</code> <p>Pre-loaded data dictionary with schema, typically extracted using <code>extract_h5_data</code>. Required if <code>path</code> is not provided.</p> <code>None</code> <code>proj_data</code> <code>ndarray</code> <p>Precomputed projected data (real part of transformed complex values). If not provided, it will be computed using <code>transform_data</code>.</p> <code>None</code> <code>full_output</code> <code>bool</code> <p>Whether to return projected data and the inverse transformation function.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>res</code> <code>tuple</code> <p>If <code>full_output</code> is False:     (fig, [ax_proj, ax_iq]) If <code>full_output</code> is True:     (fig, [ax_proj, ax_iq], proj_data, inv) - <code>fig</code>: matplotlib Figure object. - <code>ax_proj</code>: Axis for projection vs x-axis. - <code>ax_iq</code>: Axis for I/Q scatter plot. - <code>proj_data</code>: The real projection of the complex I/Q data. - <code>inv</code>: The inverse transformation function used during projection.</p> Notes <p>This function supports only 1D datasets. If sweep dimensions are detected, no plot is created. The projection is performed using a data transformation routine (e.g., PCA or rotation).</p> Source code in <code>sqil_core/utils/_plot.py</code> <pre><code>def plot_projection_IQ(path=None, datadict=None, proj_data=None, full_output=False):\n    \"\"\"\n    Plots the real projection of complex I/Q data versus the x-axis and the full IQ plane.\n\n    Parameters\n    ----------\n    path : str, optional\n        Path to the HDF5 file containing the data. Required if `datadict` is not provided.\n    datadict : dict, optional\n        Pre-loaded data dictionary with schema, typically extracted using `extract_h5_data`.\n        Required if `path` is not provided.\n    proj_data : np.ndarray, optional\n        Precomputed projected data (real part of transformed complex values).\n        If not provided, it will be computed using `transform_data`.\n    full_output : bool, default False\n        Whether to return projected data and the inverse transformation function.\n\n    Returns\n    -------\n    res : tuple\n        If `full_output` is False:\n            (fig, [ax_proj, ax_iq])\n        If `full_output` is True:\n            (fig, [ax_proj, ax_iq], proj_data, inv)\n        - `fig`: matplotlib Figure object.\n        - `ax_proj`: Axis for projection vs x-axis.\n        - `ax_iq`: Axis for I/Q scatter plot.\n        - `proj_data`: The real projection of the complex I/Q data.\n        - `inv`: The inverse transformation function used during projection.\n\n    Notes\n    -----\n    This function supports only 1D datasets. If sweep dimensions are detected, no plot is created.\n    The projection is performed using a data transformation routine (e.g., PCA or rotation).\n    \"\"\"\n\n    all_data, all_info, _ = get_data_and_info(path=path, datadict=datadict)\n    x_data, y_data, sweeps = all_data\n    x_info, y_info, sweep_info = all_info\n\n    # Get y_unit\n    y_unit = f\" [{y_info.rescaled_unit}]\" if y_info.unit else \"\"\n\n    set_plot_style(plt)\n\n    if len(sweeps) == 0:\n        # Project data\n        if proj_data is None:\n            proj_data, inv = transform_data(y_data, inv_transform=True)\n\n        set_plot_style(plt)\n        fig = plt.figure(figsize=(20, 7), constrained_layout=True)\n        gs = GridSpec(nrows=1, ncols=10, figure=fig, wspace=0.2)\n\n        # Plot the projection\n        ax_proj = fig.add_subplot(gs[:, :6])  # 6/10 width\n        ax_proj.plot(x_data * x_info.scale, proj_data.real * y_info.scale, \"o\")\n        ax_proj.set_xlabel(x_info.name_and_unit)\n        ax_proj.set_ylabel(\"Projected\" + y_unit)\n\n        # Plot IQ data\n        ax_iq = fig.add_subplot(gs[:, 6:])  # 4/10 width\n        ax_iq.scatter(0, 0, marker=\"+\", color=\"black\", s=150)\n        ax_iq.plot(y_data.real * y_info.scale, y_data.imag * y_info.scale, \"o\")\n        ax_iq.set_xlabel(\"In-Phase\" + y_unit)\n        ax_iq.set_ylabel(\"Quadrature\" + y_unit)\n        ax_iq.set_aspect(aspect=\"equal\", adjustable=\"datalim\")\n\n    if full_output:\n        res = (fig, [ax_proj, ax_iq], proj_data, inv)\n    else:\n        res = (fig, [ax_proj, ax_iq])\n    return res\n</code></pre>"},{"location":"API%20reference/utils/plot/#sqil_core.utils._plot.reset_plot_style","title":"<code>reset_plot_style(plt)</code>","text":"<p>Resets the matplotlib plotting style to its default value.</p> Source code in <code>sqil_core/utils/_plot.py</code> <pre><code>def reset_plot_style(plt):\n    \"\"\"Resets the matplotlib plotting style to its default value.\"\"\"\n    return plt.rcParams.update(plt.rcParamsDefault)\n</code></pre>"},{"location":"API%20reference/utils/plot/#sqil_core.utils._plot.set_plot_style","title":"<code>set_plot_style(plt)</code>","text":"<p>Sets the matplotlib plotting style to a SQIL curated one.</p> Source code in <code>sqil_core/utils/_plot.py</code> <pre><code>def set_plot_style(plt):\n    \"\"\"Sets the matplotlib plotting style to a SQIL curated one.\"\"\"\n    style = {\n        \"font.size\": 20,\n        \"xtick.labelsize\": 18,  # X-axis tick labels\n        \"ytick.labelsize\": 18,  # Y-axis tick labels\n        \"lines.linewidth\": 2.5,  # Line width\n        # \"lines.marker\": \"o\",\n        \"lines.markersize\": 7,  # Marker size\n        \"lines.markeredgewidth\": 1.5,  # Marker line width\n        \"lines.markerfacecolor\": \"none\",\n        \"axes.grid\": True,\n        \"grid.linestyle\": \"--\",\n        \"xtick.major.size\": 8,\n        \"xtick.major.width\": 1.5,\n        \"ytick.major.size\": 8,\n        \"ytick.major.width\": 1.5,\n        \"figure.figsize\": (20, 7),\n    }\n    reset_plot_style(plt)\n    return plt.rcParams.update(style)\n</code></pre>"},{"location":"API%20reference/utils/read%20data/","title":"Read data","text":""},{"location":"API%20reference/utils/read%20data/#sqil_core.utils._read.extract_h5_data","title":"<code>extract_h5_data(path, keys=None, schema=False)</code>","text":"<p>Extract data at the given keys from an HDF5 file. If no keys are given (None) returns the data field of the object.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>path to the HDF5 file or a folder in which is contained a data.ddh5 file</p> required <code>keys</code> <code>None or List</code> <p>list of keys to extract from file['data'], by default None</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict or Tuple[ndarray, ...]</code> <p>The full data dictionary if keys = None. The tuple with the requested keys otherwise.</p> Example <pre><code>Extract the data object from the dataset:\n&gt;&gt;&gt; data = extract_h5_data(path)\nExtracting only 'amp' and 'phase' from the dataset:\n&gt;&gt;&gt; amp, phase = extract_h5_data(path, ['amp', 'phase'])\nExtracting only 'phase':\n&gt;&gt;&gt; phase, = extract_h5_data(path, ['phase'])\n</code></pre> Source code in <code>sqil_core/utils/_read.py</code> <pre><code>def extract_h5_data(\n    path: str, keys: list[str] | None = None, schema=False\n) -&gt; dict | tuple[np.ndarray, ...]:\n    \"\"\"Extract data at the given keys from an HDF5 file. If no keys are\n    given (None) returns the data field of the object.\n\n    Parameters\n    ----------\n    path : str\n        path to the HDF5 file or a folder in which is contained a data.ddh5 file\n    keys : None or List, optional\n        list of keys to extract from file['data'], by default None\n\n    Returns\n    -------\n    Dict or Tuple[np.ndarray, ...]\n        The full data dictionary if keys = None.\n        The tuple with the requested keys otherwise.\n\n    Example\n    -------\n        Extract the data object from the dataset:\n        &gt;&gt;&gt; data = extract_h5_data(path)\n        Extracting only 'amp' and 'phase' from the dataset:\n        &gt;&gt;&gt; amp, phase = extract_h5_data(path, ['amp', 'phase'])\n        Extracting only 'phase':\n        &gt;&gt;&gt; phase, = extract_h5_data(path, ['phase'])\n    \"\"\"\n    # If the path is to a folder open /data.ddh5\n    if os.path.isdir(path):\n        path = os.path.join(path, \"data.ddh5\")\n\n    with h5py.File(path, \"r\") as h5file:\n        data = h5file[\"data\"]\n        data_keys = data.keys()\n\n        db_schema = None\n        if schema:\n            db_schema = json.loads(data.attrs.get(\"__schema__\"))\n\n        # Extract only the requested keys\n        if bool(keys) and (len(keys) &gt; 0):\n            res = []\n            for key in keys:\n                key = str(key)\n                if (not bool(key)) | (key not in data_keys):\n                    res.append([])\n                    continue\n                res.append(np.array(data[key][:]))\n            if not schema and len(res) == 1:\n                return res[0]\n            return tuple(res) if not schema else (*tuple(res), db_schema)\n        # Extract the whole data dictionary\n        h5_dict = _h5_to_dict(data)\n        return h5_dict if not schema else {**h5_dict, \"schema\": db_schema}\n</code></pre>"},{"location":"API%20reference/utils/read%20data/#sqil_core.utils._read.extract_mapped_data","title":"<code>extract_mapped_data(path)</code>","text":"<p>Loads measurement data from an HDF5 file and maps it into x_data, y_data and sweeps. The map and the database schema on which it relies are also returned.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str or Path</code> <p>Path to the HDF5 file containing experimental data and schema definitions.</p> required <p>Returns:</p> Name Type Description <code>x_data</code> <code>ndarray</code> <p>Array of x-axis values extracted according to the schema.</p> <code>y_data</code> <code>ndarray</code> <p>Array of measured data values (y-axis).</p> <code>sweeps</code> <code>list[ndarray]</code> <p>List of arrays for any additional swept parameters defined in the schema.</p> <code>datadict_map</code> <code>dict</code> <p>Mapping of keys used for <code>\"x_data\"</code>, <code>\"y_data\"</code>, and <code>\"sweeps\"</code> in the original file.</p> <code>schema</code> <code>dict</code> <p>The schema used to interpret the data structure and field roles.</p> Notes <ul> <li>This function expects the file to contain a top-level \"schema\" key that defines the   role of each dataset (e.g., \"data\", \"x-axis\", \"axis\").</li> <li>Uses <code>extract_h5_data</code> and <code>map_data_dict</code> internally for loading and interpretation.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; x, y, sweeps, datadict_map, schema = extract_mapped_data(path)\n</code></pre> Source code in <code>sqil_core/utils/_read.py</code> <pre><code>def extract_mapped_data(path: str):\n    \"\"\"\n    Loads measurement data from an HDF5 file and maps it into x_data, y_data and sweeps.\n    The map and the database schema on which it relies are also returned.\n\n    Parameters\n    ----------\n    path : str or Path\n        Path to the HDF5 file containing experimental data and schema definitions.\n\n    Returns\n    -------\n    x_data : np.ndarray\n        Array of x-axis values extracted according to the schema.\n    y_data : np.ndarray\n        Array of measured data values (y-axis).\n    sweeps : list[np.ndarray]\n        List of arrays for any additional swept parameters defined in the schema.\n    datadict_map : dict\n        Mapping of keys used for `\"x_data\"`, `\"y_data\"`, and `\"sweeps\"` in the original file.\n    schema : dict\n        The schema used to interpret the data structure and field roles.\n\n    Notes\n    -----\n    - This function expects the file to contain a top-level \"schema\" key that defines the\n      role of each dataset (e.g., \"data\", \"x-axis\", \"axis\").\n    - Uses `extract_h5_data` and `map_data_dict` internally for loading and interpretation.\n\n    Examples\n    --------\n    &gt;&gt;&gt; x, y, sweeps, datadict_map, schema = extract_mapped_data(path)\n    \"\"\"\n\n    datadict = extract_h5_data(path, schema=True)\n    schema = datadict.get(\"schema\")\n    x_data, y_data, sweeps, datadict_map = map_data_dict(datadict)\n    return x_data, y_data, sweeps, datadict_map, schema\n</code></pre>"},{"location":"API%20reference/utils/read%20data/#sqil_core.utils._read.map_data_dict","title":"<code>map_data_dict(data_dict)</code>","text":"<p>Maps experimental data to standardized arrays using a provided schema.</p> <p>This function interprets the structure of a measurement data dictionary (obtained using extract_h5_data) by extracting relevant data fields according to roles specified in the database schema. It returns the x-axis values, y-axis data, any additional sweep parameters, and a mapping of keys used for each role.</p> <p>Parameters:</p> Name Type Description Default <code>data_dict</code> <code>dict</code> <p>Dictionary containing measurement data and an associated 'schema' key that defines the role of each field (e.g., \"x-axis\", \"data\", \"axis\").</p> required <p>Returns:</p> Name Type Description <code>x_data</code> <code>ndarray</code> <p>Array containing the x-axis values.</p> <code>y_data</code> <code>ndarray</code> <p>Array containing the y-axis (measured) data.</p> <code>sweeps</code> <code>list[ndarray]</code> <p>List of additional swept parameter arrays (if any).</p> <code>key_map</code> <code>dict</code> <p>Dictionary with keys <code>\"x_data\"</code>, <code>\"y_data\"</code>, and <code>\"sweeps\"</code> indicating the corresponding keys used in the original <code>data_dict</code>.</p> Notes <ul> <li>If the schema is missing, the function prints a warning and returns empty arrays.</li> <li>Each item in the schema must be a dictionary with a <code>\"role\"</code> key.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; x, y, sweeps, mapping = map_data_dict(experiment_data)\n&gt;&gt;&gt; print(f\"x-axis data from key: {mapping['x_data']}\")\n</code></pre> Source code in <code>sqil_core/utils/_read.py</code> <pre><code>def map_data_dict(data_dict: dict):\n    \"\"\"\n    Maps experimental data to standardized arrays using a provided schema.\n\n    This function interprets the structure of a measurement data dictionary\n    (obtained using extract_h5_data) by extracting relevant data fields according\n    to roles specified in the database schema. It returns the x-axis values, y-axis data,\n    any additional sweep parameters, and a mapping of keys used for each role.\n\n    Parameters\n    ----------\n    data_dict : dict\n        Dictionary containing measurement data and an associated 'schema' key\n        that defines the role of each field (e.g., \"x-axis\", \"data\", \"axis\").\n\n    Returns\n    -------\n    x_data : np.ndarray\n        Array containing the x-axis values.\n    y_data : np.ndarray\n        Array containing the y-axis (measured) data.\n    sweeps : list[np.ndarray]\n        List of additional swept parameter arrays (if any).\n    key_map : dict\n        Dictionary with keys `\"x_data\"`, `\"y_data\"`, and `\"sweeps\"` indicating\n        the corresponding keys used in the original `data_dict`.\n\n    Notes\n    -----\n    - If the schema is missing, the function prints a warning and returns empty arrays.\n    - Each item in the schema must be a dictionary with a `\"role\"` key.\n\n    Examples\n    --------\n    &gt;&gt;&gt; x, y, sweeps, mapping = map_data_dict(experiment_data)\n    &gt;&gt;&gt; print(f\"x-axis data from key: {mapping['x_data']}\")\n    \"\"\"\n\n    schema = data_dict.get(\"schema\", None)\n    if schema is None:\n        print(\n            \"Cannot automatically read data: no database schema was provided by the experiment.\"\n        )\n\n    x_data, y_data, sweeps = np.array([]), np.array([]), []\n    key_map = {\"x_data\": \"\", \"y_data\": \"\", \"sweeps\": []}\n\n    for key, value in schema.items():\n        if type(value) is not dict:\n            continue\n        role = value.get(\"role\", None)\n        if role == \"data\":\n            key_map[\"y_data\"] = key\n            y_data = data_dict[key]\n        elif role == \"x-axis\":\n            key_map[\"x_data\"] = key\n            x_data = data_dict[key]\n        elif role == \"axis\":\n            key_map[\"sweeps\"].append(key)\n            sweeps.append(data_dict[key])\n\n    return x_data, y_data, sweeps, key_map\n</code></pre>"},{"location":"API%20reference/utils/read%20data/#sqil_core.utils._read.read_json","title":"<code>read_json(path)</code>","text":"<p>Reads a json file and returns the data as a dictionary.</p> Source code in <code>sqil_core/utils/_read.py</code> <pre><code>def read_json(path: str) -&gt; dict:\n    \"\"\"Reads a json file and returns the data as a dictionary.\"\"\"\n    with open(path) as f:\n        dictionary = json.load(f)\n    return dictionary\n</code></pre>"},{"location":"API%20reference/utils/read%20data/#sqil_core.utils._read.read_qpu","title":"<code>read_qpu(dir_path, filename)</code>","text":"<p>Reads QPU file stored in dir_path/filename using laboneq serializers.</p> Source code in <code>sqil_core/utils/_read.py</code> <pre><code>def read_qpu(dir_path: str, filename: str) -&gt; QPU:\n    \"\"\"Reads QPU file stored in dir_path/filename using laboneq serializers.\"\"\"\n    qpu = serializers.load(os.path.join(dir_path, filename))\n    return qpu\n</code></pre>"},{"location":"Notebooks/exp_quick_start/","title":"Get started with","text":"<p>Clone the github repository</p> In\u00a0[\u00a0]: Copied! <pre>git clone https://github.com/SQIL-EPFL/sqil-experiments.git\n</pre> git clone https://github.com/SQIL-EPFL/sqil-experiments.git <p>Navigate to the <code>sqil-experiments</code> folder and install the dependencies using poetry. If you still haven't installed poetry, <code>pip install poetry poetry-plugin-shell</code></p> In\u00a0[\u00a0]: Copied! <pre>cd sqil-experiments\npoetry install\npoetry env activate\n</pre> cd sqil-experiments poetry install poetry env activate <p>Separately install Qt bindings, which are needed for plottr to work and cannot be packaged with the other dependencies.</p> In\u00a0[\u00a0]: Copied! <pre>poetry run pip install PyQt5\n</pre> poetry run pip install PyQt5 <p>Requirements for <code>sqil-experimental</code> to work:</p> <ul> <li>A <code>config.yaml</code> file</li> <li>A setup file</li> </ul> <p>The config file (<code>config.yaml</code>) it's located in the root directory of <code>sqil-experiments</code> and must NOT be deleted or renamed.</p> <p>It's currently used only to point to a setup file</p> setup_path: \"./setup/setup_test.py\"  <p>This allows you to easily switch between different setups, which could be useful if two experiments share the same measurement computer. For example, let's say that experiment 1 requires a Yoko to control flux and experiment 2 doesn't. Create a setup file for each experiment, <code>setup_1.py</code> and <code>setup_2.py</code> in which you specify all the instruments connected to the line. Then specify which setup file to use based on what you need to measure.</p> <p>The setup file controls all the things related to your experimental setup</p> <ol> <li>Choose where the experimental data is saved</li> <li>Define the Zuirch Instruments setup object</li> <li>Generate a QPU file when it's not available (e.g. first time running an experiment)</li> <li>Define the instruments used by your experiments</li> <li>Change the default behavior of the instruments</li> </ol> <p>IMPORTANT: The setup file is a python file, meaning that complex functionality can be defined and passed to the experiment.</p> <p><code>db_root</code> and <code>db_root_local</code> are used to define the respective remote and local database directories. <code>data_folder_name</code> is used to name the specific data collection folder.</p> <p>Data will be saved in the following directories:</p> <ul> <li><code>db_path_local/data_folder_name/</code></li> <li><code>db_path/data_folder_name/</code> We'll refer to these as 'data folders' or 'data paths'</li> </ul> <p>The QPU file will be saved in both data folders with name specified by <code>qpu_filename</code>.</p> In\u00a0[7]: Copied! <pre>import os\n\ndata_folder_name = \"test\"\n\n# Data storage\ndb_root = r\"C:\\Users\\sqil\\Desktop\\code\\sqil-experiments\\data\"\ndb_root_local = r\"C:\\Users\\sqil\\Desktop\\code\\sqil-experiments\\data_local\"\nstorage = {\n    \"db_type\": \"plottr\",\n    \"db_path\": os.path.join(db_root, data_folder_name),\n    \"db_path_local\": os.path.join(db_root_local, data_folder_name),\n    \"qpu_filename\": \"qpu.json\",\n}\n</pre> import os  data_folder_name = \"test\"  # Data storage db_root = r\"C:\\Users\\sqil\\Desktop\\code\\sqil-experiments\\data\" db_root_local = r\"C:\\Users\\sqil\\Desktop\\code\\sqil-experiments\\data_local\" storage = {     \"db_type\": \"plottr\",     \"db_path\": os.path.join(db_root, data_folder_name),     \"db_path_local\": os.path.join(db_root_local, data_folder_name),     \"qpu_filename\": \"qpu.json\", } <p>A function that specifies how to generate the Zurich Instruments setup object. It's called every time is needed to connect to Zurich Instruments devices. This method is not required if you're not using ZI.</p> <p>It's recmmended to construct this function using the <code>generate_device_setup</code> provided by LaboneQ.</p> In\u00a0[8]: Copied! <pre>from laboneq.contrib.example_helpers.generate_device_setup import generate_device_setup\n\ndef generate_zi_setup():\n    return generate_device_setup(\n        number_qubits=1,\n        shfqc=[\n            {\"serial\": \"dev12183\", \"number_of_channels\": 4, \"options\": \"SHFQC/QC4CH\"}\n        ],\n        include_flux_lines=False,\n        multiplex_drive_lines=True,\n        query_options=False,\n    )\n</pre> from laboneq.contrib.example_helpers.generate_device_setup import generate_device_setup  def generate_zi_setup():     return generate_device_setup(         number_qubits=1,         shfqc=[             {\"serial\": \"dev12183\", \"number_of_channels\": 4, \"options\": \"SHFQC/QC4CH\"}         ],         include_flux_lines=False,         multiplex_drive_lines=True,         query_options=False,     ) <p>The QPU, quantum processing unit, contains all the information about your qubits. The <code>generate_qpu</code> function gives the experiment instructions on how to generate the QPU file, in case one it's not already available. This means <code>generate_qpu</code> is supposed to run only once: when the first experiment is run.</p> <p>You can avoid defining this function if you already have a QPU file. For example if you're re-measuring the same qubit on the same setup, just copy the old QPU file into the new data folder (<code>db_path_local/data_folder_name/</code>)</p> <p>It's recommended to use the <code>from_device_setup</code> function if you have ZI in your setup.</p> <p>NOTE: A QPU is required even if you're not using any device from Zurich Instruments.</p> In\u00a0[9]: Copied! <pre>from helpers.sqil_transmon.operations import SqilTransmonOperations\nfrom helpers.sqil_transmon.qubit import SqilTransmon\nfrom laboneq.dsl.quantum import QPU\n\ndef generate_qpu(zi_setup):\n    qubits = SqilTransmon.from_device_setup(zi_setup)\n    quantum_operations = SqilTransmonOperations()\n    qpu = QPU(qubits, quantum_operations)\n\n    # Set required qubit parameters\n    for qubit in qpu.quantum_elements:\n        qubit.update(\n            **{\n                \"readout_lo_frequency\": 7e9,\n                \"drive_lo_frequency\": 5e9,\n            }\n        )\n    return qpu\n</pre> from helpers.sqil_transmon.operations import SqilTransmonOperations from helpers.sqil_transmon.qubit import SqilTransmon from laboneq.dsl.quantum import QPU  def generate_qpu(zi_setup):     qubits = SqilTransmon.from_device_setup(zi_setup)     quantum_operations = SqilTransmonOperations()     qpu = QPU(qubits, quantum_operations)      # Set required qubit parameters     for qubit in qpu.quantum_elements:         qubit.update(             **{                 \"readout_lo_frequency\": 7e9,                 \"drive_lo_frequency\": 5e9,             }         )     return qpu <p>It's a good idea t define here the LO frequencies for readout and drive. The QPU is generated with these two values empty, but not experiment can start without the LOs.</p> <p>The <code>instruments</code> dictionary contains all the information about your instruments. Every instrument needs a <code>type</code>, which is used by <code>sqil-core</code> to cast it to the correct class and control it properly. If there can be multiple models of the same instrument type, also a <code>model</code> is required.</p> <p>In the example below the experimental setup is made of our Zurich Instruments and an SGS used as external LO.</p> <p>Even if we created the <code>generate_setup</code> function for ZI earlier, now we need to bind it to the dictionary entry.</p> <p>For the SGS, we need to specify a <code>type</code> and a <code>model</code>, since we could use multiple instruments as LO sources. The <code>address</code> is required to connect to it, while the <code>name</code> is a human readable string used for logs.</p> <p>The <code>variables</code> field can be used by some instruments quickly access variables present in your experiment context. This is generally used to automatically allow sweeps on the instrument variables. With the old experimental code you would control the LO through the parameter \"ext_LO_freq\". The new way is more abstract and the LO can be controlled by any variable present in the experiment (qubit paramenters, experiment options, etc.). To bind a variable you need to specify a function that returns the value you want, given the experiment object.</p> In\u00a0[10]: Copied! <pre>instruments = {\n    \"zi\": {\n        \"type\": \"ZI\",\n        \"address\": \"localhost\",\n        \"generate_setup\": generate_zi_setup,\n        \"generate_qpu\": generate_qpu,\n    },\n    \"lo\": {\n        \"type\": \"LO\",\n        \"model\": \"RohdeSchwarzSGS100A\",\n        \"name\": \"SGSA100\",\n        \"address\": \"TCPIP0::192.168.1.56::inst0::INSTR\",\n        \"variables\": {\n            \"frequency\": lambda exp: exp.qpu.quantum_elements[0].parameters.external_lo_frequency,\n            \"power\": lambda exp: exp.qpu.quantum_elements[0].parameters.external_lo_power,\n        },\n    },\n}\n</pre> instruments = {     \"zi\": {         \"type\": \"ZI\",         \"address\": \"localhost\",         \"generate_setup\": generate_zi_setup,         \"generate_qpu\": generate_qpu,     },     \"lo\": {         \"type\": \"LO\",         \"model\": \"RohdeSchwarzSGS100A\",         \"name\": \"SGSA100\",         \"address\": \"TCPIP0::192.168.1.56::inst0::INSTR\",         \"variables\": {             \"frequency\": lambda exp: exp.qpu.quantum_elements[0].parameters.external_lo_frequency,             \"power\": lambda exp: exp.qpu.quantum_elements[0].parameters.external_lo_power,         },     }, } <p>In the dictionary defining the instruments for your experiments, the key lo represents the variable name used to control the Local Oscillator (LO). In this case, the dictionary key \"lo\" is associated with the SGS, and when writing your experiment code, you interact with it as if it\u2019s a generic LO object\u2014regardless of its specific model. For example, you would call <code>lo.set_frequency(11e9)</code> to set the frequency, no matter whether you're using an SGS or a different LO.</p> <p>The reason this is useful is that the sqil-core framework generalizes all LOs, abstracting away the specific details of the underlying hardware. This means you don\u2019t need to worry about treating the SGS as an SGS. Instead, you just treat it as an abstract LO object that you can control in a standardized way.</p> <p>If, in the future, you decide to switch to a different LO source, e.g. a Signal Core, you don\u2019t have to go through and modify every experiment script. You simply update the dictionary by changing the lo entry to reflect the new model and name, and your experiment code will continue working exactly the same way without needing any further adjustments.</p> <p>Some instruments have a default behavior, like turning on before the experiment and turning off after the experiment. You can change these behaviors or add new ones by overriding the instrument functions.</p> <ul> <li><code>connect</code>: how the experiment should connect to the instrument</li> <li><code>setup</code>: how to setup the instrument after it's connected (e.g. turn on phase locking or set a specific power)</li> <li><code>before_experiment</code>: function that runs before the experiment starts</li> <li><code>before_sequence</code>: function that runs just before the pulse sequence is sent</li> <li><code>after_sequence</code>:  function that runs just after the pulse sequence is sent</li> <li><code>before_experiment</code>: function that runs after the experiment ends</li> </ul> <p>The difference between <code>before_experiment</code> and <code>before_sequence</code> is that the first one runs only once, while the second one runs every time the pulse sequence is sent and can be used to handle sweeps.</p> <p>NOTE: when overriding these functions you are overriding the instrument class default for that function, which means you have access to the <code>self</code> argument and can access class attributes, like <code>name</code>, <code>address</code>, etc., and even the instrument's functions, like <code>turn_on</code>, <code>set_frequency</code>, etc.</p> <p>NOTE: most instruments have default behaviors for these functions, before overriding them check what they're doing, because they may be handling something you didn't think about. Like safely handling connections or forcing some useful behaviors by default, like turning on phase locking.</p> In\u00a0[\u00a0]: Copied! <pre>def lo_after_experiment(self, *args, **kwargs):\n    print(\"Setting low power and turning off {self.name}\")\n    self.set_power(-60)\n    self.turn_off()\n\ninstruments = {\n    \"zi\": {\n        \"type\": \"ZI\",\n        \"address\": \"localhost\",\n        \"generate_setup\": generate_zi_setup,\n        \"generate_qpu\": generate_qpu,\n    },\n    \"lo\": {\n        \"type\": \"LO\",\n        \"model\": \"RohdeSchwarzSGS100A\",\n        \"name\": \"SGSA100\",\n        \"address\": \"TCPIP0::192.168.1.56::inst0::INSTR\",\n        \"variables\": {\n            \"frequency\": lambda exp: (\n                exp.qpu.quantum_elements[0].parameters.external_lo_frequency\n            ),\n            \"power\": lambda exp: (\n                exp.qpu.quantum_elements[0].parameters.external_lo_power\n            ),\n        },\n        # Bind the new function to the instrument\n        \"after_experiment\": lo_after_experiment,\n    },\n}\n</pre> def lo_after_experiment(self, *args, **kwargs):     print(\"Setting low power and turning off {self.name}\")     self.set_power(-60)     self.turn_off()  instruments = {     \"zi\": {         \"type\": \"ZI\",         \"address\": \"localhost\",         \"generate_setup\": generate_zi_setup,         \"generate_qpu\": generate_qpu,     },     \"lo\": {         \"type\": \"LO\",         \"model\": \"RohdeSchwarzSGS100A\",         \"name\": \"SGSA100\",         \"address\": \"TCPIP0::192.168.1.56::inst0::INSTR\",         \"variables\": {             \"frequency\": lambda exp: (                 exp.qpu.quantum_elements[0].parameters.external_lo_frequency             ),             \"power\": lambda exp: (                 exp.qpu.quantum_elements[0].parameters.external_lo_power             ),         },         # Bind the new function to the instrument         \"after_experiment\": lo_after_experiment,     }, } In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nfrom time_rabi import TimeRabi, TimeRabiOptions\n\ntime_rabi = TimeRabi()\noptions = TimeRabiOptions()\noptions.count = 2**8\n\npulse_lengths = np.linspace(1e-10,600e-9, 53)\n\nresult = time_rabi.run(pulse_lengths, options=options)\n</pre> import numpy as np from time_rabi import TimeRabi, TimeRabiOptions  time_rabi = TimeRabi() options = TimeRabiOptions() options.count = 2**8  pulse_lengths = np.linspace(1e-10,600e-9, 53)  result = time_rabi.run(pulse_lengths, options=options)"},{"location":"Notebooks/exp_quick_start/#get-started-with-sqil-experiments","title":"Get started with <code>sqil-experiments</code>\u00b6","text":""},{"location":"Notebooks/exp_quick_start/#installation","title":"Installation\u00b6","text":""},{"location":"Notebooks/exp_quick_start/#1-the-config-file","title":"1. The config file\u00b6","text":""},{"location":"Notebooks/exp_quick_start/#2-the-setup-file","title":"2. The setup file\u00b6","text":""},{"location":"Notebooks/exp_quick_start/#21-data-storage","title":"2.1. Data storage\u00b6","text":""},{"location":"Notebooks/exp_quick_start/#22-zuirch-instruments-setup","title":"2.2. Zuirch Instruments setup\u00b6","text":""},{"location":"Notebooks/exp_quick_start/#23-generate-qpu","title":"2.3. Generate QPU\u00b6","text":""},{"location":"Notebooks/exp_quick_start/#24-define-the-instruments","title":"2.4. Define the instruments\u00b6","text":""},{"location":"Notebooks/exp_quick_start/#25-change-instrument-behavior","title":"2.5. Change instrument behavior\u00b6","text":""},{"location":"Notebooks/exp_quick_start/#3-run-an-experiment","title":"3. Run an experiment\u00b6","text":""},{"location":"Notebooks/fit_input_decorator/","title":"The @fit_input decorator","text":"<p>The <code>@fit_input</code> decorator makes it easy to define bounds for the fit function in a more readable way. It also lets users include a <code>fixed_params</code> argument in any fit function it decorates, even if that function doesn\u2019t normally support <code>fixed_params</code>.</p> <p>As the name suggests <code>fixed_params</code> allows users to define which parameters in their initial guess should not be modified during the optimization. This is achieved by setting tight bounds around the fixed parameters, +/- param_value / tolerance. The tolerance is by default 1e-6, but it can be set using the <code>fixed_bound_factor</code> argument.</p> <p>IMPORTANT: This decorator requires the x and y input vectors to be named <code>x_data</code> and <code>y_data</code>. The initial guess must be called <code>guess</code> and the bounds <code>bounds</code>.</p> <p>IMPORTANT: Note that scipy doesn't allow <code>bounds = None</code>. If you're writing a fit function with the <code>@fit_input</code> decorator and don't want to force the user to pass bounds every time, you should be mindful of that. To solve this issue you can write your own logic to handle invalid bounds, or set the bounds input argument to have a default of <code>bounds = (-np.inf, np.inf)</code>, which will leave ALL parameters free.</p> In\u00a0[1]: Copied! <pre>import numpy as np\n\n# Define the Lorentzian function\ndef lorentzian(x, A, x0, w):\n    return (A / np.pi) * (w / ((x - x0)**2 + w**2))\n\n# Generate synthetic data\ntrue_params = [1, 0, 2]  # A=1, x0=0, w=2\nnp.random.seed(11)\nx_data = np.linspace(-10, 10, 100)\nnoise = 0.01 * np.random.normal(size=len(x_data))\ny_data = lorentzian(x_data, *true_params) + noise  # Add noise\n</pre> import numpy as np  # Define the Lorentzian function def lorentzian(x, A, x0, w):     return (A / np.pi) * (w / ((x - x0)**2 + w**2))  # Generate synthetic data true_params = [1, 0, 2]  # A=1, x0=0, w=2 np.random.seed(11) x_data = np.linspace(-10, 10, 100) noise = 0.01 * np.random.normal(size=len(x_data)) y_data = lorentzian(x_data, *true_params) + noise  # Add noise <p>Then we write a fit function that supports bounds and apply the <code>fit_input</code> decorator, remembering to give the correct names to the input arguments.</p> In\u00a0[2]: Copied! <pre>from scipy.optimize import curve_fit\nfrom sqil_core.fit import fit_input\n\n@fit_input\ndef fit_lorentzian(x_data, y_data, guess=None, bounds=(-np.inf, np.inf)):\n    return curve_fit(lorentzian, x_data, y_data, p0=guess, bounds=bounds)\n</pre> from scipy.optimize import curve_fit from sqil_core.fit import fit_input  @fit_input def fit_lorentzian(x_data, y_data, guess=None, bounds=(-np.inf, np.inf)):     return curve_fit(lorentzian, x_data, y_data, p0=guess, bounds=bounds) In\u00a0[3]: Copied! <pre># No guess and no bounds\nres = fit_lorentzian(x_data, y_data)\nprint(\"Optimized parameters\\tNo guess/bounds\\t\", res[0])\n</pre> # No guess and no bounds res = fit_lorentzian(x_data, y_data) print(\"Optimized parameters\\tNo guess/bounds\\t\", res[0]) <pre>Optimized parameters\tNo guess/bounds\t [1.00732229 0.04351166 2.01522901]\n</pre> <p>The initial guess is just an array of parameters, so you need to be careful about the order. The order in which you put the parameters in the initial guess must be the same as the one used by your model function.</p> <p>In our example, the parameter order must be the same as the one used by <code>lorentzian(x, A, x0, w)</code>, so <code>[A, x0, w]</code>.</p> In\u00a0[4]: Copied! <pre># Guess   A   x0   w\nguess = [0.5, 0.5, 1]\n\n# Only guess\nres = fit_lorentzian(x_data, y_data, guess=guess)\nprint(\"Optimized parameters\\tOnly guess\\t\",res[0])\n</pre> # Guess   A   x0   w guess = [0.5, 0.5, 1]  # Only guess res = fit_lorentzian(x_data, y_data, guess=guess) print(\"Optimized parameters\\tOnly guess\\t\",res[0]) <pre>Optimized parameters\tOnly guess\t [1.0073222  0.04351161 2.01522867]\n</pre> <p>The parameter order might not at all be obvious. If you're writing a fitting function <code>fit_lorentzian</code> the users cannot see your <code>lorentzian</code> model function. So it's HIGHLY recommended that you write down the parameter order in the fit function's docstring, like so</p> In\u00a0[5]: Copied! <pre>@fit_input\ndef fit_lorentzian(x_data, y_data, guess=None, bounds=(-np.inf, np.inf)):\n    \"\"\"Function to fit lorentzians :)\n\n    Parameters\n    ----------\n    x_data : np.ndarray\n        The independent variable\n    y_data : np.ndarray\n        The dependent variable\n    guess : list, optional\n        The initial guess for the parameters [A, x0, w], by default None\n    bounds : list[tuple] | tuple, optional\n        The bounds for the optimization in the form [(min, max), ...], by default (-np.inf, np.inf)\n\n    Returns\n    -------\n    tuple\n        popt, pcov\n    \"\"\"\n    return curve_fit(lorentzian, x_data, y_data, p0=guess, bounds=bounds)\n</pre> @fit_input def fit_lorentzian(x_data, y_data, guess=None, bounds=(-np.inf, np.inf)):     \"\"\"Function to fit lorentzians :)      Parameters     ----------     x_data : np.ndarray         The independent variable     y_data : np.ndarray         The dependent variable     guess : list, optional         The initial guess for the parameters [A, x0, w], by default None     bounds : list[tuple] | tuple, optional         The bounds for the optimization in the form [(min, max), ...], by default (-np.inf, np.inf)      Returns     -------     tuple         popt, pcov     \"\"\"     return curve_fit(lorentzian, x_data, y_data, p0=guess, bounds=bounds) <p>Note how in the guess line the parameter order is specified. To write the docstring template automatically you can download the autoDosctring VS Code extension, then type \"\"\" (right below your function's definition) followed by TAB.</p> <p>Bounds must be given in an array of tuples, following the same order as the guess array.</p> In\u00a0[6]: Copied! <pre># Bounds    A       x0        w\nbounds = [(0,2), (-3, 1), (0.7, 2.1)]\n\n# Only bounds\nres = fit_lorentzian(x_data, y_data, bounds=bounds)\nprint(\"Optimized parameters\\tOnly bounds\\t\",res[0])\n</pre> # Bounds    A       x0        w bounds = [(0,2), (-3, 1), (0.7, 2.1)]  # Only bounds res = fit_lorentzian(x_data, y_data, bounds=bounds) print(\"Optimized parameters\\tOnly bounds\\t\",res[0]) <pre>Optimized parameters\tOnly bounds\t [1.0073222  0.0435117  2.01522867]\n</pre> <p>The <code>fit_lorentzian</code> function doesn't allow for a <code>fixed_params</code> argument directly, but it inherits it from the <code>@fit_input</code> decorator. So, even if it's not present in the function definition, you can still pass it.</p> <p><code>fixed_params</code> allows users to define which parameters should not be optimized. To be able to fix the values an initial guess must be provided.</p> <p>It must be passed as an array of indices. These indices are relative to the initial guess. So, for example, if we wanted to fix the amplitude A of the lorentzian, we would set <code>fixed_params=[0]</code></p> In\u00a0[7]: Copied! <pre># Guess   A   x0   w\nguess = [0.5, 0.5, 1]\n\n# Fix A to its initial value\nfixed_params = [0]\n\n# Fit with fixed A\nres = fit_lorentzian(x_data, y_data, guess=guess, fixed_params=fixed_params)\nprint(\"Optimized parameters\\tFixed A = 0.5\\t\",res[0])\n</pre> # Guess   A   x0   w guess = [0.5, 0.5, 1]  # Fix A to its initial value fixed_params = [0]  # Fit with fixed A res = fit_lorentzian(x_data, y_data, guess=guess, fixed_params=fixed_params) print(\"Optimized parameters\\tFixed A = 0.5\\t\",res[0]) <pre>Optimized parameters\tFixed A = 0.5\t [0.5000005  0.08076683 1.09458635]\n</pre> <p>See how the value of A remained fixed, up to a relative precision of 1e-6.</p> <p>See how in the definition of <code>fit_lorentzian</code> the default value for <code>bounds</code> is <code>(-np.inf, np.inf)</code>. If <code>bounds</code> was <code>None</code> by default and the user forgot to set the bounds manually, we would get a scipy error, since <code>None</code> bounds are not allowed.</p> In\u00a0[8]: Copied! <pre>@fit_input\ndef fit_lorentzian(x_data, y_data, guess=None, bounds=None):\n    return curve_fit(lorentzian, x_data, y_data, p0=guess, bounds=bounds)\n\ntry:\n    fit_lorentzian(x_data, y_data)\nexcept Exception as err:\n    print(f\"ERROR: {err}\")\n</pre> @fit_input def fit_lorentzian(x_data, y_data, guess=None, bounds=None):     return curve_fit(lorentzian, x_data, y_data, p0=guess, bounds=bounds)  try:     fit_lorentzian(x_data, y_data) except Exception as err:     print(f\"ERROR: {err}\") <pre>ERROR: 'NoneType' object is not iterable\n</pre>"},{"location":"Notebooks/fit_input_decorator/#the-fit_input-decorator","title":"The @fit_input decorator\u00b6","text":""},{"location":"Notebooks/fit_input_decorator/#lorentzian-fit-example","title":"Lorentzian fit example\u00b6","text":"<p>Let's define the lorentzian function and create some synthetic data for our example</p>"},{"location":"Notebooks/fit_input_decorator/#fitting-without-initial-guess-and-without-bounds","title":"Fitting without initial guess and without bounds\u00b6","text":""},{"location":"Notebooks/fit_input_decorator/#fitting-with-an-inital-guess","title":"Fitting with an inital guess\u00b6","text":""},{"location":"Notebooks/fit_input_decorator/#fitting-with-bounds","title":"Fitting with bounds\u00b6","text":""},{"location":"Notebooks/fit_input_decorator/#fixing-parameters","title":"Fixing parameters\u00b6","text":""},{"location":"Notebooks/fit_input_decorator/#important-notes","title":"Important notes\u00b6","text":""},{"location":"Notebooks/fit_output_decorator/","title":"The @fit_output decorator","text":"<p>The <code>@fit_output</code> decorator ensures a consistent output format across different optimization methods and libraries, making it easier to work with fit results in a standardized way.</p> <p>If you're new to decorators, think of them as a way to extend a function\u2019s behavior without modifying its core logic. In this case, <code>@fit_output</code> enhances fit functions by computing additional metrics and providing visualization tools automatically.</p> <p>This is possible because the decorator can recognize and distinguish the outputs of the most used <code>scipy.optimize</code> and <code>lmfit</code> functions. Once the library and fit method has been recognized, <code>@fit_output</code> computes missing metrics and rearranges the output in a standardized format.</p> <p>The output of all the fit functions that use the decorator is a FitResult, which gives you access to useful information, such as:</p> <ul> <li><code>.params</code>, the optimized parameters</li> <li><code>.std_err</code>, the standard error on the parameters</li> <li><code>.metrics</code>, metrics used to evaluate the goodness of the fit</li> <li><code>.output</code>, the raw output of the fit function</li> <li><code>.predict</code>, a function of only x, used to make predictions using the fitted parameters</li> <li><code>.metadata</code>, additional information</li> </ul> <p>In some cases not all of this information can be extracted from the raw fit output, especially the prediction function. If that's the case, you can pass these parameters manually, as we'll see in the following examples.</p> <p>Note: it is important to note that <code>@fi_output</code> decorator computes the standard error by adjusting the covariance matrix by the reduced chi squared, which can lead to better results. <code>lmfit</code> automatically performs this procedure, so results will be consistend. <code>scipy</code> doesn't automatically perform it, which means that manually calculating the standard error from the <code>scipy</code> covariance matrix will give a different result. If however the experimental error is given to the optimizer, <code>scipy</code> will also automatically rescale the errors.</p> <p><code>@fit_output</code> guarantees that the standard errors have been rescaled in all cases.</p> In\u00a0[1]: Copied! <pre>import numpy as np\n\n# Define the Lorentzian function\ndef lorentzian(x, A, x0, w):\n    return (A / np.pi) * (w / ((x - x0)**2 + w**2))\n\n# Generate synthetic data\ntrue_params = [1, 0, 2]  # A=1, x0=0, w=2\nnp.random.seed(11)\nx_data = np.linspace(-10, 10, 100)\nnoise = 0.01 * np.random.normal(size=len(x_data))\ny_data = lorentzian(x_data, *true_params) + noise  # Add noise\n</pre> import numpy as np  # Define the Lorentzian function def lorentzian(x, A, x0, w):     return (A / np.pi) * (w / ((x - x0)**2 + w**2))  # Generate synthetic data true_params = [1, 0, 2]  # A=1, x0=0, w=2 np.random.seed(11) x_data = np.linspace(-10, 10, 100) noise = 0.01 * np.random.normal(size=len(x_data)) y_data = lorentzian(x_data, *true_params) + noise  # Add noise <p>Let's look at an example of a lorentzian fit using <code>scipy</code>'s <code>curve_fit</code>.</p> <p>Step 1: we perform the optimization using <code>curve_fit</code></p> In\u00a0[2]: Copied! <pre>from scipy.optimize import curve_fit\n\n# Fit the data using curve_fit\npopt, pcov = curve_fit(lorentzian, x_data, y_data, p0=[1, 0, 1])\n</pre> from scipy.optimize import curve_fit  # Fit the data using curve_fit popt, pcov = curve_fit(lorentzian, x_data, y_data, p0=[1, 0, 1]) <p>Step 2: extract the parameters and compute the chi-squared and the standard errors to evaluate the goodness of the fit</p> In\u00a0[3]: Copied! <pre># Extract fitted parameters\nA_fit, x0_fit, w_fit = popt\nprint(f\"Fitted parameters: A={A_fit:.3f}, x0={x0_fit:.3f}, w={w_fit:.3f}\")\n\n# Extract standard errors (diagonal of the covariance matrix)\nperr = np.sqrt(np.diag(pcov))\nprint(f\"Standard errors: A={perr[0]:.3f}, x0={perr[1]:.3f}, w={perr[2]:.3f}\")\n\n# Compute the chi-squared value\ny_fit = lorentzian(x_data, *popt)  # Fitted curve\nresiduals = y_data - y_fit\nchi_squared = np.sum((residuals)**2)\nprint(f\"Chi-squared: {chi_squared:.3f}\")\n</pre> # Extract fitted parameters A_fit, x0_fit, w_fit = popt print(f\"Fitted parameters: A={A_fit:.3f}, x0={x0_fit:.3f}, w={w_fit:.3f}\")  # Extract standard errors (diagonal of the covariance matrix) perr = np.sqrt(np.diag(pcov)) print(f\"Standard errors: A={perr[0]:.3f}, x0={perr[1]:.3f}, w={perr[2]:.3f}\")  # Compute the chi-squared value y_fit = lorentzian(x_data, *popt)  # Fitted curve residuals = y_data - y_fit chi_squared = np.sum((residuals)**2) print(f\"Chi-squared: {chi_squared:.3f}\") <pre>Fitted parameters: A=1.007, x0=0.044, w=2.015\nStandard errors: A=0.022, x0=0.043, w=0.061\nChi-squared: 0.009\n</pre> <p>While this is a valid way of performing the fit, there are a few issues.</p> <ul> <li>There is no immediate way of assessing the fit quality</li> <li>While we have the covariance matrix, the standard errors are not immediately available</li> <li>There is no easy way of printing the parameters and their respective errors</li> </ul> <p>Step 1: create the fit function and apply the decorator</p> In\u00a0[4]: Copied! <pre>from sqil_core.fit import fit_output\n\n@fit_output\ndef fit_lorentzian(x_data, y_data):\n    result = curve_fit(lorentzian, x_data, y_data, p0=[1, 0, 1], full_output=True)\n    return result\n</pre> from sqil_core.fit import fit_output  @fit_output def fit_lorentzian(x_data, y_data):     result = curve_fit(lorentzian, x_data, y_data, p0=[1, 0, 1], full_output=True)     return result <p>Step 2: run the optimization and print the summary</p> In\u00a0[5]: Copied! <pre>fit_result = fit_lorentzian(x_data, y_data)\n\nfit_result.summary()\n</pre> fit_result = fit_lorentzian(x_data, y_data)  fit_result.summary() <pre>reduced \u03c7\u00b2\t8.869e-05\tGREAT (or overfitting)\n|   Param |   Fitted value |   STD error |   % Error |\n|---------|----------------|-------------|-----------|\n|       0 |      1.00732   |   0.0002031 |      0.02 |\n|       1 |      0.0435118 |   0.0004014 |      0.92 |\n|       2 |      2.01523   |   0.0005755 |      0.03 |\n\n</pre> <p>The result is much easier to read. <code>fit_result</code> is now a <code>FitResult</code> object, which contains even more information. You can read more about it here. From <code>fit_result</code> we can access the following properties:</p> In\u00a0[6]: Copied! <pre># Prints a summary of the fit, including parameters values, their error\n# and metrics to evaluate the goodness of the fit.\nfit_result.summary#()\n\n# The parameters as a numpy array\nfit_result.params\n\n# The standard errror on the parameters as a numpy array\nfit_result.std_err\n\n# Fit metrics (like reduced chi-squared)\nfit_result.metrics\n\n# The raw output of the optimization, in this case it's the\n# output of scipy curve_fit\nfit_result.output\n\n# Names of the parameters (not always available)\nfit_result.param_names\n\n# Predict y given x, with the current optimized parameters (not always available)\nfit_result.predict#(x_fit)\n;\n</pre> # Prints a summary of the fit, including parameters values, their error # and metrics to evaluate the goodness of the fit. fit_result.summary#()  # The parameters as a numpy array fit_result.params  # The standard errror on the parameters as a numpy array fit_result.std_err  # Fit metrics (like reduced chi-squared) fit_result.metrics  # The raw output of the optimization, in this case it's the # output of scipy curve_fit fit_result.output  # Names of the parameters (not always available) fit_result.param_names  # Predict y given x, with the current optimized parameters (not always available) fit_result.predict#(x_fit) ; Out[6]: <pre>''</pre> <p>You may have noticed that some attributes are not always available, that's because they can't be guessed from the optimizer's output. However, you can still provide them to the decorator by returning a tuple in your fit function, instead of just the optimizer's output. The tuple should contain a second element, <code>metadata</code>, which is a dictionary used to provide additional information.</p> <p>In the example below we manually add the parameter's names, this way the <code>summary()</code> becomes easier to read.</p> In\u00a0[7]: Copied! <pre>@fit_output\ndef fit_lorentzian(x_data, y_data):\n    result = curve_fit(lorentzian, x_data, y_data, p0=[1, 0, 1], full_output=True)\n    metadata = {\n        \"param_names\": [\"A\", \"x0\", \"w\"]\n    }\n    return result, metadata\n\nfit_result = fit_lorentzian(x_data, y_data)\nfit_result.summary()\n</pre> @fit_output def fit_lorentzian(x_data, y_data):     result = curve_fit(lorentzian, x_data, y_data, p0=[1, 0, 1], full_output=True)     metadata = {         \"param_names\": [\"A\", \"x0\", \"w\"]     }     return result, metadata  fit_result = fit_lorentzian(x_data, y_data) fit_result.summary() <pre>reduced \u03c7\u00b2\t8.869e-05\tGREAT (or overfitting)\n| Param   |   Fitted value |   STD error |   % Error |\n|---------|----------------|-------------|-----------|\n| A       |      1.00732   |   0.0002031 |      0.02 |\n| x0      |      0.0435118 |   0.0004014 |      0.92 |\n| w       |      2.01523   |   0.0005755 |      0.03 |\n\n</pre> <p>Step 3: vistualize the results</p> <p>In the metadata dictionary we can also pass a function that can be used to evaluate the fit on a given x vector. In our case it's the <code>lorenzian</code> function. The decorator takes care of passing the right parameters into this prediction function, which will depend only on x.</p> In\u00a0[8]: Copied! <pre>@fit_output\ndef fit_lorentzian(x_data, y_data):\n    result = curve_fit(lorentzian, x_data, y_data, p0=[1, 0, 1], full_output=True)\n    metadata = {\n        \"param_names\": [\"A\", \"x0\", \"w\"],\n        \"predict\": lorentzian\n    }\n    return result, metadata\n\n# Perform the fit\nfit_result = fit_lorentzian(x_data, y_data)\n\n# Compute fitted curve\nx_fit = np.linspace(-10, 10, 200)\ny_fit = fit_result.predict(x_fit)\n\n# Plot result\nimport matplotlib.pyplot as plt\nplt.scatter(x_data, y_data, label=\"Noisy Data\", color=\"black\", s=10)\nplt.plot(x_fit, y_fit, label=\"Lorentzian Fit\", color=\"red\", linewidth=2)\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.title(\"Lorentzian Curve Fitting\")\nplt.legend()\nplt.show()\n</pre> @fit_output def fit_lorentzian(x_data, y_data):     result = curve_fit(lorentzian, x_data, y_data, p0=[1, 0, 1], full_output=True)     metadata = {         \"param_names\": [\"A\", \"x0\", \"w\"],         \"predict\": lorentzian     }     return result, metadata  # Perform the fit fit_result = fit_lorentzian(x_data, y_data)  # Compute fitted curve x_fit = np.linspace(-10, 10, 200) y_fit = fit_result.predict(x_fit)  # Plot result import matplotlib.pyplot as plt plt.scatter(x_data, y_data, label=\"Noisy Data\", color=\"black\", s=10) plt.plot(x_fit, y_fit, label=\"Lorentzian Fit\", color=\"red\", linewidth=2) plt.xlabel(\"x\") plt.ylabel(\"y\") plt.title(\"Lorentzian Curve Fitting\") plt.legend() plt.show() <p>The power of the decorator lies also in its ability to standardize output from libraries or methods. For example, the output given by scipy <code>minimize</code> is different from the output of scipy <code>curve_fit</code>, so in order to compute fit metrics, standard errors, etc. different functions must be used. However, applying the decorator to both <code>minimize</code> and <code>curve_fit</code> produces the same output.</p> <p>Minimize without decorator</p> In\u00a0[9]: Copied! <pre>from scipy.optimize import minimize\n\n# Define the cost function to minimize (sum of squared residuals)\ndef cost_function(params, x, y):\n    A, x0, w = params\n    y_model = lorentzian(x, A, x0, w)\n    return np.sum((y - y_model) ** 2)\n\n# Use scipy.optimize.minimize to fit the data\nresult = minimize(cost_function, [1,0,1], args=(x_data, y_data), method='BFGS')\n\n# Print parameters\nA_fit, x0_fit, w_fit = result.x\nprint(f\"Fitted parameters: A={A_fit:.3f}, x0={x0_fit:.3f}, w={w_fit:.3f}\")\n\n# Compute chi-squared\ny_fit = lorentzian(x_data, *result.x)\nresiduals = y_data - y_fit\nchi_squared = np.sum((residuals) ** 2)\nprint(f\"Chi-squared: {chi_squared:.3f}\")\n\n# Approximate the covariance with the inverse hessian\nhessian_inv = result.hess_inv\n# Compute standard errors\nstandard_errors = np.sqrt(np.diagonal(hessian_inv))\nprint(f\"Standard errors: A={standard_errors[0]:.3f}, x0={standard_errors[1]:.3f}, w={standard_errors[2]:.3f}\")\n</pre> from scipy.optimize import minimize  # Define the cost function to minimize (sum of squared residuals) def cost_function(params, x, y):     A, x0, w = params     y_model = lorentzian(x, A, x0, w)     return np.sum((y - y_model) ** 2)  # Use scipy.optimize.minimize to fit the data result = minimize(cost_function, [1,0,1], args=(x_data, y_data), method='BFGS')  # Print parameters A_fit, x0_fit, w_fit = result.x print(f\"Fitted parameters: A={A_fit:.3f}, x0={x0_fit:.3f}, w={w_fit:.3f}\")  # Compute chi-squared y_fit = lorentzian(x_data, *result.x) residuals = y_data - y_fit chi_squared = np.sum((residuals) ** 2) print(f\"Chi-squared: {chi_squared:.3f}\")  # Approximate the covariance with the inverse hessian hessian_inv = result.hess_inv # Compute standard errors standard_errors = np.sqrt(np.diagonal(hessian_inv)) print(f\"Standard errors: A={standard_errors[0]:.3f}, x0={standard_errors[1]:.3f}, w={standard_errors[2]:.3f}\") <pre>Fitted parameters: A=1.007, x0=0.044, w=2.015\nChi-squared: 0.009\nStandard errors: A=1.521, x0=3.044, w=3.974\n</pre> <p>Minimize with decorator</p> In\u00a0[10]: Copied! <pre>@fit_output\ndef minimize_lorentzian(x_data, y_data):\n    result = minimize(cost_function, [1,0,1], args=(x_data, y_data), method='BFGS')\n    metadata = {\n        \"param_names\": [\"A\", \"x0\", \"w\"],\n        \"predict\": lorentzian\n    }\n    return result, metadata\n\nfit_result = minimize_lorentzian(x_data, y_data)\nfit_result.summary()\n</pre> @fit_output def minimize_lorentzian(x_data, y_data):     result = minimize(cost_function, [1,0,1], args=(x_data, y_data), method='BFGS')     metadata = {         \"param_names\": [\"A\", \"x0\", \"w\"],         \"predict\": lorentzian     }     return result, metadata  fit_result = minimize_lorentzian(x_data, y_data) fit_result.summary() <pre>reduced \u03c7\u00b2\t8.869e-05\tGREAT (or overfitting)\n| Param   |   Fitted value |   STD error |   % Error |\n|---------|----------------|-------------|-----------|\n| A       |      1.00732   |     0.01433 |      1.42 |\n| x0      |      0.0435014 |     0.02867 |     65.9  |\n| w       |      2.01521   |     0.03742 |      1.86 |\n\n</pre> <p><code>lmfit</code> with decorator</p> In\u00a0[11]: Copied! <pre>from lmfit import Model\n\n@fit_output\ndef lmfit_lorentzian(x_data, y_data):\n    # Create an lmfit model for the Lorentzian function\n    lorentzian_model = Model(lorentzian)\n    # Define the parameters\n    params = lorentzian_model.make_params(A=1, x0=0, w=1)\n    # Perform the fit\n    result = lorentzian_model.fit(y_data, params, x=x_data)\n    return result\n\nfit_result = lmfit_lorentzian(x_data, y_data)\nfit_result.summary()\n</pre> from lmfit import Model  @fit_output def lmfit_lorentzian(x_data, y_data):     # Create an lmfit model for the Lorentzian function     lorentzian_model = Model(lorentzian)     # Define the parameters     params = lorentzian_model.make_params(A=1, x0=0, w=1)     # Perform the fit     result = lorentzian_model.fit(y_data, params, x=x_data)     return result  fit_result = lmfit_lorentzian(x_data, y_data) fit_result.summary() <pre>reduced \u03c7\u00b2\t8.869e-05\tGREAT (or overfitting)\n| Param   |   Fitted value |   STD error |   % Error |\n|---------|----------------|-------------|-----------|\n| A       |      1.00732   |     0.02157 |      2.14 |\n| x0      |      0.0435118 |     0.04263 |     97.97 |\n| w       |      2.01523   |     0.06111 |      3.03 |\n\n</pre> <p>Note how in the lmfit case no metadata is required, because all the information can be directly extracted from the lmfit output.</p> <p>We start by defining the gaussian function and some synthetic data</p> In\u00a0[12]: Copied! <pre># Define the gaussian function\ndef gaussian(x, A, mu, sigma):\n    return A * np.exp(-(x - mu)**2 / (2 * sigma**2))\n\n# Generate synthetic data\ntrue_params = [1, 0, 2]  # A=1, mu=0, sigma=2\nnp.random.seed(11)\nx_data = np.linspace(-10, 10, 100)\nnoise = 0.01 * np.random.normal(size=len(x_data))\ny_data = gaussian(x_data, *true_params) + noise\n</pre> # Define the gaussian function def gaussian(x, A, mu, sigma):     return A * np.exp(-(x - mu)**2 / (2 * sigma**2))  # Generate synthetic data true_params = [1, 0, 2]  # A=1, mu=0, sigma=2 np.random.seed(11) x_data = np.linspace(-10, 10, 100) noise = 0.01 * np.random.normal(size=len(x_data)) y_data = gaussian(x_data, *true_params) + noise <p>Then we write the gaussian fit function. Instead of immediately returning the result, we perform additional calculations on the optimized parameters, find the fwhm and pass it into the metadata dictionary</p> In\u00a0[13]: Copied! <pre>@fit_output\ndef fit_gaussian(x_data, y_data):\n    # Perform the fit\n    result = curve_fit(gaussian, x_data, y_data, full_output=True)\n\n    # Extract the value of sigma from the optimized parameters\n    params = result[0]\n    _, _, sigma = params\n    # Compute the FWHM from sigma\n    fwhm = 2 * np.sqrt(2 * np.log(2)) * sigma\n\n    # Create a metadata dict that includes the fwhm\n    metadata = {\n        \"fwhm\": fwhm\n    }\n\n    return result, metadata\n</pre> @fit_output def fit_gaussian(x_data, y_data):     # Perform the fit     result = curve_fit(gaussian, x_data, y_data, full_output=True)      # Extract the value of sigma from the optimized parameters     params = result[0]     _, _, sigma = params     # Compute the FWHM from sigma     fwhm = 2 * np.sqrt(2 * np.log(2)) * sigma      # Create a metadata dict that includes the fwhm     metadata = {         \"fwhm\": fwhm     }      return result, metadata <p>From the FitResult object we can access <code>.metadata</code> and get the value of the fwhm</p> In\u00a0[14]: Copied! <pre>fit_result = fit_gaussian(x_data, y_data)\nfwhm = fit_result.metadata['fwhm']\n\nprint(f\"FWHM = {fwhm:.2f}\")\n</pre> fit_result = fit_gaussian(x_data, y_data) fwhm = fit_result.metadata['fwhm']  print(f\"FWHM = {fwhm:.2f}\") <pre>FWHM = 4.72\n</pre> <p>Sometimes it's also useful automatically perform calculations on the data that the decorator computes.</p> <p>For example, let's say we want to calculate <code>u = x * sigma</code>. Then we would need to follow the exact same steps as the previous example and pass <code>u</code> instead of <code>fwhm</code>. However, if we're interested in the error on <code>u</code> it's more complicated.</p> <p>We need to compute the error propagation, but we don't have access to the standard errors form the <code>curve_fit</code> result. To solve this issue, it's possible to name elements of the metadata dictionary starting with <code>@</code>. These special elements will be processed as functions that take as input <code>sqil_dict</code>. <code>sqil_dict</code> is the dictionary used internally by the <code>@fit_output</code> decorator to create the <code>FitResult</code> object. It contains all the information of <code>FitResult</code>, but in a dictionary format.</p> <p>Knowing this, we can create a function to compute the error propagation that takes as input <code>sqil_dict</code>. This function will be called by the decorator and we'll be able to access the result of the function from the <code>.metadata</code> field of the fit result.</p> <p>Let's create out fit function first</p> In\u00a0[15]: Copied! <pre>@fit_output\ndef fit_gaussian(x_data, y_data):\n    # Perform the fit\n    result = curve_fit(gaussian, x_data, y_data, full_output=True)\n\n    # Define the error propagation function, which must take sqil_dict as input\n    def compute_fwhm_error(sqil_dict):\n        # Extract parameters and errors from sqil_dict\n        _, mu, sigma = sqil_dict[\"params\"]\n        _, mu_err, sigma_err = sqil_dict[\"std_err\"]\n        # Compute the error propagation\n        return np.sqrt((sigma * mu_err)**2 + (mu * sigma_err)**2)\n\n    # Add the function to the metadata, with a name starting with @\n    metadata = {\n        \"@fwhm_err\": compute_fwhm_error\n    }\n\n    return result, metadata\n</pre> @fit_output def fit_gaussian(x_data, y_data):     # Perform the fit     result = curve_fit(gaussian, x_data, y_data, full_output=True)      # Define the error propagation function, which must take sqil_dict as input     def compute_fwhm_error(sqil_dict):         # Extract parameters and errors from sqil_dict         _, mu, sigma = sqil_dict[\"params\"]         _, mu_err, sigma_err = sqil_dict[\"std_err\"]         # Compute the error propagation         return np.sqrt((sigma * mu_err)**2 + (mu * sigma_err)**2)      # Add the function to the metadata, with a name starting with @     metadata = {         \"@fwhm_err\": compute_fwhm_error     }      return result, metadata <p>Now we run the optimization, <code>@fit_output</code> will compute the standard errors for the fit parameters, and after that it will evaluate <code>compute_fwhm_error</code>. We will be able to access the fwhm error value from the metadata, under the field <code>fwhm_err</code></p> In\u00a0[16]: Copied! <pre>fit_result = fit_gaussian(x_data, y_data)\nfwhm = fit_result.metadata['fwhm_err']\n\nprint(f\"FWHM standard error = {fwhm:.6f}\")\n</pre> fit_result = fit_gaussian(x_data, y_data) fwhm = fit_result.metadata['fwhm_err']  print(f\"FWHM standard error = {fwhm:.6f}\") <pre>FWHM standard error = 0.000121\n</pre> <p>Note how the metadata field is not <code>fwhm_err</code> and NOT <code>@fwhm_err</code>, because the function has been evaluated.</p>"},{"location":"Notebooks/fit_output_decorator/#the-fit_output-decorator","title":"The @fit_output decorator\u00b6","text":""},{"location":"Notebooks/fit_output_decorator/#lorentzian-fit-example","title":"Lorentzian fit example\u00b6","text":"<p>Let's define the lorentzian function and create some synthetic data for our example</p>"},{"location":"Notebooks/fit_output_decorator/#without-the-decorator","title":"Without the decorator\u00b6","text":""},{"location":"Notebooks/fit_output_decorator/#with-the-decorator","title":"With the decorator\u00b6","text":""},{"location":"Notebooks/fit_output_decorator/#with-other-fit-librariesmethods","title":"With other fit libraries/methods\u00b6","text":""},{"location":"Notebooks/fit_output_decorator/#advanced-uses-of-metadata","title":"Advanced uses of metadata\u00b6","text":""},{"location":"Notebooks/fit_output_decorator/#passing-extra-information","title":"Passing extra information\u00b6","text":"<p>Let's say we need to perform a gaussian fit. Our fitting parameters will be an amplitude <code>A</code>, the mean <code>mu</code> and the standard deviation <code>sigma</code>. However, in some cases it's useful to know also the FWHM, so we can compute it and pass it into the metadata field.</p>"},{"location":"Notebooks/fit_output_decorator/#computing-extra-information","title":"Computing extra information\u00b6","text":""},{"location":"Notebooks/resonator_fit/","title":"Fitting resonators","text":"In\u00a0[7]: Copied! <pre>import sqil_core as sqil\nimport numpy as np\n\npath = r\"Z:\\Projects\\BottomLoader\\data\\20250612_AQUA_skinny\\2025-06-12\\00000-pulsed_onetone_2025-06-12T205752\"\nmeasurement = \"transmission\"\n\nfreq, data = sqil.extract_h5_data(path, [\"ro_freq\", \"data\"])\n\n# Quick fit to estimate parameters\nparams = sqil.resonator.quick_fit(freq, data, measurement)\n# Full fit\nfit_res = sqil.resonator.full_fit(freq, data, measurement, *params)\n\n# Plot\nx_fit = np.linspace(freq[0], freq[-1], np.max([len(freq), 1000]))\nsqil.resonator.plot_resonator(freq, data, x_fit, fit_res.predict(x_fit))\n\n# Print results\nsqil.resonator.print_resonator_params(fit_res.params, measurement)\n</pre> import sqil_core as sqil import numpy as np  path = r\"Z:\\Projects\\BottomLoader\\data\\20250612_AQUA_skinny\\2025-06-12\\00000-pulsed_onetone_2025-06-12T205752\" measurement = \"transmission\"  freq, data = sqil.extract_h5_data(path, [\"ro_freq\", \"data\"])  # Quick fit to estimate parameters params = sqil.resonator.quick_fit(freq, data, measurement) # Full fit fit_res = sqil.resonator.full_fit(freq, data, measurement, *params)  # Plot x_fit = np.linspace(freq[0], freq[-1], np.max([len(freq), 1000])) sqil.resonator.plot_resonator(freq, data, x_fit, fit_res.predict(x_fit))  # Print results sqil.resonator.print_resonator_params(fit_res.params, measurement) <pre>| Param     | Value       |\n|-----------|-------------|\n| fr        | 7.46257 GHz |\n| Q_tot     | 5074        |\n| kappa_tot | 1.471 MHz   |\n</pre> <p>For extra information on your fit run</p> In\u00a0[8]: Copied! <pre>fit_res.summary();\n</pre> fit_res.summary(); <pre>reduced \u03c7\u00b2  2.720e-11             GREAT (or overfitting)\nnrmse       5.960e-03-1.300e-04j  GREAT\n| Param   |   Fitted value |    STD error |   % Error |\n|---------|----------------|--------------|-----------|\n| a       |    0.000711667 |    1.566e-06 |      0.22 |\n| alpha   |   -0.666959    |    0.01078   |     -1.62 |\n| tau     |   -5.40365e-08 |    8.397e-11 |     -0.16 |\n| Q_tot   | 5074.46        |   16.21      |      0.32 |\n| fr      |    7.46257e+09 | 2285         |      0    |\n\n</pre> <p>The magnitude background can create problems for the fit, so we need a way to account for it.</p> In\u00a0[9]: Copied! <pre>import sqil_core as sqil\nimport numpy as np\nimport matplotlib.pyplot as plt\n\npath = r\"Z:\\Projects\\BottomLoader\\RT_measurement\\20241212_cavities\\2024-12-12\\00082-cavity_check_2024-12-12T174140\"\nmeasurement = \"hanger\"\n\n[freq], [mag_db], [phase] = sqil.extract_h5_data(path, [\"frequency\", \"mag\", \"phase\"])\nlinmag = 10 ** (mag_db / 20)\ndata = linmag * np.exp(1j * phase)\nfreq = freq\n\n\nplt.figure(figsize=(4,2))\nplt.title(\"Magnitude\")\nplt.plot(freq, linmag, '.-')\nplt.show()\n</pre> import sqil_core as sqil import numpy as np import matplotlib.pyplot as plt  path = r\"Z:\\Projects\\BottomLoader\\RT_measurement\\20241212_cavities\\2024-12-12\\00082-cavity_check_2024-12-12T174140\" measurement = \"hanger\"  [freq], [mag_db], [phase] = sqil.extract_h5_data(path, [\"frequency\", \"mag\", \"phase\"]) linmag = 10 ** (mag_db / 20) data = linmag * np.exp(1j * phase) freq = freq   plt.figure(figsize=(4,2)) plt.title(\"Magnitude\") plt.plot(freq, linmag, '.-') plt.show() <p>Find a way to estimate your background. It's recommended to use a simple fit, linear or at most polynomial. It could be helpful to take a wider shot of your data.</p> <p>IMPORTANT: The background can be ANY array of the same size as the frequency, so it can be completely arbitrary.</p> In\u00a0[10]: Copied! <pre># Estimate the linear background from the last few points\n[y0, m] = sqil.estimate_linear_background(freq, linmag, cut_from_back=True)\n\nplt.figure(figsize=(4, 2))\nplt.title(\"Magnitude\")\nplt.plot(freq, linmag, \".-\")\nplt.plot(freq, m * freq + y0, \"-.\", color='tab:green')\nplt.show()\n</pre> # Estimate the linear background from the last few points [y0, m] = sqil.estimate_linear_background(freq, linmag, cut_from_back=True)  plt.figure(figsize=(4, 2)) plt.title(\"Magnitude\") plt.plot(freq, linmag, \".-\") plt.plot(freq, m * freq + y0, \"-.\", color='tab:green') plt.show() <p>Then cut the data closer to the resonance. Don't be afraid to get quite close. Remmeber to also cut the background!</p> In\u00a0[11]: Copied! <pre>cut = slice(300, 600)\nfreq, data = freq[cut], data[cut]\n\n# After cutting the data don't forget to define\n# the background on the newly cut frequency\nmag_bg = m * freq + y0\n\nplt.figure(figsize=(4, 2))\nplt.title(\"Magnitude\")\nplt.plot(freq, np.abs(data), \".-\")\nplt.plot(freq, mag_bg, \"-.\", color=\"tab:green\")\nplt.show()\n</pre> cut = slice(300, 600) freq, data = freq[cut], data[cut]  # After cutting the data don't forget to define # the background on the newly cut frequency mag_bg = m * freq + y0  plt.figure(figsize=(4, 2)) plt.title(\"Magnitude\") plt.plot(freq, np.abs(data), \".-\") plt.plot(freq, mag_bg, \"-.\", color=\"tab:green\") plt.show() In\u00a0[12]: Copied! <pre># Quick fit to estimate parameters\nparams = sqil.resonator.quick_fit(freq, data, measurement, mag_bg=mag_bg)\n# Full fit\nfit_res = sqil.resonator.full_fit(freq, data, measurement, *params, mag_bg=mag_bg)\n\n# Print results\nsqil.resonator.print_resonator_params(fit_res.params, measurement)\n\n# Plot\nsqil.resonator.plot_resonator(freq, data, freq, fit_res.predict(freq), mag_bg=mag_bg)\n</pre> # Quick fit to estimate parameters params = sqil.resonator.quick_fit(freq, data, measurement, mag_bg=mag_bg) # Full fit fit_res = sqil.resonator.full_fit(freq, data, measurement, *params, mag_bg=mag_bg)  # Print results sqil.resonator.print_resonator_params(fit_res.params, measurement)  # Plot sqil.resonator.plot_resonator(freq, data, freq, fit_res.predict(freq), mag_bg=mag_bg) <pre>| Param     | Value       |\n|-----------|-------------|\n| fr        | 7.74145 GHz |\n| Re[Q_ext] | 824         |\n| Q_int     | 3564        |\n| Q_tot     | 2237        |\n| kappa_ext | 9.39 MHz    |\n| kappa_int | 2.172 MHz   |\n| kappa_tot | 3.461 MHz   |\n</pre>"},{"location":"Notebooks/resonator_fit/#fitting-resonators","title":"Fitting resonators\u00b6","text":""},{"location":"Notebooks/resonator_fit/#general-usecase","title":"General usecase\u00b6","text":"<ul> <li>Import your data as a complex number, make sure your magnitude is linear</li> <li>Select the correct <code>measurement</code> type between <code>'reflection'</code>, <code>'hanger'</code> and <code>'transmission'</code></li> </ul> <p>Read the SQIL-core documentation to learn about all the options.</p>"},{"location":"Notebooks/resonator_fit/#fitting-with-a-background","title":"Fitting with a background\u00b6","text":""}]}